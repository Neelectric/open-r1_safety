.PHONY: style quality

# make sure to test the local checkout in scripts and not the pre-installed one (don't use quotes!)
export PYTHONPATH = src

check_dirs := src tests

VENV_PATH ?= /root/openr1_v2 #this puts venv on local SSD which is hopefully much faster than NFS

# dev dependencies
install:
	uv venv $(VENV_PATH) --python 3.12
	. $(VENV_PATH)/bin/activate && uv pip install --upgrade pip && \
	uv pip install setuptools && \
	GIT_LFS_SKIP_SMUDGE=1 uv pip install -e ".[dev]"
	. $(VENV_PATH)/bin/activate && uv pip uninstall torch && \
	uv pip install torch==2.9.0 && \
	uv pip install git+https://github.com/huggingface/trl.git && \
	uv pip install https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.3/flash_attn-2.8.3+cu12torch2.9cxx11abiTRUE-cp312-cp312-linux_x86_64.whl --no-build-isolation --no-deps && \
	GIT_LFS_SKIP_SMUDGE=1 uv pip install git+https://github.com/Neelectric/lighteval && \
	uv pip install vllm==0.11.2 && \
	uv pip install gpustat gpuselect && \
	uv pip install more_itertools && \
	uv pip install ipykernel jupyter matplotlib seaborn


	
# lighteval==0.13.0 requires "vllm>=0.10.0,<0.10.2", but vllm==0.10.1 requires torch==2.7.1 which is incompatible with trl and FA2 versions. forked own lighteval branch with vllm req lifted

# . openr1_v2/bin/activate && uv pip install flashinfer-python

# uv pip install https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.0/flash_attn-2.8.0.post2+cu12torch2.6cxx11abiFALSE-cp311-cp311-linux_x86_64.whl --no-build-isolation --no-deps && \
	
# uv pip install trl==0.25.0 && \
# uv pip install https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.3/flash_attn-2.8.3+cu12torch2.8cxx11abiFALSE-cp311-cp311-linux_x86_64.whl --no-build-isolation --no-deps

style:
	ruff format --line-length 119 --target-version py310 $(check_dirs) setup.py
	isort $(check_dirs) setup.py

quality:
	ruff check --line-length 119 --target-version py310 $(check_dirs) setup.py
	isort --check-only $(check_dirs) setup.py
	flake8 --max-line-length 119 $(check_dirs) setup.py

test:
	pytest -sv --ignore=tests/slow/ tests/

slow_test:
	pytest -sv -vv tests/slow/

# Evaluation

evaluate:
	$(eval PARALLEL_ARGS := $(if $(PARALLEL),$(shell \
		if [ "$(PARALLEL)" = "data" ]; then \
			echo "data_parallel_size=$(NUM_GPUS)"; \
		elif [ "$(PARALLEL)" = "tensor" ]; then \
			echo "tensor_parallel_size=$(NUM_GPUS)"; \
		fi \
	),))
	$(if $(filter tensor,$(PARALLEL)),export VLLM_WORKER_MULTIPROC_METHOD=spawn &&,) \
	MODEL_ARGS="pretrained=$(MODEL),dtype=bfloat16,$(PARALLEL_ARGS),max_model_length=32768,gpu_memory_utilization=0.8,generation_parameters={max_new_tokens:32768,temperature:0.6,top_p:0.95}" && \
	if [ "$(TASK)" = "lcb" ]; then \
		lighteval vllm $$MODEL_ARGS "extended|lcb:codegeneration|0|0" \
			--use-chat-template \
			--output-dir data/evals/$(MODEL); \
	else \
		lighteval vllm $$MODEL_ARGS "lighteval|$(TASK)|0|0" \
			--use-chat-template \
			--output-dir data/evals/$(MODEL); \
	fi
