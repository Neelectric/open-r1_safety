{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testbed for testing Fisher-based continual learning for safety"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and helper functions\n",
    "- Mostly boilerplate, skippable code.\n",
    "- Loads model onto device, loads tokenizer and sets assistant tags and reasoning system prompt as expected by trainer.\n",
    "- Tries to load pre-processed/-tokenized dataset from local dir. Otherwise, downloads dataset, prepares it for DataCollator by setting assistant_tokens_mask, and saves to local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, get_scheduler\n",
    "import torch\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from tqdm import tqdm\n",
    "from trl.trainer.sft_trainer import DataCollatorForLanguageModeling\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_id, device):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id,dtype=torch.bfloat16,device_map=device,)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id,)\n",
    "    return model, tokenizer\n",
    "\n",
    "def load_or_preprocess_dataset(model_id, dataset_id, tokenizer, max_length=4096):\n",
    "    local_ds_id = f\"datasets/{model_id}/{dataset_id}\"\n",
    "    num_proc = 16\n",
    "    try:\n",
    "        filtered_dataset = load_from_disk(local_ds_id)\n",
    "        print(f\"Loaded dataset from local dir {local_ds_id}\")\n",
    "    except:\n",
    "        print(f\"Dataset not found locally, processing and caching...\")\n",
    "        dataset = load_dataset(dataset_id)[\"train\"]\n",
    "        if False:\n",
    "            messages = dataset[0]['messages']\n",
    "            print(messages)\n",
    "            tokenized = tokenizer.apply_chat_template(messages, tokenize=True, return_assistant_tokens_mask=True, return_dict=True)\n",
    "            print(tokenized)\n",
    "        def preprocess(example):\n",
    "            tokenized = tokenizer.apply_chat_template(\n",
    "                example[\"messages\"],\n",
    "                tokenize=True,\n",
    "                return_assistant_tokens_mask=True,\n",
    "                return_dict=True,\n",
    "                return_tensors=\"pt\",\n",
    "                # max_length=max_length,\n",
    "                # truncation=True,\n",
    "            )\n",
    "            return {\n",
    "                \"input_ids\": tokenized[\"input_ids\"],\n",
    "                \"assistant_masks\": tokenized[\"assistant_masks\"],\n",
    "            }\n",
    "        tokenized_dataset = dataset.map(preprocess, remove_columns=dataset.column_names, num_proc=num_proc, desc=\"Tokenizing\")\n",
    "        def shorter_than(example):\n",
    "            input_ids = example[\"input_ids\"][0]\n",
    "            length = len(input_ids)\n",
    "            return length <= max_length\n",
    "        filtered_dataset = tokenized_dataset.filter(shorter_than, desc=f\"Filtering to chosen max length of {max_length}\", num_proc=num_proc)\n",
    "        \n",
    "        print(f\"Tokenized dataset has length {len(tokenized_dataset)}, filtered_dataset has length {len(filtered_dataset)}\")\n",
    "        filtered_dataset.save_to_disk(local_ds_id)\n",
    "    return filtered_dataset\n",
    "\n",
    "\n",
    "def create_dataloader(tokenizer, tokenized_dataset, batch_size):\n",
    "    collator = DataCollatorForLanguageModeling(pad_token_id=tokenizer.pad_token_id,)\n",
    "    dataloader = DataLoader(\n",
    "        tokenized_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collator,\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "def add_reasoning_chat_template(tokenizer):\n",
    "    if \"qwen\" in tokenizer.name_or_path.lower():\n",
    "        # we have to use DataCollatorForLanguageModeling with completion_only_loss=True\n",
    "        # however, for that tokenizer needs to have return_assistant_tokens_mask=True, and qwen decided against adding support for {% generation %} / {% endgeneration %} functionality\n",
    "        # so we download a community qwen3 chat template that has it\n",
    "        !wget -O all_assistant.jinja --no-check-certificate https://raw.githubusercontent.com/HarryMayne/qwen_3_chat_templates/refs/heads/main/all_assistant.jinja\n",
    "        !mv all_assistant.jinja chat_templates/all_assistant.jinja\n",
    "        with open('chat_templates/all_assistant.jinja', 'r') as f:\n",
    "            tokenizer.chat_template = f.read()\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model/Dataset IDs, hyperparam choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_model_ids = [\n",
    "    \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    \"allenai/OLMo-2-0425-1B-Instruct\",\n",
    "    \"Qwen/Qwen3-0.6B\"\n",
    "]\n",
    "big_model_ids = [\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    \"allenai/OLMo-2-1124-7B-Instruct\",\n",
    "    \"Qwen/Qwen3-8B\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"Neelectric/OpenR1-Math-220k_CN-K12_OLMo-2_4096toks\"\n",
    "device = \"cuda:0\"\n",
    "model_id = small_model_ids[2]\n",
    "batch_size = 2\n",
    "max_length = 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model, tokenizer, dataset, dataloader, optimizer, LR scheduler, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in Qwen/Qwen3-0.6B\n",
      "--2025-12-26 13:47:22--  https://raw.githubusercontent.com/HarryMayne/qwen_3_chat_templates/refs/heads/main/all_assistant.jinja\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4153 (4.1K) [text/plain]\n",
      "Saving to: ‘all_assistant.jinja’\n",
      "\n",
      "all_assistant.jinja 100%[===================>]   4.06K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-12-26 13:47:22 (35.9 MB/s) - ‘all_assistant.jinja’ saved [4153/4153]\n",
      "\n",
      "Dataset not found locally, processing and caching...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b8fea3dc4da467bb424407e50bab0ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering to chosen max length of 4096 (num_proc=16):   0%|          | 0/69132 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized dataset has length 69132, filtered_dataset has length Dataset({\n",
      "    features: ['input_ids', 'assistant_masks'],\n",
      "    num_rows: 68633\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ccb92a1b6ca4e63835f718d67ffa045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/4 shards):   0%|          | 0/68633 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Loading in {model_id}\")\n",
    "model, tokenizer = load_model_and_tokenizer(model_id, device)\n",
    "tokenizer = add_reasoning_chat_template(tokenizer)\n",
    "tokenized_dataset = load_or_preprocess_dataset(model_id, dataset_id, tokenizer, max_length=max_length)\n",
    "dataloader = create_dataloader(tokenizer, tokenized_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'labels', 'attention_mask'])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to Tensor.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m idx = \u001b[32m0\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, (tok, label) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(batch[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m][idx], batch[\u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m][idx])):\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m3d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtok\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m6d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m6d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer.decode([tok])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m200\u001b[39m: \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/open-r1_safety/openr1_v3/lib/python3.11/site-packages/torch/_tensor.py:1130\u001b[39m, in \u001b[36mTensor.__format__\u001b[39m\u001b[34m(self, format_spec)\u001b[39m\n\u001b[32m   1126\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dim() == \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m Tensor:\n\u001b[32m   1127\u001b[39m     \u001b[38;5;66;03m# Use detach() here to avoid the warning when converting a scalar Tensor that\u001b[39;00m\n\u001b[32m   1128\u001b[39m     \u001b[38;5;66;03m# requires gradients to a python number. It is ok for formatting.\u001b[39;00m\n\u001b[32m   1129\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.detach().item().\u001b[34m__format__\u001b[39m(format_spec)\n\u001b[32m-> \u001b[39m\u001b[32m1130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__format__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_spec\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: unsupported format string passed to Tensor.__format__"
     ]
    }
   ],
   "source": [
    "batch = next(iter(dataloader))\n",
    "print(batch.keys())  # should have input_ids, attention_mask, labels\n",
    "\n",
    "idx = 0\n",
    "for i, (tok, label) in enumerate(zip(batch[\"input_ids\"][idx], batch[\"labels\"][idx])):\n",
    "    print(f\"{i:3d} | {tok:6d} | {label:6d} | {tokenizer.decode([tok])}\")\n",
    "    if i == 200: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([2, 3735]),\n",
       " 'labels': torch.Size([2, 3735]),\n",
       " 'attention_mask': torch.Size([2, 3735])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    break\n",
    "batch_shapes = {k: v.shape for k, v in batch.items()}\n",
    "batch_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "batch_device = {}\n",
    "for key, val in batch.items():\n",
    "    val = val.to(model.device)\n",
    "    batch_device[key] = val\n",
    "\n",
    "\n",
    "outputs = model(**batch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_sft():\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Epochs\", dynamic_ncols=True):\n",
    "        for batch in tqdm(train_dataloader, desc=\"Steps in Epoch\", dynamic_ncols=True):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final eval of methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openr1_v3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
