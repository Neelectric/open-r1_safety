{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testbed for testing Fisher-based continual learning for safety"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and helper functions\n",
    "- Mostly boilerplate, skippable code.\n",
    "- Loads model onto device, loads tokenizer and sets assistant tags and reasoning system prompt as expected by trainer.\n",
    "- Tries to load pre-processed/-tokenized dataset from local dir. Otherwise, downloads dataset, prepares it for DataCollator by setting assistant_tokens_mask, and saves to local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm # this makes tqdm.write() work with notebooks!\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, get_scheduler\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "from trl.trainer.sft_trainer import DataCollatorForLanguageModeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_id, device):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id,dtype=torch.bfloat16,device_map=device,)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id,)\n",
    "    return model, tokenizer\n",
    "\n",
    "def load_or_preprocess_dataset(model_id, dataset_id, tokenizer, max_length=4096):\n",
    "    local_ds_id = f\"datasets/{model_id}/{dataset_id}\"\n",
    "    num_proc = 16\n",
    "    # try:\n",
    "    #     # final_dataset = load_from_disk(local_ds_id)\n",
    "    #     # print(f\"Loaded dataset from local dir {local_ds_id}\")\n",
    "    # except:\n",
    "    if True:\n",
    "        print(f\"Dataset not found locally, processing and caching...\")\n",
    "        raw_dataset = load_dataset(dataset_id)[\"train\"]\n",
    "        # raw_dataset = raw_dataset.select(range(5))  # use .select() not slicing - slicing returns a dict!\n",
    "        \n",
    "        def preprocess(example):\n",
    "            tokenized = tokenizer.apply_chat_template(\n",
    "                example[\"messages\"],\n",
    "                tokenize=True,\n",
    "                return_assistant_tokens_mask=True,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            return {\n",
    "                \"input_ids\": tokenized[\"input_ids\"],\n",
    "                \"assistant_masks\": tokenized[\"assistant_masks\"],\n",
    "            }\n",
    "        \n",
    "        tokenized_dataset = raw_dataset.map(preprocess, remove_columns=raw_dataset.column_names, num_proc=num_proc, desc=\"Tokenizing\")\n",
    "        def shorter_than(example):\n",
    "            return len(example[\"input_ids\"]) <= max_length\n",
    "        final_dataset = tokenized_dataset.filter(shorter_than, num_proc=num_proc, desc=f\"Filtering to max length {max_length}\")\n",
    "        print(f\"Tokenized: {len(tokenized_dataset)}, After filtering: {len(final_dataset)}\")\n",
    "        final_dataset.save_to_disk(local_ds_id)\n",
    "    return final_dataset\n",
    "\n",
    "\n",
    "def create_dataloader(tokenizer, tokenized_dataset, batch_size):\n",
    "    collator = DataCollatorForLanguageModeling(pad_token_id=tokenizer.pad_token_id,)\n",
    "    dataloader = DataLoader(\n",
    "        tokenized_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collator,\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "def add_reasoning_chat_template(tokenizer):\n",
    "    if \"qwen\" in tokenizer.name_or_path.lower():\n",
    "        # we have to use DataCollatorForLanguageModeling with completion_only_loss=True\n",
    "        # however, for that tokenizer needs to have return_assistant_tokens_mask=True, and qwen decided against adding support for {% generation %} / {% endgeneration %} functionality\n",
    "        # so we download a community qwen3 chat template that has it\n",
    "        !wget -O all_assistant.jinja --no-check-certificate https://raw.githubusercontent.com/HarryMayne/qwen_3_chat_templates/refs/heads/main/all_assistant.jinja\n",
    "        !mv all_assistant.jinja chat_templates/all_assistant.jinja\n",
    "        with open('chat_templates/all_assistant.jinja', 'r') as f:\n",
    "            tokenizer.chat_template = f.read()\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model/Dataset IDs, hyperparam choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_model_ids = [\n",
    "    \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    \"allenai/OLMo-2-0425-1B-Instruct\",\n",
    "    \"Qwen/Qwen3-0.6B\",\n",
    "    \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "]\n",
    "big_model_ids = [\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    \"allenai/OLMo-2-1124-7B-Instruct\",\n",
    "    \"Qwen/Qwen3-8B\",\n",
    "    \"HuggingFaceTB/SmolLM2-1.7B-Instruct\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"Neelectric/OpenR1-Math-220k_CN-K12_OLMo-2_4096toks\"\n",
    "device = \"cuda:0\"\n",
    "model_id = small_model_ids[2]\n",
    "batch_size = 8\n",
    "max_length = 1024\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model, tokenizer, dataset, dataloader, optimizer, LR scheduler, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in Qwen/Qwen3-0.6B\n",
      "--2025-12-31 13:19:43--  https://raw.githubusercontent.com/HarryMayne/qwen_3_chat_templates/refs/heads/main/all_assistant.jinja\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4153 (4.1K) [text/plain]\n",
      "Saving to: ‘all_assistant.jinja’\n",
      "\n",
      "all_assistant.jinja 100%[===================>]   4.06K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-12-31 13:19:43 (12.9 MB/s) - ‘all_assistant.jinja’ saved [4153/4153]\n",
      "\n",
      "Dataset not found locally, processing and caching...\n",
      "Tokenized: 69132, After filtering: 4749\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54e64b6e6c0145089b477c7b58e7d1a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4749 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Loading in {model_id}\")\n",
    "model, tokenizer = load_model_and_tokenizer(model_id, device)\n",
    "tokenizer = add_reasoning_chat_template(tokenizer)\n",
    "tokenized_dataset = load_or_preprocess_dataset(model_id, dataset_id, tokenizer, max_length=max_length)\n",
    "dataloader = create_dataloader(tokenizer, tokenized_dataset, batch_size)\n",
    "num_training_steps = num_epochs * len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "594"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'labels', 'attention_mask'])\n",
      "torch.Size([8, 1022])\n",
      "  0 | 151644 |   -100 | <|im_start|>\n",
      "  1 |    872 |   -100 | user\n",
      "  2 |    198 |   -100 | \n",
      "\n",
      "  3 |   2679 |   -100 | If\n",
      "  4 |    400 |   -100 |  $\n",
      "  5 |     17 |   -100 | 2\n",
      "  6 |  47822 |   -100 | ^{\n",
      "  7 |     64 |   -100 | a\n",
      "  8 |  51185 |   -100 | }=\n",
      "  9 |     21 |   -100 | 6\n",
      " 10 |  54876 |   -100 | $,\n",
      " 11 |    400 |   -100 |  $\n",
      " 12 |     65 |   -100 | b\n",
      " 13 |  34433 |   -100 | =\\\n",
      " 14 |    839 |   -100 | log\n",
      " 15 |  15159 |   -100 | _{\n",
      " 16 |     17 |   -100 | 2\n",
      " 17 |     92 |   -100 | }\n",
      " 18 |     18 |   -100 | 3\n",
      " 19 |  54876 |   -100 | $,\n",
      " 20 |   1221 |   -100 |  then\n",
      " 21 |    400 |   -100 |  $\n",
      " 22 |     64 |   -100 | a\n",
      " 23 |   1455 |   -100 | -b\n",
      " 24 |   3186 |   -100 | =$\n",
      " 25 |   1124 |   -100 |  \\\n",
      " 26 |  56014 |   -100 | _\\\n",
      " 27 |  56014 |   -100 | _\\\n",
      " 28 |  56014 |   -100 | _\\\n",
      " 29 |  56014 |   -100 | _\\\n",
      " 30 |  56014 |   -100 | _\\\n",
      " 31 |   4950 |   -100 | _.\n",
      " 32 | 151645 |   -100 | <|im_end|>\n",
      " 33 |    198 |   -100 | \n",
      "\n",
      " 34 | 151644 |   -100 | <|im_start|>\n",
      " 35 |  77091 |   -100 | assistant\n",
      " 36 |    198 |   -100 | \n",
      "\n",
      " 37 | 151667 | 151667 | <think>\n",
      " 38 |    198 |    198 | \n",
      "\n",
      " 39 |  32313 |  32313 | Okay\n",
      " 40 |     11 |     11 | ,\n",
      " 41 |    773 |    773 |  so\n",
      " 42 |    358 |    358 |  I\n",
      " 43 |   1184 |   1184 |  need\n",
      " 44 |    311 |    311 |  to\n",
      " 45 |  11625 |  11625 |  solve\n",
      " 46 |    369 |    369 |  for\n",
      " 47 |  17767 |  17767 |  \\(\n",
      " 48 |    264 |    264 |  a\n",
      " 49 |    481 |    481 |  -\n",
      " 50 |    293 |    293 |  b\n",
      " 51 |   1124 |   1124 |  \\\n",
      " 52 |      8 |      8 | )\n",
      " 53 |   2661 |   2661 |  given\n",
      " 54 |    429 |    429 |  that\n",
      " 55 |  17767 |  17767 |  \\(\n",
      " 56 |    220 |    220 |  \n",
      " 57 |     17 |     17 | 2\n",
      " 58 |     61 |     61 | ^\n",
      " 59 |     64 |     64 | a\n",
      " 60 |    284 |    284 |  =\n",
      " 61 |    220 |    220 |  \n",
      " 62 |     21 |     21 | 6\n",
      " 63 |   1124 |   1124 |  \\\n",
      " 64 |      8 |      8 | )\n",
      " 65 |    323 |    323 |  and\n",
      " 66 |  17767 |  17767 |  \\(\n",
      " 67 |    293 |    293 |  b\n",
      " 68 |    284 |    284 |  =\n",
      " 69 |   1124 |   1124 |  \\\n",
      " 70 |    839 |    839 | log\n",
      " 71 |  15159 |  15159 | _{\n",
      " 72 |     17 |     17 | 2\n",
      " 73 |     92 |     92 | }\n",
      " 74 |     18 |     18 | 3\n",
      " 75 |   1124 |   1124 |  \\\n",
      " 76 |    568 |    568 | ).\n",
      " 77 |  88190 |  88190 |  Hmm\n",
      " 78 |     11 |     11 | ,\n",
      " 79 |   1077 |   1077 |  let\n",
      " 80 |    594 |    594 | 's\n",
      " 81 |   1191 |   1191 |  start\n",
      " 82 |    553 |    553 |  by\n",
      " 83 |   8660 |   8660 |  understanding\n",
      " 84 |   1128 |   1128 |  what\n",
      " 85 |   1817 |   1817 |  each\n",
      " 86 |    949 |    949 |  part\n",
      " 87 |   3363 |   3363 |  means\n",
      " 88 |    382 |    382 | .\n",
      "\n",
      "\n",
      " 89 |   5338 |   5338 | First\n",
      " 90 |     11 |     11 | ,\n",
      " 91 |    279 |    279 |  the\n",
      " 92 |  23606 |  23606 |  equation\n",
      " 93 |  17767 |  17767 |  \\(\n",
      " 94 |    220 |    220 |  \n",
      " 95 |     17 |     17 | 2\n",
      " 96 |     61 |     61 | ^\n",
      " 97 |     64 |     64 | a\n",
      " 98 |    284 |    284 |  =\n",
      " 99 |    220 |    220 |  \n",
      "100 |     21 |     21 | 6\n",
      "101 |   1124 |   1124 |  \\\n",
      "102 |    568 |    568 | ).\n",
      "103 |   1096 |   1096 |  This\n",
      "104 |    374 |    374 |  is\n",
      "105 |    458 |    458 |  an\n",
      "106 |  58755 |  58755 |  exponential\n",
      "107 |  23606 |  23606 |  equation\n",
      "108 |     11 |     11 | ,\n",
      "109 |    323 |    323 |  and\n",
      "110 |    358 |    358 |  I\n",
      "111 |   1414 |   1414 |  know\n",
      "112 |    429 |    429 |  that\n",
      "113 |    311 |    311 |  to\n",
      "114 |  11625 |  11625 |  solve\n",
      "115 |    369 |    369 |  for\n",
      "116 |    279 |    279 |  the\n",
      "117 |  27690 |  27690 |  exponent\n",
      "118 |  17767 |  17767 |  \\(\n",
      "119 |    264 |    264 |  a\n",
      "120 |   1124 |   1124 |  \\\n",
      "121 |    701 |    701 | ),\n",
      "122 |    358 |    358 |  I\n",
      "123 |    646 |    646 |  can\n",
      "124 |    990 |    990 |  use\n",
      "125 |  89936 |  89936 |  logarith\n",
      "126 |   1011 |   1011 | ms\n",
      "127 |     13 |     13 | .\n",
      "128 |   8704 |   8704 |  Since\n",
      "129 |    279 |    279 |  the\n",
      "130 |   2331 |   2331 |  base\n",
      "131 |    374 |    374 |  is\n",
      "132 |    220 |    220 |  \n",
      "133 |     17 |     17 | 2\n",
      "134 |     11 |     11 | ,\n",
      "135 |   7196 |   7196 |  maybe\n",
      "136 |   4633 |   4633 |  taking\n",
      "137 |    279 |    279 |  the\n",
      "138 |  89936 |  89936 |  logarith\n",
      "139 |     76 |     76 | m\n",
      "140 |   2331 |   2331 |  base\n",
      "141 |    220 |    220 |  \n",
      "142 |     17 |     17 | 2\n",
      "143 |    315 |    315 |  of\n",
      "144 |   2176 |   2176 |  both\n",
      "145 |  11067 |  11067 |  sides\n",
      "146 |   1035 |   1035 |  would\n",
      "147 |   1492 |   1492 |  help\n",
      "148 |     13 |     13 | .\n",
      "149 |   6771 |   6771 |  Let\n",
      "150 |    752 |    752 |  me\n",
      "151 |   3270 |   3270 |  write\n",
      "152 |    429 |    429 |  that\n",
      "153 |   1495 |   1495 |  down\n",
      "154 |   1447 |   1447 | :\n",
      "\n",
      "\n",
      "155 |  44292 |  44292 | \\(\n",
      "156 |   1124 |   1124 |  \\\n",
      "157 |    839 |    839 | log\n",
      "158 |  15159 |  15159 | _{\n",
      "159 |     17 |     17 | 2\n",
      "160 |  25547 |  25547 | }(\n",
      "161 |     17 |     17 | 2\n",
      "162 |     61 |     61 | ^\n",
      "163 |     64 |     64 | a\n",
      "164 |      8 |      8 | )\n",
      "165 |    284 |    284 |  =\n",
      "166 |   1124 |   1124 |  \\\n",
      "167 |    839 |    839 | log\n",
      "168 |  15159 |  15159 | _{\n",
      "169 |     17 |     17 | 2\n",
      "170 |     92 |     92 | }\n",
      "171 |     21 |     21 | 6\n",
      "172 |   1124 |   1124 |  \\\n",
      "173 |   3593 |   3593 | ).\n",
      "\n",
      "\n",
      "174 |     50 |     50 | S\n",
      "175 |   6383 |   6383 | impl\n",
      "176 |   7766 |   7766 | ifying\n",
      "177 |    279 |    279 |  the\n",
      "178 |   2115 |   2115 |  left\n",
      "179 |   3108 |   3108 |  side\n",
      "180 |     11 |     11 | ,\n",
      "181 |   1576 |   1576 |  because\n",
      "182 |  17767 |  17767 |  \\(\n",
      "183 |   1124 |   1124 |  \\\n",
      "184 |    839 |    839 | log\n",
      "185 |  15159 |  15159 | _{\n",
      "186 |     65 |     65 | b\n",
      "187 |  25547 |  25547 | }(\n",
      "188 |     65 |     65 | b\n",
      "189 |     61 |     61 | ^\n",
      "190 |     87 |     87 | x\n",
      "191 |      8 |      8 | )\n",
      "192 |    284 |    284 |  =\n",
      "193 |    856 |    856 |  x\n",
      "194 |   1124 |   1124 |  \\\n",
      "195 |    701 |    701 | ),\n",
      "196 |    773 |    773 |  so\n",
      "197 |    429 |    429 |  that\n",
      "198 |   9044 |   9044 |  becomes\n",
      "199 |   1447 |   1447 | :\n",
      "\n",
      "\n",
      "200 |  44292 |  44292 | \\(\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(dataloader))\n",
    "print(batch.keys())  # should have input_ids, attention_mask, labels\n",
    "print(batch[\"input_ids\"].shape)\n",
    "idx = 0\n",
    "for i, (tok, label) in enumerate(zip(batch[\"input_ids\"][idx], batch[\"labels\"][idx])):\n",
    "    print(f\"{i:3d} | {tok:6d} | {label:6d} | {tokenizer.decode([tok])}\")\n",
    "    if i == 200: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([8, 966]),\n",
       " 'labels': torch.Size([8, 966]),\n",
       " 'attention_mask': torch.Size([8, 966])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(dataloader))\n",
    "batch_shapes = {k: v.shape for k, v in batch.items()}\n",
    "batch_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "594"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0.05,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "num_training_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps in Epoch:   0%|          | 1/594 [00:01<10:38,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss 0.7292410731315613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps in Epoch:   4%|▍         | 26/594 [00:17<05:58,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss 0.7244220972061157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps in Epoch:   9%|▊         | 51/594 [00:33<05:59,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss 0.5984898209571838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps in Epoch:  13%|█▎        | 76/594 [00:48<05:46,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss 0.6751097440719604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps in Epoch:  17%|█▋        | 101/594 [01:04<05:27,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss 0.5758796334266663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps in Epoch:  21%|██        | 126/594 [01:20<05:15,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss 0.5432909727096558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps in Epoch:  25%|██▌       | 151/594 [01:36<05:03,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss 0.5252648591995239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps in Epoch:  30%|██▉       | 176/594 [01:53<04:44,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss 0.6038434505462646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps in Epoch:  34%|███▍      | 201/594 [02:09<04:27,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss 0.44723621010780334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps in Epoch:  38%|███▊      | 226/594 [02:25<04:12,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss 0.4527837932109833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps in Epoch:  42%|████▏     | 251/594 [02:42<03:52,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss 0.5779042840003967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps in Epoch:  46%|████▋     | 276/594 [02:58<03:39,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss 0.48495909571647644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps in Epoch:  51%|█████     | 301/594 [03:15<03:23,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss 0.5230841636657715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps in Epoch:  55%|█████▍    | 326/594 [03:31<03:04,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss 0.5650283098220825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps in Epoch:  59%|█████▉    | 351/594 [03:48<02:47,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss 0.5074013471603394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps in Epoch:  63%|██████▎   | 376/594 [04:04<02:30,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss 0.4817239046096802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps in Epoch:  68%|██████▊   | 401/594 [04:21<02:09,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss 0.369612455368042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps in Epoch:  72%|███████▏  | 426/594 [04:37<01:55,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss 0.3914449214935303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps in Epoch:  76%|███████▌  | 451/594 [04:53<01:37,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss 0.376362681388855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps in Epoch:  80%|████████  | 476/594 [05:10<01:20,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss 0.47279730439186096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps in Epoch:  84%|████████▍ | 501/594 [05:26<01:04,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss 0.26223501563072205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps in Epoch:  89%|████████▊ | 526/594 [05:43<00:46,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss 0.4997696578502655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps in Epoch:  93%|█████████▎| 551/594 [05:59<00:29,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss 0.4144408404827118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps in Epoch:  97%|█████████▋| 576/594 [06:15<00:12,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss 0.3838016390800476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps in Epoch: 100%|██████████| 594/594 [06:27<00:00,  1.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# def train_with_sft():\n",
    "model.train()\n",
    "epoch = 1\n",
    "# for epoch in tqdm(range(num_epochs), desc=\"Epochs\", dynamic_ncols=True):\n",
    "for i in tqdm(range(num_training_steps), desc=\"Steps in Epoch\", dynamic_ncols=True):\n",
    "    batch = next(iter(dataloader))\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    outputs = model(**batch)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "    if i % 25 == 0:\n",
    "        tqdm.write(f\"Epoch {epoch}, loss {loss.to('cpu')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[151644,    872,    198,  11510,   6556,    362,   7755,   5780,    369,\n",
      "            264,    400,     24,      3,     12,  85526,  20408,  23791,   4227,\n",
      "            323,  17933,    518,    264,  10799,   8061,  26807,     13,   3197,\n",
      "           1340,  22479,    518,    264,   6783,   4628,    315,    400,     82,\n",
      "              3,  40568,    817,   6460,     11,    279,   4227,   4990,   1059,\n",
      "            220,     19,   4115,     11,   2670,    400,     83,      3,   4420,\n",
      "           7391,    304,    279,  10799,   8061,     13,   3197,   1340,  22479,\n",
      "            400,     82,     10,     17,      3,  40568,    817,   6460,     11,\n",
      "            279,   4227,   4990,   1059,    220,     17,   4115,    323,    220,\n",
      "             17,     19,   4420,     11,   2670,    400,     83,      3,   4420,\n",
      "           7391,    304,    279,  10799,   8061,     13,  82610,    362,   7755,\n",
      "          22479,    518,    400,     82,     10,    200,  19959,     90,     16,\n",
      "          15170,     17,  31716,  40568,    817,   6460,     13,   7379,    279,\n",
      "           1372,    315,   4420,    279,   4227,   4990,   1059,     11,   2670,\n",
      "            279,    400,     83,      3,   4420,   7391,    304,    279,  10799,\n",
      "           8061,     13, 151645,    198, 151644,  77091,    198]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Every morning Aya goes for a $9$-kilometer-long walk and stops at a coffee shop afterwards. When she walks at a constant speed of $s$ kilometers per hour, the walk takes her 4 hours, including $t$ minutes spent in the coffee shop. When she walks $s+2$ kilometers per hour, the walk takes her 2 hours and 24 minutes, including $t$ minutes spent in the coffee shop. Suppose Aya walks at $s+\\frac{1}{2}$ kilometers per hour. Find the number of minutes the walk takes her, including the $t$ minutes spent in the coffee shop.\"},\n",
    "]\n",
    "tokenized = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=True,\n",
    "                add_generation_prompt=True,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(\"cuda\")\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "    tokenized,\n",
    "    do_sample=False,\n",
    "    max_new_tokens=2048\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|im_start|>user\\nEvery morning Aya goes for a $9$-kilometer-long walk and stops at a coffee shop afterwards. When she walks at a constant speed of $s$ kilometers per hour, the walk takes her 4 hours, including $t$ minutes spent in the coffee shop. When she walks $s+2$ kilometers per hour, the walk takes her 2 hours and 24 minutes, including $t$ minutes spent in the coffee shop. Suppose Aya walks at $s+\\x0crac{1}{2}$ kilometers per hour. Find the number of minutes the walk takes her, including the $t$ minutes spent in the coffee shop.<|im_end|>\\n<|im_start|>assistant\\n<think>\\nOkay, let\\'s see. So, Aya walks 9 km long every morning, stops at a coffee shop, and then continues walking at a constant speed of s km/h. The total time for the walk is 4 hours, including t minutes spent in the coffee shop. Then, when she walks at s + 2 km/h, the total time is 2 hours and 24 minutes, again including t minutes. We need to find the total time she takes, including t minutes, when she walks at s + 1/2 km/h.\\n\\nHmm, let\\'s break this down. First, let\\'s find the value of s. The first scenario is walking at s km/h for 4 hours, which is 4s km. Since the total distance is 9 km, we can set up the equation:\\n\\n4s = 9\\n\\nSolving for s, we divide both sides by 4:\\n\\ns = 9 / 4 = 2.25 km/h\\n\\nOkay, so s is 2.25 km/h. Now, moving on to the second scenario where she walks at s + 2 km/h. The total time for this is 2 hours and 24 minutes. Let\\'s convert that to hours to make it easier. 2 hours and 24 minutes is 2 + 24/60 = 2 + 0.4 = 2.4 hours. So, the distance covered in this case is:\\n\\n(s + 2) * 2.4 = 2.4s + 4.8\\n\\nBut the total distance is still 9 km, so we can set up the equation:\\n\\n2.4s + 4.8 = 9\\n\\nSubtract 4.8 from both sides:\\n\\n2.4s = 4.2\\n\\nThen, divide both sides by 2.4:\\n\\ns = 4.2 / 2.4 = 1.75 km/h\\n\\nWait, but we already found s as 2.25 km/h. That doesn\\'t match. Hmm, so there must be a mistake here. Let me check again.\\n\\nFirst scenario: 4 hours at s km/h, total distance 9 km. So 4s = 9 => s = 2.25. Correct.\\n\\nSecond scenario: 2 hours and 24 minutes is 2.4 hours. Walking at s + 2 km/h. So distance is (s + 2)*2.4. But total distance is still 9 km. So:\\n\\n(s + 2)*2.4 = 9\\n\\nSubstituting s = 2.25:\\n\\n(2.25 + 2)*2.4 = 9\\n\\n4.25 * 2.4 = 10.2\\n\\nWait, 4.25 * 2.4. Let\\'s calculate that. 4 * 2.4 = 9.6, 0.25 * 2.4 = 0.6. So total is 9.6 + 0.6 = 10.2. But the total distance is 9 km. So that\\'s not possible. Therefore, there\\'s a contradiction here. Which means my calculation for s is wrong.\\n\\nWait, so if s = 2.25, then in the second scenario, the distance would be (2.25 + 2)*2.4 = 4.25*2.4 = 10.2 km, which is more than 9 km. But the problem states that she walks 9 km. So this is impossible. Therefore, my calculation for s must be incorrect.\\n\\nSo, where did I go wrong? Let\\'s check the first equation again. The first scenario: 4 hours at s km/h, total distance 9 km. So 4s = 9 => s = 2.25. Correct. Then the second scenario: 2 hours and 24 minutes is 2.4 hours. So (s + 2)*2.4 = 9. Substituting s = 2.25:\\n\\n(2.25 + 2)*2.4 = 4.25*2.4 = 10.2. But 10.2 km is more than 9 km. So this is impossible. Therefore, there\\'s a mistake in the problem statement? Or in my calculations?\\n\\nWait, maybe I misread the problem. Let me check again.\\n\\nThe problem says: \"When she walks at a constant speed of s kilometers per hour, the walk takes her 4 hours, including t minutes spent in the coffee shop. When she walks s+2 kilometers per hour, the walk takes her 2 hours and 24 minutes, including t minutes spent in the coffee shop.\"\\n\\nSo, in the first case, total time is 4 hours, total distance 9 km. So 4s = 9 => s = 2.25.\\n\\nIn the second case, total time is 2 hours and 24 minutes, which is 2.4 hours. So (s + 2)*2.4 = 9. But with s = 2.25, this gives 10.2 km, which is more than 9 km. Therefore, this is impossible. So the problem is contradictory? But that can\\'t be. So there must be a mistake in my calculations.\\n\\nWait, maybe the problem is that when she walks at s + 2 km/h, the total time is 2 hours and 24 minutes, but the total distance is still 9 km. So:\\n\\n(s + 2) * (2 + 24/60) = 9\\n\\nWhich is:\\n\\n(s + 2) * (2 + 0.4) = 9\\n\\n(s + 2) * 2.4 = 9\\n\\nSo solving for s:\\n\\ns + 2 = 9 / 2.4\\n\\ns + 2 = 3.75\\n\\nTherefore, s = 1.75 km/h\\n\\nOh! So s is 1.75 km/h. Then, in the first scenario, 4 hours at 1.75 km/h: 4 * 1.75 = 7 km, which is less than 9 km. So that\\'s possible. Then, in the second scenario, 2 hours and 24 minutes is 2.4 hours, so (1.75 + 2) * 2.4 = 3.75 * 2.4 = 9 km. So that works. So my previous calculation for s was wrong because I thought the total time was 4 hours, but actually, the total time is 2 hours and 24 minutes, which is 2.4 hours. So solving for s gives 1.75 km/h. Then, in the first scenario, 4 hours at 1.75 km/h gives 7 km, which is less than 9 km, so that\\'s okay. Then, in the second scenario, 2.4 hours at 3.75 km/h gives 9 km. So that\\'s correct.\\n\\nTherefore, the total time when walking at s + 1/2 km/h is:\\n\\nTime = (s + 1/2) * (2 + 24/60)\\n\\nWe already know s is 1.75, so:\\n\\nTime = (1.75 + 0.5) * (2 + 0.4)\\n\\nTime = 2.25 * 2.4\\n\\nCalculating that: 2.25 * 2.4. Let\\'s compute 2 * 2.4 = 4.8, 0.25 * 2.4 = 0.6. Total is 4.8 + 0.6 = 5.4 hours.\\n\\nBut the problem asks for the time including t minutes spent in the coffee shop. So we need to find t. In the first scenario, t minutes is the time spent in the coffee shop, which is 4 hours minus the time spent walking. Wait, no. Wait, the total time is 4 hours, which includes t minutes spent in the coffee shop. So t minutes is part of the total time. So the time spent walking is 4 hours minus t minutes. But we need to find t.\\n\\nAlternatively, since the total time is 4 hours, which includes t minutes, then the time spent walking is 4 hours minus t minutes. But we also know that the distance walked is 9 km at speed s km/h. So:\\n\\ns * (4 - t/60) = 9\\n\\nBecause 4 hours is 4 hours, which is 4*60=240 minutes, so 4 - t/60 hours. So:\\n\\ns * (4 - t/60) = 9\\n\\nBut we know s is 1.75, so:\\n\\n1.75 * (4 - t/60) = 9\\n\\nLet\\'s solve for t.\\n\\nFirst, divide both sides by 1.75:\\n\\n4 - t/60 = 9 / 1.75\\n\\nCalculate 9 / 1.75. 1.75 is 7/4, so 9 divided by 7/4 is 9 * 4/7 = 36/7 ≈ 5.142857...\\n\\nSo:\\n\\n4 - t/60 = 36/7\\n\\nSubtract 4 from both sides:\\n\\nt/60 = 4 - 36/7\\n\\nConvert 4']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode(outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final eval of methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openr1_v3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
