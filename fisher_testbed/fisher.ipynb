{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testbed for testing Fisher-based continual learning for safety"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and helper functions\n",
    "- Mostly boilerplate, skippable code.\n",
    "- Loads model onto device, loads tokenizer and sets assistant tags and reasoning system prompt as expected by trainer.\n",
    "- Tries to load pre-processed/-tokenized dataset from local dir. Otherwise, downloads dataset, prepares it for DataCollator by setting assistant_tokens_mask, and saves to local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, get_scheduler\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "from trl.trainer.sft_trainer import DataCollatorForLanguageModeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_id, device):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id,dtype=torch.bfloat16,device_map=device,)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id,)\n",
    "    return model, tokenizer\n",
    "\n",
    "def load_or_preprocess_dataset(model_id, dataset_id, tokenizer, max_length=4096):\n",
    "    local_ds_id = f\"datasets/{model_id}/{dataset_id}\"\n",
    "    num_proc = 16\n",
    "    # try:\n",
    "    #     # final_dataset = load_from_disk(local_ds_id)\n",
    "    #     # print(f\"Loaded dataset from local dir {local_ds_id}\")\n",
    "    # except:\n",
    "    if True:\n",
    "        print(f\"Dataset not found locally, processing and caching...\")\n",
    "        raw_dataset = load_dataset(dataset_id)[\"train\"]\n",
    "        # raw_dataset = raw_dataset.select(range(5))  # use .select() not slicing - slicing returns a dict!\n",
    "        \n",
    "        def preprocess(example):\n",
    "            tokenized = tokenizer.apply_chat_template(\n",
    "                example[\"messages\"],\n",
    "                tokenize=True,\n",
    "                return_assistant_tokens_mask=True,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            return {\n",
    "                \"input_ids\": tokenized[\"input_ids\"],\n",
    "                \"assistant_masks\": tokenized[\"assistant_masks\"],\n",
    "            }\n",
    "        \n",
    "        tokenized_dataset = raw_dataset.map(preprocess, remove_columns=raw_dataset.column_names, num_proc=num_proc, desc=\"Tokenizing\")\n",
    "        def shorter_than(example):\n",
    "            return len(example[\"input_ids\"]) <= max_length\n",
    "        final_dataset = tokenized_dataset.filter(shorter_than, num_proc=num_proc, desc=f\"Filtering to max length {max_length}\")\n",
    "        print(f\"Tokenized: {len(tokenized_dataset)}, After filtering: {len(final_dataset)}\")\n",
    "        final_dataset.save_to_disk(local_ds_id)\n",
    "    return final_dataset\n",
    "\n",
    "\n",
    "def create_dataloader(tokenizer, tokenized_dataset, batch_size):\n",
    "    collator = DataCollatorForLanguageModeling(pad_token_id=tokenizer.pad_token_id,)\n",
    "    dataloader = DataLoader(\n",
    "        tokenized_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collator,\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "def add_reasoning_chat_template(tokenizer):\n",
    "    if \"qwen\" in tokenizer.name_or_path.lower():\n",
    "        # we have to use DataCollatorForLanguageModeling with completion_only_loss=True\n",
    "        # however, for that tokenizer needs to have return_assistant_tokens_mask=True, and qwen decided against adding support for {% generation %} / {% endgeneration %} functionality\n",
    "        # so we download a community qwen3 chat template that has it\n",
    "        !wget -O all_assistant.jinja --no-check-certificate https://raw.githubusercontent.com/HarryMayne/qwen_3_chat_templates/refs/heads/main/all_assistant.jinja\n",
    "        !mv all_assistant.jinja chat_templates/all_assistant.jinja\n",
    "        with open('chat_templates/all_assistant.jinja', 'r') as f:\n",
    "            tokenizer.chat_template = f.read()\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model/Dataset IDs, hyperparam choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_model_ids = [\n",
    "    \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    \"allenai/OLMo-2-0425-1B-Instruct\",\n",
    "    \"Qwen/Qwen3-0.6B\",\n",
    "    \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "]\n",
    "big_model_ids = [\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    \"allenai/OLMo-2-1124-7B-Instruct\",\n",
    "    \"Qwen/Qwen3-8B\",\n",
    "    \"HuggingFaceTB/SmolLM2-1.7B-Instruct\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"Neelectric/OpenR1-Math-220k_CN-K12_OLMo-2_4096toks\"\n",
    "device = \"mps\"\n",
    "model_id = small_model_ids[2]\n",
    "batch_size = 8\n",
    "max_length = 1024\n",
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model, tokenizer, dataset, dataloader, optimizer, LR scheduler, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in Qwen/Qwen3-0.6B\n",
      "--2025-12-27 16:13:26--  https://raw.githubusercontent.com/HarryMayne/qwen_3_chat_templates/refs/heads/main/all_assistant.jinja\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4153 (4.1K) [text/plain]\n",
      "Saving to: ‘all_assistant.jinja’\n",
      "\n",
      "all_assistant.jinja 100%[===================>]   4.06K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-12-27 16:13:26 (14.5 MB/s) - ‘all_assistant.jinja’ saved [4153/4153]\n",
      "\n",
      "Dataset not found locally, processing and caching...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c88064e22544f1f8ddc29dd1c02b9d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering to max length 1024 (num_proc=16):   0%|          | 0/69132 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized: 69132, After filtering: 4749\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eb3af5fd5be47b79101e7161bc372cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4749 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Loading in {model_id}\")\n",
    "model, tokenizer = load_model_and_tokenizer(model_id, device)\n",
    "tokenizer = add_reasoning_chat_template(tokenizer)\n",
    "tokenized_dataset = load_or_preprocess_dataset(model_id, dataset_id, tokenizer, max_length=max_length)\n",
    "dataloader = create_dataloader(tokenizer, tokenized_dataset, batch_size)\n",
    "num_training_steps = num_epochs * len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "594"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'labels', 'attention_mask'])\n",
      "torch.Size([8, 1018])\n",
      "  0 | 151644 |   -100 | <|im_start|>\n",
      "  1 |    872 |   -100 | user\n",
      "  2 |    198 |   -100 | \n",
      "\n",
      "  3 |  22043 |   -100 | Given\n",
      "  4 |    429 |   -100 |  that\n",
      "  5 |    279 |   -100 |  the\n",
      "  6 |  23033 |   -100 |  diameter\n",
      "  7 |    315 |   -100 |  of\n",
      "  8 |    264 |   -100 |  a\n",
      "  9 |  25366 |   -100 |  sphere\n",
      " 10 |    374 |   -100 |  is\n",
      " 11 |    220 |   -100 |  \n",
      " 12 |     19 |   -100 | 4\n",
      " 13 |     11 |   -100 | ,\n",
      " 14 |    279 |   -100 |  the\n",
      " 15 |   7329 |   -100 |  surface\n",
      " 16 |   3082 |   -100 |  area\n",
      " 17 |    315 |   -100 |  of\n",
      " 18 |    279 |   -100 |  the\n",
      " 19 |  25366 |   -100 |  sphere\n",
      " 20 |    374 |   -100 |  is\n",
      " 21 |   1124 |   -100 |  \\\n",
      " 22 |  56014 |   -100 | _\\\n",
      " 23 |  56014 |   -100 | _\\\n",
      " 24 |  56014 |   -100 | _\\\n",
      " 25 |  56014 |   -100 | _\\\n",
      " 26 |  56014 |   -100 | _\\\n",
      " 27 |   4950 |   -100 | _.\n",
      " 28 | 151645 |   -100 | <|im_end|>\n",
      " 29 |    198 |   -100 | \n",
      "\n",
      " 30 | 151644 |   -100 | <|im_start|>\n",
      " 31 |  77091 |   -100 | assistant\n",
      " 32 |    198 |   -100 | \n",
      "\n",
      " 33 | 151667 | 151667 | <think>\n",
      " 34 |    198 |    198 | \n",
      "\n",
      " 35 |  32313 |  32313 | Okay\n",
      " 36 |     11 |     11 | ,\n",
      " 37 |    773 |    773 |  so\n",
      " 38 |    358 |    358 |  I\n",
      " 39 |   1184 |   1184 |  need\n",
      " 40 |    311 |    311 |  to\n",
      " 41 |   1477 |   1477 |  find\n",
      " 42 |    279 |    279 |  the\n",
      " 43 |   7329 |   7329 |  surface\n",
      " 44 |   3082 |   3082 |  area\n",
      " 45 |    315 |    315 |  of\n",
      " 46 |    264 |    264 |  a\n",
      " 47 |  25366 |  25366 |  sphere\n",
      " 48 |    979 |    979 |  when\n",
      " 49 |   1181 |   1181 |  its\n",
      " 50 |  23033 |  23033 |  diameter\n",
      " 51 |    374 |    374 |  is\n",
      " 52 |    220 |    220 |  \n",
      " 53 |     19 |     19 | 4\n",
      " 54 |     13 |     13 | .\n",
      " 55 |  88190 |  88190 |  Hmm\n",
      " 56 |     11 |     11 | ,\n",
      " 57 |   1077 |   1077 |  let\n",
      " 58 |    594 |    594 | 's\n",
      " 59 |   1191 |   1191 |  start\n",
      " 60 |    553 |    553 |  by\n",
      " 61 |  88646 |  88646 |  recalling\n",
      " 62 |    279 |    279 |  the\n",
      " 63 |  14806 |  14806 |  formula\n",
      " 64 |    369 |    369 |  for\n",
      " 65 |    279 |    279 |  the\n",
      " 66 |   7329 |   7329 |  surface\n",
      " 67 |   3082 |   3082 |  area\n",
      " 68 |    315 |    315 |  of\n",
      " 69 |    264 |    264 |  a\n",
      " 70 |  25366 |  25366 |  sphere\n",
      " 71 |     13 |     13 | .\n",
      " 72 |    358 |    358 |  I\n",
      " 73 |   1744 |   1744 |  think\n",
      " 74 |    432 |    432 |  it\n",
      " 75 |    594 |    594 | 's\n",
      " 76 |   2494 |   2494 |  something\n",
      " 77 |   1075 |   1075 |  like\n",
      " 78 |    220 |    220 |  \n",
      " 79 |     19 |     19 | 4\n",
      " 80 |  48245 |  48245 | π\n",
      " 81 |     81 |     81 | r\n",
      " 82 |  29456 |  29456 | ²\n",
      " 83 |     11 |     11 | ,\n",
      " 84 |   1290 |   1290 |  right\n",
      " 85 |     30 |     30 | ?\n",
      " 86 |  21607 |  21607 |  Yeah\n",
      " 87 |     11 |     11 | ,\n",
      " 88 |    429 |    429 |  that\n",
      " 89 |  10362 |  10362 |  sounds\n",
      " 90 |  11285 |  11285 |  familiar\n",
      " 91 |     13 |     13 | .\n",
      " 92 |   2055 |   2055 |  So\n",
      " 93 |   7329 |   7329 |  surface\n",
      " 94 |   3082 |   3082 |  area\n",
      " 95 |  16819 |  16819 |  equals\n",
      " 96 |   3040 |   3040 |  four\n",
      " 97 |   8938 |   8938 |  pi\n",
      " 98 |   3039 |   3039 |  times\n",
      " 99 |    279 |    279 |  the\n",
      "100 |  10578 |  10578 |  radius\n",
      "101 |  52263 |  52263 |  squared\n",
      "102 |     13 |     13 | .\n",
      "103 |   4710 |   4710 |  \n",
      "\n",
      "\n",
      "104 |  14190 |  14190 | Wait\n",
      "105 |     11 |     11 | ,\n",
      "106 |    714 |    714 |  but\n",
      "107 |    279 |    279 |  the\n",
      "108 |   3491 |   3491 |  problem\n",
      "109 |   6696 |   6696 |  gives\n",
      "110 |    752 |    752 |  me\n",
      "111 |    279 |    279 |  the\n",
      "112 |  23033 |  23033 |  diameter\n",
      "113 |     11 |     11 | ,\n",
      "114 |    892 |    892 |  which\n",
      "115 |    374 |    374 |  is\n",
      "116 |    220 |    220 |  \n",
      "117 |     19 |     19 | 4\n",
      "118 |     13 |     13 | .\n",
      "119 |   2055 |   2055 |  So\n",
      "120 |    279 |    279 |  the\n",
      "121 |  10578 |  10578 |  radius\n",
      "122 |    374 |    374 |  is\n",
      "123 |   4279 |   4279 |  half\n",
      "124 |    315 |    315 |  of\n",
      "125 |    279 |    279 |  the\n",
      "126 |  23033 |  23033 |  diameter\n",
      "127 |     11 |     11 | ,\n",
      "128 |    773 |    773 |  so\n",
      "129 |    429 |    429 |  that\n",
      "130 |   1035 |   1035 |  would\n",
      "131 |    387 |    387 |  be\n",
      "132 |    220 |    220 |  \n",
      "133 |     19 |     19 | 4\n",
      "134 |  17779 |  17779 |  divided\n",
      "135 |    553 |    553 |  by\n",
      "136 |    220 |    220 |  \n",
      "137 |     17 |     17 | 2\n",
      "138 |     13 |     13 | .\n",
      "139 |   6771 |   6771 |  Let\n",
      "140 |    752 |    752 |  me\n",
      "141 |   3270 |   3270 |  write\n",
      "142 |    429 |    429 |  that\n",
      "143 |   1495 |   1495 |  down\n",
      "144 |     13 |     13 | .\n",
      "145 |  40453 |  40453 |  Radius\n",
      "146 |    435 |    435 |  r\n",
      "147 |    284 |    284 |  =\n",
      "148 |  23033 |  23033 |  diameter\n",
      "149 |    608 |    608 |  /\n",
      "150 |    220 |    220 |  \n",
      "151 |     17 |     17 | 2\n",
      "152 |    284 |    284 |  =\n",
      "153 |    220 |    220 |  \n",
      "154 |     19 |     19 | 4\n",
      "155 |    608 |    608 |  /\n",
      "156 |    220 |    220 |  \n",
      "157 |     17 |     17 | 2\n",
      "158 |    284 |    284 |  =\n",
      "159 |    220 |    220 |  \n",
      "160 |     17 |     17 | 2\n",
      "161 |     13 |     13 | .\n",
      "162 |   2055 |   2055 |  So\n",
      "163 |    279 |    279 |  the\n",
      "164 |  10578 |  10578 |  radius\n",
      "165 |    374 |    374 |  is\n",
      "166 |    220 |    220 |  \n",
      "167 |     17 |     17 | 2\n",
      "168 |   8153 |   8153 |  units\n",
      "169 |    382 |    382 | .\n",
      "\n",
      "\n",
      "170 |  71486 |  71486 | Alright\n",
      "171 |     11 |     11 | ,\n",
      "172 |   1431 |   1431 |  now\n",
      "173 |  19633 |  19633 |  plug\n",
      "174 |    429 |    429 |  that\n",
      "175 |   1119 |   1119 |  into\n",
      "176 |    279 |    279 |  the\n",
      "177 |   7329 |   7329 |  surface\n",
      "178 |   3082 |   3082 |  area\n",
      "179 |  14806 |  14806 |  formula\n",
      "180 |     13 |     13 | .\n",
      "181 |   2055 |   2055 |  So\n",
      "182 |  31334 |  31334 |  substit\n",
      "183 |  10607 |  10607 | uting\n",
      "184 |    435 |    435 |  r\n",
      "185 |    284 |    284 |  =\n",
      "186 |    220 |    220 |  \n",
      "187 |     17 |     17 | 2\n",
      "188 |   1119 |   1119 |  into\n",
      "189 |    220 |    220 |  \n",
      "190 |     19 |     19 | 4\n",
      "191 |  48245 |  48245 | π\n",
      "192 |     81 |     81 | r\n",
      "193 |  29456 |  29456 | ²\n",
      "194 |     11 |     11 | ,\n",
      "195 |    582 |    582 |  we\n",
      "196 |    633 |    633 |  get\n",
      "197 |    220 |    220 |  \n",
      "198 |     19 |     19 | 4\n",
      "199 |  48245 |  48245 | π\n",
      "200 |   6599 |   6599 | *(\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(dataloader))\n",
    "print(batch.keys())  # should have input_ids, attention_mask, labels\n",
    "print(batch[\"input_ids\"].shape)\n",
    "idx = 0\n",
    "for i, (tok, label) in enumerate(zip(batch[\"input_ids\"][idx], batch[\"labels\"][idx])):\n",
    "    print(f\"{i:3d} | {tok:6d} | {label:6d} | {tokenizer.decode([tok])}\")\n",
    "    if i == 200: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([8, 1000]),\n",
       " 'labels': torch.Size([8, 1000]),\n",
       " 'attention_mask': torch.Size([8, 1000])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(dataloader))\n",
    "batch_shapes = {k: v.shape for k, v in batch.items()}\n",
    "batch_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1782"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0.05,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "num_training_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_sft():\n",
    "    model.train()\n",
    "    epoch = 1\n",
    "    # for epoch in tqdm(range(num_epochs), desc=\"Epochs\", dynamic_ncols=True):\n",
    "    for batch in tqdm(dataloader, desc=\"Steps in Epoch\", dynamic_ncols=True):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        tqdm.write(f\"Epoch {epoch}, loss {loss.to('cpu')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps in Epoch:   0%|          | 1/594 [00:07<1:14:42,  7.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss 0.6378394961357117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps in Epoch:   0%|          | 2/594 [00:14<1:08:13,  6.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss 0.73519366979599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps in Epoch:   1%|          | 3/594 [00:22<1:16:03,  7.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss 1.1045308113098145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps in Epoch:   1%|          | 4/594 [00:30<1:16:22,  7.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss 0.9211974740028381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps in Epoch:   1%|          | 5/594 [01:19<3:41:58, 22.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss 0.9282127022743225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps in Epoch:   1%|          | 6/594 [02:43<7:06:04, 43.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss 0.6994381546974182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps in Epoch:   1%|          | 7/594 [04:19<9:54:36, 60.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss 0.8320025205612183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps in Epoch:   1%|▏         | 8/594 [05:25<10:09:46, 62.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss 0.8460947275161743\n"
     ]
    }
   ],
   "source": [
    "train_with_sft()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final eval of methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openr1_v3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
