{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiments with Fisher\n",
        "\n",
        "This whole script takes ~45mins to run with an H100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('hello')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'openr1_v2 (Python 3.12.12)' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/pvc/repos/open-r1_safety/openr1_v2/bin/python -m pip install ipykernel -U --force-reinstall'"
          ]
        }
      ],
      "source": [
        "!uv pip install ipykernel jupyter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup 'Update' and 'Protect' texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import copy\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from tqdm.notebook import tqdm # this makes tqdm.write() work with notebooks!\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, get_scheduler\n",
        "from datasets import load_dataset, load_from_disk\n",
        "\n",
        "from trl.trainer.sft_trainer import DataCollatorForLanguageModeling\n",
        "from copy import deepcopy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_generation_chat_template(tokenizer):\n",
        "    if \"qwen\" in tokenizer.name_or_path.lower():\n",
        "        # we have to use DataCollatorForLanguageModeling with completion_only_loss=True\n",
        "        # however, for that tokenizer needs to have return_assistant_tokens_mask=True, and qwen decided against adding support for {% generation %} / {% endgeneration %} functionality\n",
        "        # so we download a community qwen3 chat template that has it\n",
        "        !wget -O all_assistant.jinja --no-check-certificate https://raw.githubusercontent.com/HarryMayne/qwen_3_chat_templates/refs/heads/main/all_assistant.jinja\n",
        "        !mv all_assistant.jinja chat_templates/qwen_all_assistant.jinja\n",
        "        with open('chat_templates/qwen_all_assistant.jinja', 'r') as f:\n",
        "            tokenizer.chat_template = f.read()\n",
        "    if \"smollm2\" in tokenizer.name_or_path.lower():\n",
        "        with open('chat_templates/smollm2_all_assistant.jinja', 'r') as f:\n",
        "            tokenizer.chat_template = f.read()\n",
        "    if \"llama\" in tokenizer.name_or_path.lower():\n",
        "        with open('chat_templates/llama3_all_assistant.jinja', 'r') as f:\n",
        "            tokenizer.chat_template = f.read()\n",
        "    return tokenizer\n",
        "\n",
        "def load_or_preprocess_dataset(model_id, dataset_id, tokenizer, max_length=4096):\n",
        "    local_ds_id = f\"datasets/{model_id}/{dataset_id}\"\n",
        "    num_proc = 16\n",
        "    if True:\n",
        "        print(f\"Dataset not found locally, processing and caching...\")\n",
        "        raw_dataset = load_dataset(dataset_id)[\"train\"]\n",
        "        def preprocess(example):\n",
        "            tokenized = tokenizer.apply_chat_template(\n",
        "                example[\"messages\"],\n",
        "                tokenize=True,\n",
        "                return_assistant_tokens_mask=True,\n",
        "                return_dict=True,\n",
        "            )\n",
        "            return {\n",
        "                \"input_ids\": tokenized[\"input_ids\"],\n",
        "                \"assistant_masks\": tokenized[\"assistant_masks\"],\n",
        "            }\n",
        "        \n",
        "        tokenized_dataset = raw_dataset.map(preprocess, remove_columns=raw_dataset.column_names, num_proc=num_proc, desc=\"Tokenizing\")\n",
        "        def shorter_than(example):\n",
        "            return len(example[\"input_ids\"]) <= max_length\n",
        "        final_dataset = tokenized_dataset.filter(shorter_than, num_proc=num_proc, desc=f\"Filtering to max length {max_length}\")\n",
        "        print(f\"Tokenized: {len(tokenized_dataset)}, After filtering: {len(final_dataset)}\")\n",
        "        final_dataset.save_to_disk(local_ds_id)\n",
        "    return final_dataset\n",
        "\n",
        "def create_dataloader(tokenized_dataset, batch_size):\n",
        "    collator = DataCollatorForLanguageModeling(pad_token_id=tokenizer.pad_token_id, completion_only_loss=True)\n",
        "    dataloader = DataLoader(\n",
        "        tokenized_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=collator,\n",
        "    )\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import set_seed\n",
        "random_seed = 42\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "small_model_ids = [\n",
        "    \"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "    \"allenai/OLMo-2-0425-1B-Instruct\",\n",
        "    \"Qwen/Qwen3-0.6B\",\n",
        "    \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
        "]\n",
        "big_model_ids = [\n",
        "    \"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "    \"allenai/OLMo-2-1124-7B-Instruct\",\n",
        "    \"Qwen/Qwen3-8B\",\n",
        "    \"HuggingFaceTB/SmolLM2-1.7B-Instruct\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "--2026-01-12 15:40:45--  https://raw.githubusercontent.com/HarryMayne/qwen_3_chat_templates/refs/heads/main/all_assistant.jinja\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4153 (4.1K) [text/plain]\n",
            "Saving to: ‘all_assistant.jinja’\n",
            "\n",
            "all_assistant.jinja 100%[===================>]   4.06K  --.-KB/s    in 0s      \n",
            "\n",
            "2026-01-12 15:40:45 (31.9 MB/s) - ‘all_assistant.jinja’ saved [4153/4153]\n",
            "\n",
            "{%- if tools %}\n",
            "    {{- '<|im_start|>system\\n' }}\n",
            "    {%- if messages[0].role == 'system' %}\n",
            "        {{- messages[0].content + '\\n\\n' }}\n",
            "    {%- endif %}\n",
            "    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
            "    {%- for tool in tools %}\n",
            "        {{- \"\\n\" }}\n",
            "        {{- tool | tojson }}\n",
            "    {%- endfor %}\n",
            "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
            "{%- else %}\n",
            "    {%- if messages[0].role == 'system' %}\n",
            "        {{- '<|im_start|>system\\n' + messages[0].content + '<|im_end|>\\n' }}\n",
            "    {%- endif %}\n",
            "{%- endif %}\n",
            "{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n",
            "{%- for message in messages[::-1] %}\n",
            "    {%- set index = (messages|length - 1) - loop.index0 %}\n",
            "    {%- if ns.multi_step_tool and message.role == \"user\" and message.content is string and not(message.content.startswith('<tool_response>') and message.content.endswith('</tool_response>')) %}\n",
            "        {%- set ns.multi_step_tool = false %}\n",
            "        {%- set ns.last_query_index = index %}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "{%- for message in messages %}\n",
            "    {%- if message.content is string %}\n",
            "        {%- set content = message.content %}\n",
            "    {%- else %}\n",
            "        {%- set content = '' %}\n",
            "    {%- endif %}\n",
            "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n",
            "        {{- '<|im_start|>' + message.role + '\\n' + content + '<|im_end|>' + '\\n' }}\n",
            "    {%- elif message.role == \"assistant\" %}\n",
            "        {%- set reasoning_content = '' %}\n",
            "        {%- if message.reasoning_content is string %}\n",
            "            {%- set reasoning_content = message.reasoning_content %}\n",
            "        {%- else %}\n",
            "            {%- if '</think>' in content %}\n",
            "                {%- set reasoning_content = content.split('</think>')[0].rstrip('\\n').split('<think>')[-1].lstrip('\\n') %}\n",
            "                {%- set content = content.split('</think>')[-1].lstrip('\\n') %}\n",
            "            {%- endif %}\n",
            "        {%- endif %}\n",
            "\n",
            "        {{- '<|im_start|>' + message.role }}\n",
            "        {% generation %}\n",
            "        {%- if loop.index0 > ns.last_query_index %}\n",
            "            {%- if loop.last or (not loop.last and reasoning_content) %}\n",
            "                {{- '<think>\\n' + reasoning_content.strip('\\n') + '\\n</think>\\n\\n' + content.lstrip('\\n') }}\n",
            "            {%- else %}\n",
            "                {{- content }}\n",
            "            {%- endif %}\n",
            "        {%- else %}\n",
            "            {{- content }}\n",
            "        {%- endif %}\n",
            "        {%- if message.tool_calls %}\n",
            "            {%- for tool_call in message.tool_calls %}\n",
            "                {%- if (loop.first and content) or (not loop.first) %}\n",
            "                    {{- '\\n' }}\n",
            "                {%- endif %}\n",
            "                {%- if tool_call.function %}\n",
            "                    {%- set tool_call = tool_call.function %}\n",
            "                {%- endif %}\n",
            "                {{- '<tool_call>\\n{\"name\": \"' }}\n",
            "                {{- tool_call.name }}\n",
            "                {{- '\", \"arguments\": ' }}\n",
            "                {%- if tool_call.arguments is string %}\n",
            "                    {{- tool_call.arguments }}\n",
            "                {%- else %}\n",
            "                    {{- tool_call.arguments | tojson }}\n",
            "                {%- endif %}\n",
            "                {{- '}\\n</tool_call>' }}\n",
            "            {%- endfor %}\n",
            "        {%- endif %}\n",
            "        {{- '<|im_end|>' }}\n",
            "        {% endgeneration %}\n",
            "    {%- elif message.role == \"tool\" %}\n",
            "        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n",
            "            {{- '<|im_start|>user' }}\n",
            "        {%- endif %}\n",
            "        {{- '\\n<tool_response>\\n' }}\n",
            "        {{- content }}\n",
            "        {{- '\\n</tool_response>' }}\n",
            "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
            "            {{- '<|im_end|>\\n' }}\n",
            "        {%- endif %}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "{%- if add_generation_prompt %}\n",
            "    {{- '<|im_start|>assistant\\n' }}\n",
            "    {%- if enable_thinking is defined and enable_thinking is false %}\n",
            "        {{- '<think>\\n\\n</think>\\n\\n' }}\n",
            "    {%- endif %}\n",
            "{%- endif %}\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# model_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
        "model_name = small_model_ids[2]\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token  # for batching\n",
        "tokenizer = add_generation_chat_template(tokenizer)\n",
        "print(tokenizer.chat_template)\n",
        "\n",
        "batch_size = 4 #8 for smollm2, 1 for llama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset not found locally, processing and caching...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d516551a37654a75a7a17bbede3854ac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/880 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff6882cd63664941b94f1d7a451abdd7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00000-of-00007.parquet:   0%|          | 0.00/113M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "79f5462b6b9e4090849aae4bd65af3b9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00001-of-00007.parquet:   0%|          | 0.00/113M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2a36ce7512a74b3f8885200061b34fb4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00002-of-00007.parquet:   0%|          | 0.00/112M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "37aeea34f589410593cfc1f9f876d95c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00003-of-00007.parquet:   0%|          | 0.00/113M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7987744c723b4c53933bf2de200ae891",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00004-of-00007.parquet:   0%|          | 0.00/113M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "469a2e8f5494485b99cf16896acca183",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00005-of-00007.parquet:   0%|          | 0.00/113M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "38435787a10244ccac795620e4eb74ae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00006-of-00007.parquet:   0%|          | 0.00/113M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "08b9066b9d104b6d871beb7bc3ef74aa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/86158 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7bc809727a5c40099506e4e2948f10ae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing (num_proc=16):   0%|          | 0/86158 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "893e54670ac244fca54ebf42673e0114",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filtering to max length 1024 (num_proc=16):   0%|          | 0/86158 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized: 86158, After filtering: 5017\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "08e4f5f13b56430392bf9864c143a0d1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/5017 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "update_id = \"Neelectric/OpenR1-Math-220k_extended_Llama3_4096toks\"\n",
        "update_ds = load_or_preprocess_dataset(model_name, update_id, tokenizer, 1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset not found locally, processing and caching...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "10aca72384664ade92cffd43a12515cf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/523 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c2eec76ac0a34a3fb8a5b666f2277873",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00000-of-00001.parquet:   0%|          | 0.00/228M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3b12959ea7f040f7976ed1242e7b06bf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/86745 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "757c68dc7c3a4482bff65ea35d4d10bd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing (num_proc=16):   0%|          | 0/86745 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "562a0590fc7046dea160d0cd77a251ae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filtering to max length 1024 (num_proc=16):   0%|          | 0/86745 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized: 86745, After filtering: 78354\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bdbc5ee1888c415596620446d6c6cdf5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/78354 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "retain_id = \"Neelectric/wildguardmix_Llama-3.1-8B-Instruct_4096toks\"\n",
        "retain_ds = load_or_preprocess_dataset(model_name, retain_id, tokenizer, 1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2508\n",
            "2508\n"
          ]
        }
      ],
      "source": [
        "full_length = len(update_ds) //2\n",
        "print(full_length)\n",
        "update_ds = update_ds.select(range(full_length))\n",
        "retain_ds = retain_ds.select(range(full_length))\n",
        "print(len(retain_ds))\n",
        "update_ds = update_ds.shuffle(seed=random_seed)\n",
        "retain_ds = retain_ds.shuffle(seed=random_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322,
          "referenced_widgets": [
            "0a9cd86371154105b73bcf2ede9feea1",
            "4017aa81e3504205b797a47b4541d236",
            "1afded3de67843e89d9ce05dacf97cf6",
            "3d2e1581055140788c0638914354f8ed",
            "b07617e6f8d34273a5929b6ed5998120",
            "c27c084743f643cc972967bcb0eb5753",
            "6e69c53fb73d44caaf004f4a1baf5330",
            "4a197dbff6da43a6b445dfe17b2c2522",
            "412823a0f00e42059339c56fb4bd9c9f",
            "7bc5dd35c071495b82c20903d663b66c",
            "91a9aec8a55f4f418af506009da10f35",
            "d2e42c54ade243779ec71eb5fd4a51bf",
            "6099f135e0814a47822b3749934696b3",
            "3ee498aad5c54b4c956bb39619959b60",
            "5df1c3945e404094b3f18c099b502a84",
            "7dddedc6cfcb46848461c6aa231650ec",
            "a2f3c8562c204b1d9222851eed5f106f",
            "f521b4a4d7154de6ba16f3e6bdf59b51",
            "e6c8823a320447f7bae20c76e4070c0b",
            "8c9e710110a6423b84ade1d41b8a0529",
            "00a45ce331f049659ddc5a8d3a733689",
            "b7e41e3eac364027a5a1bf7342f7bd49",
            "c199e25df4d54001919f0481edc0b358",
            "e7c2de72aff4428db80fd3ddd8d82557",
            "cb45de6c1d7d4dc7809c42da301e9fba",
            "d8e0603c141f4674a1f57399cee4ef45",
            "8d5615ddaac748d7a1ba5a26726aa6fb",
            "4e9ea7427ef0479c9b0216fdb9604c8a",
            "7805724c2c5448d4b29119483f055636",
            "d928ee45c23f436a9aa89ca5ef53df92",
            "ff49ae10aa48498dbb0cc35ab638c298",
            "019d2a1857fe46099f9fc01c6ba6b8d7",
            "92bf0487d45e4c06869ae88ee0bb877c",
            "46ab261f4cc549e181eeae76c646abaa",
            "21357a168091432fbf46ec848df81842",
            "53dbd36dc7e4462f8d94a403a0d82875",
            "c1bac9dec2244a1e9eef085386d047f1",
            "ec9d2bfc9b82450199239de8eada416d",
            "b5f3adfc279949e58696997e532348ee",
            "4eb8fcc217cf4b9e98b63c09077a17ec",
            "4644d3f2f33d4f089ee5d788d8a2a0ef",
            "38706d9664be4357b79dd614fbac9b27",
            "8ebe0ad1bb4e456a97821b31d8dfb9f6",
            "4a2226e944a4410aa53f1717e07169b5",
            "19a30732e19745b8a333de9eb6332a41",
            "46a0746bf84440f38874b729d617f1df",
            "9de635c4e355447e8e2a4d9f760fdfde",
            "8998157212b8431ba8509dd0df98981b",
            "9921b84c7f784e7ba9f32a97602546a7",
            "f9182aedceb2421b9a77a245d20a9a1e",
            "95b18249d1884fa7895fb09393d2598b",
            "11481ce4d7c54a20b328fe987088d02b",
            "e97ed5800cff4055900545ff376979be",
            "675449c0ef484dc3b3baae5bbe8b5fd5",
            "65ac5143b332424ca9424506b20cd40a"
          ]
        },
        "id": "-KxrbhNtq7bc",
        "outputId": "bd4e7ec8-ef80-4cb7-d16a-808a85c8fd6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2006\n",
            "2006\n",
            "502\n",
            "2006\n",
            "502\n"
          ]
        }
      ],
      "source": [
        "# Train / test splits\n",
        "num_train = int(0.8 * full_length)\n",
        "print(num_train)\n",
        "retain_train_ds = retain_ds.select(range(num_train))\n",
        "retain_test_ds = retain_ds.select(range(num_train, full_length))\n",
        "print(len(retain_train_ds))\n",
        "print(len(retain_test_ds))\n",
        "\n",
        "update_train_ds = update_ds.select(range(num_train))\n",
        "update_test_ds = update_ds.select(range(num_train, full_length))\n",
        "print(len(update_train_ds))\n",
        "print(len(update_test_ds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynTogCxlD6qe",
        "outputId": "2fa89744-95db-4461-bd1f-e9bc372e1f99"
      },
      "outputs": [],
      "source": [
        "# block_size = 64\n",
        "# batch_size = 8\n",
        "\n",
        "# retain_train_ds  = LineByLineLMDataset(retain_train, tokenizer, block_size)\n",
        "# retain_test_ds   = LineByLineLMDataset(retain_test,  tokenizer, block_size)\n",
        "# update_train_ds = LineByLineLMDataset(update_train, tokenizer, block_size)\n",
        "# update_test_ds  = LineByLineLMDataset(update_test,  tokenizer, block_size)\n",
        "\n",
        "# update_loader = DataLoader(update_train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "# ba_loader  = DataLoader(retain_train_ds,  batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# len(update_train_ds), len(retain_train_ds)\n",
        "\n",
        "update_loader = create_dataloader(update_train_ds, batch_size)\n",
        "retain_loader = create_dataloader(update_test_ds, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3COzmCNrD8_u"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def eval_ppl(model, dataset, name, batch_size_eval=8, disable_tqdm=True):\n",
        "    model.eval()\n",
        "    loader = create_dataloader(dataset, batch_size_eval)\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "    for batch in tqdm(loader, disable=disable_tqdm):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        out = model(**batch)\n",
        "        loss = out.loss * batch[\"attention_mask\"].sum()\n",
        "        total_loss += loss.item()\n",
        "        total_tokens += batch[\"attention_mask\"].sum().item()\n",
        "    ppl = math.exp(total_loss / total_tokens)\n",
        "    print(f\"{name} perplexity: {ppl:.3f}\")\n",
        "    model.train()\n",
        "    return ppl\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Before any optim: ppl on train and protect before fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c73f896b01c84d499c2e048305d3c49b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/726 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "41e4e28b9c9741028cb03fb2fab544bb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.50G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c29651e563a5400f927f24d07d816b81",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "model = copy.deepcopy(base_model).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "25263c15bb9f445ab705390e733f29c4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/63 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline update before optim perplexity: 2.038\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4b558e919fce4ccc89696e1425bab759",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/63 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline retain before optim perplexity: 4.605\n"
          ]
        }
      ],
      "source": [
        "eng_ppl = eval_ppl(model, update_test_ds, \"Baseline update before optim\", batch_size_eval=8, disable_tqdm=False)\n",
        "ba_ppl = eval_ppl(model, retain_test_ds, \"Baseline retain before optim\", batch_size_eval=8, disable_tqdm=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527,
          "referenced_widgets": [
            "bbe725cd4bf4424490b1b6444fd128a2",
            "149b331d4d87433398ee419064405baa",
            "b0fa9161ed064445aece8d321b3d88a7",
            "cf32259002024b7da355cc4eb0a752e8",
            "503d3af66d1c4b47ac0f9fd4caf475ae",
            "c48cfdbb01a6438087a163d31de89005",
            "a8851e5f5d6f46e8860bc772fa5b81ea",
            "8ee340f1e44442aca9ad56eaf28e3ec6",
            "3c41a4d44ad349d2accb847f8ec65f41",
            "ce768476c2144ff58e413e5195528b39",
            "4f1ed9d96a5a4693ac360e9f90e42b8a",
            "8bdf857cc0fb474f9bf8c73fa0ea2257",
            "17a88ca67a994b51b5f425842c0ab3ff",
            "ecdf1063c4cc48cab41115fbda94cc85",
            "7681bec193b248ad8beca1bd01baf4c6",
            "1f6ddb9d63b442dcb6d2b772c5068212",
            "f7143d3bd72f4e1f9e119e1b29205cfb",
            "cf6bf77b10dc4e04920eb2c05b5a4347",
            "ed40a4f1820246b99ca964b03244c906",
            "a1fc267bd68947dfbafa9ed3e6fb56c9",
            "2846c79926f440708121d6b5efa2f901",
            "3adc4be8a5e442cc8c4d230d8ec3ef85"
          ]
        },
        "id": "YCnIJL95D_xR",
        "outputId": "e54bb78b-1a96-43cb-d896-271084ac0983"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before opt:\n",
            "Update new perplexity: 2.038\n",
            "Retain protected perplexity: 4.605\n",
            "=== Baseline Adam: train on Update only ===\n",
            "[Epoch 0 Step 100] loss_new = 1.2555\n",
            "[Epoch 0 Step 200] loss_new = 1.0377\n",
            "[Epoch 0 Step 300] loss_new = 1.2078\n",
            "[Epoch 0 Step 400] loss_new = 0.9986\n",
            "[Epoch 0 Step 500] loss_new = 0.9804\n",
            "Epoch 0 evaluation:\n",
            "Update new perplexity: 2.967\n",
            "Retain protected perplexity: 92.831\n",
            "[Epoch 1 Step 100] loss_new = 0.6489\n",
            "[Epoch 1 Step 200] loss_new = 0.8610\n",
            "[Epoch 1 Step 300] loss_new = 0.8533\n",
            "[Epoch 1 Step 400] loss_new = 0.7450\n",
            "[Epoch 1 Step 500] loss_new = 0.8817\n",
            "Epoch 1 evaluation:\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# SmolLM2 report mentions they use lr=3e-4 throughout SFT (page 8 section 5.2) https://arxiv.org/pdf/2502.02737\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m baseline_model = \u001b[43mrun_baseline_adam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3e-4\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mrun_baseline_adam\u001b[39m\u001b[34m(num_epochs, lr)\u001b[39m\n\u001b[32m     23\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] loss_new = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss.item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m evaluation:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     update_ppl = \u001b[43meval_ppl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_test_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mUpdate new\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     retain_ppl  = eval_ppl(model, retain_test_ds,  \u001b[33m\"\u001b[39m\u001b[33mRetain protected\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/openr1_v2/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36meval_ppl\u001b[39m\u001b[34m(model, dataset, name, batch_size_eval, disable_tqdm)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(loader, disable=disable_tqdm):\n\u001b[32m      8\u001b[39m     batch = {k: v.to(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch.items()}\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     out = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     loss = out.loss * batch[\u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m].sum()\n\u001b[32m     11\u001b[39m     total_loss += loss.item()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/openr1_v2/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/openr1_v2/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/openr1_v2/lib/python3.12/site-packages/transformers/utils/generic.py:918\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    920\u001b[39m     output = output.to_tuple()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/openr1_v2/lib/python3.12/site-packages/transformers/models/qwen3/modeling_qwen3.py:480\u001b[39m, in \u001b[36mQwen3ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    443\u001b[39m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[32m    444\u001b[39m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[32m    445\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    456\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    457\u001b[39m ) -> CausalLMOutputWithPast:\n\u001b[32m    458\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    459\u001b[39m \u001b[33;03m    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[32m    460\u001b[39m \u001b[33;03m        Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    478\u001b[39m \u001b[33;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[32m    479\u001b[39m \u001b[33;03m    ```\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m     outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    491\u001b[39m     hidden_states = outputs.last_hidden_state\n\u001b[32m    492\u001b[39m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/openr1_v2/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/openr1_v2/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/openr1_v2/lib/python3.12/site-packages/transformers/utils/generic.py:1072\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1069\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m   1071\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1072\u001b[39m     outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1074\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1075\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1076\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1077\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/openr1_v2/lib/python3.12/site-packages/transformers/models/qwen3/modeling_qwen3.py:410\u001b[39m, in \u001b[36mQwen3Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[39m\n\u001b[32m    407\u001b[39m position_embeddings = \u001b[38;5;28mself\u001b[39m.rotary_emb(hidden_states, position_ids)\n\u001b[32m    409\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers[: \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers]:\n\u001b[32m--> \u001b[39m\u001b[32m410\u001b[39m     hidden_states = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattention_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm(hidden_states)\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[32m    423\u001b[39m     last_hidden_state=hidden_states,\n\u001b[32m    424\u001b[39m     past_key_values=past_key_values \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    425\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/openr1_v2/lib/python3.12/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/openr1_v2/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/openr1_v2/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/openr1_v2/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/openr1_v2/lib/python3.12/site-packages/transformers/models/qwen3/modeling_qwen3.py:260\u001b[39m, in \u001b[36mQwen3DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    258\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m hidden_states, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    270\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    272\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/openr1_v2/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/openr1_v2/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/openr1_v2/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/openr1_v2/lib/python3.12/site-packages/transformers/models/qwen3/modeling_qwen3.py:210\u001b[39m, in \u001b[36mQwen3Attention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    208\u001b[39m     \u001b[38;5;66;03m# sin and cos are specific to RoPE models; cache_position needed for the static cache\u001b[39;00m\n\u001b[32m    209\u001b[39m     cache_kwargs = {\u001b[33m\"\u001b[39m\u001b[33msin\u001b[39m\u001b[33m\"\u001b[39m: sin, \u001b[33m\"\u001b[39m\u001b[33mcos\u001b[39m\u001b[33m\"\u001b[39m: cos, \u001b[33m\"\u001b[39m\u001b[33mcache_position\u001b[39m\u001b[33m\"\u001b[39m: cache_position}\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m     key_states, value_states = \u001b[43mpast_key_values\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    212\u001b[39m attention_interface: Callable = eager_attention_forward\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config._attn_implementation != \u001b[33m\"\u001b[39m\u001b[33meager\u001b[39m\u001b[33m\"\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/openr1_v2/lib/python3.12/site-packages/transformers/cache_utils.py:742\u001b[39m, in \u001b[36mCache.update\u001b[39m\u001b[34m(self, key_states, value_states, layer_idx, cache_kwargs)\u001b[39m\n\u001b[32m    739\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (only_non_sliding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_sliding[layer_idx]):\n\u001b[32m    740\u001b[39m         \u001b[38;5;28mself\u001b[39m.layers[layer_idx].offload()\n\u001b[32m--> \u001b[39m\u001b[32m742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mupdate\u001b[39m(\n\u001b[32m    743\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    744\u001b[39m     key_states: torch.Tensor,\n\u001b[32m    745\u001b[39m     value_states: torch.Tensor,\n\u001b[32m    746\u001b[39m     layer_idx: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m    747\u001b[39m     cache_kwargs: Optional[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    748\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor, torch.Tensor]:\n\u001b[32m    749\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    750\u001b[39m \u001b[33;03m    Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\u001b[39;00m\n\u001b[32m    751\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    764\u001b[39m \u001b[33;03m        A tuple containing the updated key and value states.\u001b[39;00m\n\u001b[32m    765\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    766\u001b[39m     \u001b[38;5;66;03m# In this case, the `layers` were not provided, and we must append as much as `layer_idx`\u001b[39;00m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "def run_baseline_adam(num_epochs=2, lr=1e-5):\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    model = copy.deepcopy(base_model).to(device)\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "    print(f\"Before opt:\")\n",
        "    update_ppl = eval_ppl(model, update_test_ds, \"Update new\")\n",
        "    retain_ppl  = eval_ppl(model, retain_test_ds,  \"Retain protected\")\n",
        "\n",
        "    print(\"=== Baseline Adam: train on Update only ===\")\n",
        "    for epoch in range(num_epochs):\n",
        "        for step, batch in enumerate(update_loader):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            model.zero_grad()\n",
        "            out = model(**batch)\n",
        "            loss = out.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if (step + 1) % 100 == 0:\n",
        "                print(f\"[Epoch {epoch} Step {step+1}] loss_new = {loss.item():.4f}\")\n",
        "\n",
        "        print(f\"Epoch {epoch} evaluation:\")\n",
        "        update_ppl = eval_ppl(model, update_test_ds, \"Update new\")\n",
        "        retain_ppl  = eval_ppl(model, retain_test_ds,  \"Retain protected\")\n",
        "    return model\n",
        "# SmolLM2 report mentions they use lr=3e-4 throughout SFT (page 8 section 5.2) https://arxiv.org/pdf/2502.02737\n",
        "baseline_model = run_baseline_adam(num_epochs=4, lr=3e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lz-u9NgkECO1",
        "outputId": "ed415c53-b8ac-47e4-822d-23c48409b63a"
      },
      "outputs": [],
      "source": [
        "def estimate_fisher_on_retain(model, num_batches=200):\n",
        "    model.eval()\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    fisher = {p: torch.zeros_like(p.data) for p in params}\n",
        "\n",
        "    # loader = DataLoader(retain_train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "    loader = create_dataloader(retain_train_ds, batch_size)\n",
        "    it = iter(loader)\n",
        "    for i in tqdm(range(num_batches)):\n",
        "        try:\n",
        "            batch = next(it)\n",
        "        except StopIteration:\n",
        "            it = iter(loader)\n",
        "            batch = next(it)\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        model.zero_grad()\n",
        "        out = model(**batch)\n",
        "        loss = out.loss\n",
        "        loss.backward()\n",
        "        for p in params:\n",
        "            if p.grad is None:\n",
        "                continue\n",
        "            fisher[p] += p.grad.data.pow(2)\n",
        "    for p in params:\n",
        "        fisher[p] /= num_batches\n",
        "    model.train()\n",
        "    return fisher\n",
        "\n",
        "def run_ewc(num_epochs=2, lr=5e-5, ewc_lambda=50.0):\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    model = copy.deepcopy(base_model).to(device)\n",
        "\n",
        "    print(\"Estimating Fisher on Retain (protected) ...\")\n",
        "    fisher = estimate_fisher_on_retain(model, num_batches=100)\n",
        "    theta0 = copy.deepcopy(model).to(device)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "\n",
        "    print(\"=== EWC: train on Update with Retain EWC penalty ===\")\n",
        "    for epoch in range(num_epochs):\n",
        "        for step, batch in enumerate(update_loader):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            model.zero_grad()\n",
        "            out = model(**batch)\n",
        "            loss_new = out.loss\n",
        "\n",
        "            ewc_loss = 0.0\n",
        "            for p, p0 in zip(params, theta0.parameters()):\n",
        "                ewc_loss = ewc_loss + (fisher[p] * (p - p0).pow(2)).sum()\n",
        "            total_loss = loss_new + 0.5 * ewc_lambda * ewc_loss\n",
        "\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if (step + 1) % 100 == 0:\n",
        "                print(f\"[Epoch {epoch} Step {step+1}] loss_new={loss_new.item():.4f}, ewc_loss={ewc_loss.item():.4f}\")\n",
        "\n",
        "        print(f\"Epoch {epoch} evaluation:\")\n",
        "        update_ppl = eval_ppl(model, update_test_ds, \"Update new (EWC)\")\n",
        "        ba_ppl  = eval_ppl(model, retain_test_ds,  \"Retain protected (EWC)\")\n",
        "    return model\n",
        "\n",
        "ewc_model = run_ewc(num_epochs=3, lr=3e-4, ewc_lambda=50.0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vS3js6UEEs8",
        "outputId": "50c1adf8-2795-458a-bc1a-6ff40a432ec3"
      },
      "outputs": [],
      "source": [
        "def run_protected_adam(\n",
        "    num_epochs=2,\n",
        "    lr=5e-5,\n",
        "    alpha_geom=1.0,\n",
        "    beta_geom=10.0,\n",
        "    gamma_exp=0.5,\n",
        "    subset_update_every=5,\n",
        "    rho_all=0.99,\n",
        "    rho_sub=0.99,\n",
        "):\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    model = copy.deepcopy(base_model).to(device)\n",
        "    model.train()\n",
        "\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "\n",
        "    state = {}\n",
        "    for p in params:\n",
        "        state[p] = {\n",
        "            \"m\": torch.zeros_like(p.data),\n",
        "            \"v_all\": torch.zeros_like(p.data),\n",
        "            \"v_sub\": torch.zeros_like(p.data),\n",
        "        }\n",
        "\n",
        "    beta1 = 0.9\n",
        "    eps = 1e-6\n",
        "    global_step = 0\n",
        "\n",
        "    def protected_adam_step():\n",
        "        nonlocal global_step\n",
        "        global_step += 1\n",
        "        for p in params:\n",
        "            if p.grad is None:\n",
        "                continue\n",
        "            grad = p.grad.data\n",
        "            s = state[p]\n",
        "\n",
        "            # first moment\n",
        "            s[\"m\"].mul_(beta1).add_(grad, alpha=1 - beta1)\n",
        "\n",
        "            # second moment on \"all\" (new Update) data\n",
        "            s[\"v_all\"].mul_(rho_all).addcmul_(grad, grad, value=1 - rho_all)\n",
        "\n",
        "            v_all = s[\"v_all\"]\n",
        "            v_sub = s[\"v_sub\"]\n",
        "            v_protect = alpha_geom * v_all + beta_geom * v_sub\n",
        "\n",
        "            m_hat = s[\"m\"] / (1 - beta1**global_step)\n",
        "            denom = (v_protect + eps).pow(gamma_exp)\n",
        "            step = m_hat / denom\n",
        "            p.data.add_(step, alpha=-lr)\n",
        "\n",
        "    def update_subset_curvature():\n",
        "        for p in params:\n",
        "            if p.grad is None:\n",
        "                continue\n",
        "            grad = p.grad.data\n",
        "            s = state[p]\n",
        "            s[\"v_sub\"].mul_(rho_sub).addcmul_(grad, grad, value=1 - rho_sub)\n",
        "\n",
        "    retain_iter = iter(retain_loader)\n",
        "\n",
        "    print(\"=== ProtectedAdam-γ: geometry shaped by Retain subset ===\")\n",
        "    print(f\"alpha_geom={alpha_geom}, beta_geom={beta_geom}, gamma_exp={gamma_exp}\")\n",
        "    for epoch in range(num_epochs):\n",
        "        for step, batch in enumerate(update_loader):\n",
        "            # 1) Update batch: gradient for new task\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            model.zero_grad()\n",
        "            out = model(**batch)\n",
        "            loss = out.loss\n",
        "            loss.backward()\n",
        "\n",
        "            # 2) Take ProtectedAdam step (updates v_all + params)\n",
        "            protected_adam_step()\n",
        "\n",
        "            # 3) Occasionally update subset curvature using Retain\n",
        "            if (step + 1) % subset_update_every == 0:\n",
        "                try:\n",
        "                    retain_batch = next(retain_iter)\n",
        "                except StopIteration:\n",
        "                    retain_iter = iter(retain_loader)\n",
        "                    retain_batch = next(retain_iter)\n",
        "                retain_batch = {k: v.to(device) for k, v in retain_batch.items()}\n",
        "                model.zero_grad()\n",
        "                retain_out = model(**retain_batch)\n",
        "                retain_loss = retain_out.loss\n",
        "                retain_loss.backward()\n",
        "                update_subset_curvature()\n",
        "                model.zero_grad()\n",
        "\n",
        "            if (step + 1) % 100 == 0:\n",
        "                print(f\"[Epoch {epoch} Step {step+1}] loss_new = {loss.item():.4f}\")\n",
        "\n",
        "        print(f\"Epoch {epoch} evaluation:\")\n",
        "        update_ppl = eval_ppl(model, update_test_ds, \"Update new (ProtectedAdam-γ)\")\n",
        "        retain_ppl  = eval_ppl(model, retain_test_ds,  \"Retain protected (ProtectedAdam-γ)\")\n",
        "\n",
        "    return model\n",
        "\n",
        "protected_model = run_protected_adam(\n",
        "    num_epochs=3,\n",
        "    lr=3e-4,\n",
        "    alpha_geom=1.0,\n",
        "    beta_geom=10.0,   # strength of protected geometry\n",
        "    gamma_exp=0.5,    # between 0.5 (Adam) and 1.0 (diag NGD)\n",
        "    subset_update_every=5,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHTG9YTgOOAf",
        "outputId": "f4d4a82a-4533-46c7-c6a2-cbc5f86580ad"
      },
      "outputs": [],
      "source": [
        "def run_protected_adam2(\n",
        "    num_epochs=3,\n",
        "    lr=5e-5,\n",
        "    alpha_geom=1.0,\n",
        "    beta_geom=10.0,\n",
        "    gamma_exp=0.5,\n",
        "    subset_update_every=5,\n",
        "    rho_all=0.99,\n",
        "    rho_sub=0.99,\n",
        "):\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    model = copy.deepcopy(base_model).to(device)\n",
        "    model.train()\n",
        "\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "\n",
        "    state = {}\n",
        "    for p in params:\n",
        "        state[p] = {\n",
        "            \"m\": torch.zeros_like(p.data),\n",
        "            \"v_all\": torch.zeros_like(p.data),\n",
        "            \"v_sub\": torch.zeros_like(p.data),\n",
        "        }\n",
        "\n",
        "    beta1 = 0.9\n",
        "    eps = 1e-6\n",
        "    global_step = 0\n",
        "\n",
        "    def protected_adam_step():\n",
        "        nonlocal global_step\n",
        "        global_step += 1\n",
        "\n",
        "        # First pass: update moments, compute v_protect, and accumulate\n",
        "        # the mean denominators for γ=0.5 (baseline) and γ=gamma_exp\n",
        "        temp = {}\n",
        "        sum_baseline = 0.0\n",
        "        sum_gamma = 0.0\n",
        "        count_tensors = 0\n",
        "\n",
        "        for p in params:\n",
        "            if p.grad is None:\n",
        "                continue\n",
        "            grad = p.grad.data\n",
        "            s = state[p]\n",
        "\n",
        "            # First moment\n",
        "            s[\"m\"].mul_(beta1).add_(grad, alpha=1 - beta1)\n",
        "\n",
        "            # Second moment on \"all\" (new Update) data\n",
        "            s[\"v_all\"].mul_(rho_all).addcmul_(grad, grad, value=1 - rho_all)\n",
        "\n",
        "            v_all = s[\"v_all\"]\n",
        "            v_sub = s[\"v_sub\"]\n",
        "            v_protect = alpha_geom * v_all + beta_geom * v_sub\n",
        "\n",
        "            # Bias-corrected first moment (optional but keeps Adam-like behaviour)\n",
        "            m_hat = s[\"m\"] / (1 - beta1**global_step)\n",
        "\n",
        "            denom_baseline = (v_protect + eps).pow(0.5)\n",
        "            denom_gamma = (v_protect + eps).pow(gamma_exp)\n",
        "\n",
        "            sum_baseline += denom_baseline.mean()\n",
        "            sum_gamma += denom_gamma.mean()\n",
        "            count_tensors += 1\n",
        "\n",
        "            temp[p] = {\n",
        "                \"m_hat\": m_hat,\n",
        "                \"v_protect\": v_protect,\n",
        "            }\n",
        "\n",
        "        if count_tensors == 0:\n",
        "            return\n",
        "\n",
        "        # Renormalization factor so that average step size matches γ=0.5 case\n",
        "        scale = (sum_baseline / sum_gamma).detach()\n",
        "\n",
        "        # Second pass: apply update with renormalized step size\n",
        "        for p in params:\n",
        "            if p.grad is None or p not in temp:\n",
        "                continue\n",
        "            buf = temp[p]\n",
        "            m_hat = buf[\"m_hat\"]\n",
        "            v_protect = buf[\"v_protect\"]\n",
        "\n",
        "            denom_gamma = (v_protect + eps).pow(gamma_exp)\n",
        "            step = (m_hat / denom_gamma) * scale\n",
        "            p.data.add_(step, alpha=-lr)\n",
        "\n",
        "    def update_subset_curvature():\n",
        "        for p in params:\n",
        "            if p.grad is None:\n",
        "                continue\n",
        "            grad = p.grad.data\n",
        "            s = state[p]\n",
        "            s[\"v_sub\"].mul_(rho_sub).addcmul_(grad, grad, value=1 - rho_sub)\n",
        "\n",
        "    retain_iter = iter(retain_loader)\n",
        "\n",
        "    print(f\"Before opt:\")\n",
        "    update_ppl = eval_ppl(model, update_test_ds, \"Update new (ProtectedAdam-γ)\")\n",
        "    retain_ppl  = eval_ppl(model, retain_test_ds,  \"Retain protected (ProtectedAdam-γ)\")\n",
        "\n",
        "\n",
        "    print(\"=== ProtectedAdam-γ: geometry shaped by Retain subset ===\")\n",
        "    print(f\"alpha_geom={alpha_geom}, beta_geom={beta_geom}, gamma_exp={gamma_exp}\")\n",
        "    for epoch in range(num_epochs):\n",
        "        for step, batch in enumerate(update_loader):\n",
        "            # 1) Update batch: gradient for new task\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            model.zero_grad()\n",
        "            out = model(**batch)\n",
        "            loss = out.loss\n",
        "            loss.backward()\n",
        "\n",
        "            # 2) Take ProtectedAdam step (updates v_all + params)\n",
        "            protected_adam_step()\n",
        "\n",
        "            # 3) Occasionally update subset curvature using Retain\n",
        "            if (step + 1) % subset_update_every == 0:\n",
        "                try:\n",
        "                    retain_batch = next(retain_iter)\n",
        "                except StopIteration:\n",
        "                    retain_iter = iter(retain_loader)\n",
        "                    retain_batch = next(retain_iter)\n",
        "                retain_batch = {k: v.to(device) for k, v in retain_batch.items()}\n",
        "                model.zero_grad()\n",
        "                retain_out = model(**retain_batch)\n",
        "                retain_loss = retain_out.loss\n",
        "                retain_loss.backward()\n",
        "                update_subset_curvature()\n",
        "                model.zero_grad()\n",
        "\n",
        "            if (step + 1) % 100 == 0:\n",
        "                print(f\"[Epoch {epoch} Step {step+1}] loss_new = {loss.item():.4f}\")\n",
        "\n",
        "        print(f\"Epoch {epoch} evaluation:\")\n",
        "        update_ppl = eval_ppl(model, update_test_ds, \"Update new (ProtectedAdam-γ)\")\n",
        "        retain_ppl  = eval_ppl(model, retain_test_ds,  \"Retain protected (ProtectedAdam-γ)\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "protected_model2 = run_protected_adam2(\n",
        "    num_epochs=3,\n",
        "    lr=3e-4,\n",
        "    alpha_geom=1.0,\n",
        "    beta_geom=10.0,   # strength of protected geometry\n",
        "    gamma_exp=0.5,    # between 0.5 (Adam) and 1.0 (diag NGD)\n",
        "    subset_update_every=5,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZnYsSOoViX2",
        "outputId": "3a83e68c-b724-4a8a-8b25-c1b0b3bb8e87"
      },
      "outputs": [],
      "source": [
        "def run_replay(\n",
        "    num_epochs=2,\n",
        "    lr=5e-5,\n",
        "    subset_update_every=5,\n",
        "    replay_weight=1.0,   # λ: strength of Retain replay loss\n",
        "):\n",
        "    \"\"\"\n",
        "    Experience Replay baseline.\n",
        "\n",
        "    - Optimizes Update CE loss every step.\n",
        "    - Every `subset_update_every` steps, also optimizes Retain CE.\n",
        "    - Total loss = CE_update + replay_weight * CE_Retain.\n",
        "    - Uses plain AdamW.\n",
        "    - No curvature, no shielding, no geometry.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load model\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    model = copy.deepcopy(base_model).to(device)\n",
        "    model.train()\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "    # Retain iterator for replay\n",
        "    retain_iter = iter(retain_loader)\n",
        "\n",
        "    print(\"=== Replay baseline: Update training + Retain replay ===\")\n",
        "    print(f\"subset_update_every={subset_update_every}, replay_weight={replay_weight}\")\n",
        "\n",
        "    print(\"Before opt:\")\n",
        "    eval_ppl(model, update_test_ds, \"Update new (replay)\")\n",
        "    eval_ppl(model, retain_test_ds,  \"Retain protected (replay)\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for step, batch in enumerate(update_loader):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            # Update forward/backward\n",
        "            model.zero_grad()\n",
        "            out = model(**batch)\n",
        "            loss_new = out.loss\n",
        "            total_loss = loss_new\n",
        "\n",
        "            # Retain replay every N steps\n",
        "            if (step + 1) % subset_update_every == 0:\n",
        "                try:\n",
        "                    ba_batch = next(retain_iter)\n",
        "                except StopIteration:\n",
        "                    retain_iter = iter(retain_loader)\n",
        "                    ba_batch = next(retain_iter)\n",
        "                ba_batch = {k: v.to(device) for k, v in ba_batch.items()}\n",
        "\n",
        "                ba_out = model(**ba_batch)\n",
        "                ba_loss = ba_out.loss\n",
        "\n",
        "                total_loss = loss_new + replay_weight * ba_loss\n",
        "\n",
        "            # Backprop + update\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Logging\n",
        "            if (step + 1) % 100 == 0:\n",
        "                if (step + 1) % subset_update_every == 0:\n",
        "                    print(\n",
        "                        f\"[Epoch {epoch} Step {step+1}] \"\n",
        "                        f\"loss_new={loss_new.item():.4f}, \"\n",
        "                        f\"loss_replay={ba_loss.item():.4f}, \"\n",
        "                        f\"total={total_loss.item():.4f}\"\n",
        "                    )\n",
        "                else:\n",
        "                    print(f\"[Epoch {epoch} Step {step+1}] loss_new={loss_new.item():.4f}\")\n",
        "\n",
        "        # End epoch eval\n",
        "        print(f\"Epoch {epoch} evaluation:\")\n",
        "        eval_ppl(model, update_test_ds, \"Update new (replay)\")\n",
        "        eval_ppl(model, retain_test_ds,  \"Retain protected (replay)\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "replay_model = run_replay(\n",
        "    num_epochs=3,\n",
        "    lr=3e-4,\n",
        "    subset_update_every=5,\n",
        "    replay_weight=1.0,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "jeeW4bw4ZbWo"
      },
      "outputs": [],
      "source": [
        "def estimate_fisher_retain(model, num_batches=200):\n",
        "    model.eval()\n",
        "    fisher = {\n",
        "        name: torch.zeros_like(p.data)\n",
        "        for name, p in model.named_parameters()\n",
        "        if p.requires_grad\n",
        "    }\n",
        "\n",
        "    loader = create_dataloader(retain_train_ds, batch_size)\n",
        "    it = iter(loader)\n",
        "\n",
        "    for i in range(num_batches):\n",
        "        try:\n",
        "            batch = next(it)\n",
        "        except StopIteration:\n",
        "            it = iter(loader)\n",
        "            batch = next(it)\n",
        "\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        model.zero_grad()\n",
        "        out = model(**batch)\n",
        "        loss = out.loss\n",
        "        loss.backward()\n",
        "\n",
        "        for name, p in model.named_parameters():\n",
        "            if not p.requires_grad or p.grad is None:\n",
        "                continue\n",
        "            fisher[name] += p.grad.data.pow(2)\n",
        "\n",
        "    for name in fisher:\n",
        "        fisher[name] /= num_batches\n",
        "\n",
        "    model.train()\n",
        "    return fisher\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "sIlC7y1Vd0Gj"
      },
      "outputs": [],
      "source": [
        "def estimate_model_fisher_retain(model, num_batches=200, top_k=100):\n",
        "    \"\"\"\n",
        "    Compute *model Fisher* diagonal using KL(p_ref || p_model),\n",
        "    with optional top-K truncation of the reference distribution.\n",
        "\n",
        "    top_k < 0  → use full distribution (no truncation)\n",
        "    top_k > 0  → keep only top_k tokens in reference distribution\n",
        "    \"\"\"\n",
        "\n",
        "    # Freeze reference model θ0\n",
        "    ref_model = copy.deepcopy(model).eval().to(device)\n",
        "    for p in ref_model.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    fisher = {\n",
        "        name: torch.zeros_like(p.data)\n",
        "        for name, p in model.named_parameters()\n",
        "        if p.requires_grad\n",
        "    }\n",
        "\n",
        "    loader = create_dataloader(retain_train_ds, batch_size)\n",
        "    it = iter(loader)\n",
        "\n",
        "    for i in range(num_batches):\n",
        "        try:\n",
        "            batch = next(it)\n",
        "        except StopIteration:\n",
        "            it = iter(loader)\n",
        "            batch = next(it)\n",
        "\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        # ---- 1. Reference distribution ----\n",
        "        with torch.no_grad():\n",
        "            ref_logits = ref_model(**batch).logits\n",
        "            ref_probs_full = ref_logits.softmax(dim=-1)  # shape [B, T, V]\n",
        "\n",
        "        # ---- 2. Possibly truncate to top-K ----\n",
        "        if top_k is not None and top_k > 0:\n",
        "            # Get top-K indices for each token\n",
        "            top_vals, top_idx = torch.topk(ref_probs_full, k=top_k, dim=-1)\n",
        "            # Renormalize probs over top-K\n",
        "            ref_probs = top_vals / top_vals.sum(dim=-1, keepdim=True)\n",
        "            # Make a tensor of zeros [B,T,V]\n",
        "            ref_probs_k = torch.zeros_like(ref_probs_full)\n",
        "            # Scatter top-K probabilities back into vocab dimension\n",
        "            ref_probs_k.scatter_(-1, top_idx, ref_probs)\n",
        "            ref_probs = ref_probs_k\n",
        "        else:\n",
        "            # use full distribution\n",
        "            ref_probs = ref_probs_full\n",
        "\n",
        "        # ---- 3. Model logits ----\n",
        "        logits = model(**batch).logits\n",
        "        log_probs = logits.log_softmax(dim=-1)\n",
        "\n",
        "        # ---- 4. KL(p_ref || p_model) ----\n",
        "        # KL per token: Σ_i q_i log(q_i/p_i)\n",
        "        kl = (ref_probs * (ref_probs.log() - log_probs)).sum(dim=-1)\n",
        "        loss = kl.mean()\n",
        "\n",
        "        # ---- 5. Backprop = model Fisher at θ0 ----\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # ---- 6. Accumulate grad^2 ----\n",
        "        for name, p in model.named_parameters():\n",
        "            if not p.requires_grad or p.grad is None:\n",
        "                continue\n",
        "            fisher[name] += p.grad.data.pow(2)\n",
        "\n",
        "    # Average\n",
        "    for name in fisher:\n",
        "        fisher[name] /= num_batches\n",
        "\n",
        "    model.train()\n",
        "    return fisher\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmCW04VjZjuS",
        "outputId": "ea63d295-eeb4-4679-e1ba-54b5551efcf9"
      },
      "outputs": [],
      "source": [
        "def run_protected_adam_precomputed(\n",
        "    num_epochs=2,\n",
        "    lr=5e-5,\n",
        "    alpha_geom=1.0,\n",
        "    beta_geom=10.0,\n",
        "    gamma_exp=0.5,\n",
        "    rho_all=0.99,\n",
        "    fisher_sub=None,   # dict[name -> tensor]\n",
        "):\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    model = copy.deepcopy(base_model).to(device)\n",
        "    model.train()\n",
        "\n",
        "    # We'll work with named parameters for alignment\n",
        "    named_params = [\n",
        "        (name, p) for name, p in model.named_parameters()\n",
        "        if p.requires_grad\n",
        "    ]\n",
        "\n",
        "    state = {}\n",
        "    for name, p in named_params:\n",
        "        if fisher_sub is not None and name in fisher_sub:\n",
        "            v_sub_init = fisher_sub[name].clone().to(device)\n",
        "        else:\n",
        "            v_sub_init = torch.zeros_like(p.data)\n",
        "\n",
        "        state[name] = {\n",
        "            \"m\": torch.zeros_like(p.data),\n",
        "            \"v_all\": torch.zeros_like(p.data),\n",
        "            \"v_sub\": v_sub_init,\n",
        "        }\n",
        "\n",
        "    beta1 = 0.9\n",
        "    eps = 1e-6\n",
        "    global_step = 0\n",
        "\n",
        "    def protected_adam_step():\n",
        "        nonlocal global_step\n",
        "        global_step += 1\n",
        "        for name, p in named_params:\n",
        "            if p.grad is None:\n",
        "                continue\n",
        "            grad = p.grad.data\n",
        "            s = state[name]\n",
        "\n",
        "            # first moment\n",
        "            s[\"m\"].mul_(beta1).add_(grad, alpha=1 - beta1)\n",
        "\n",
        "            # second moment on \"all\" (new Update) data\n",
        "            s[\"v_all\"].mul_(rho_all).addcmul_(grad, grad, value=1 - rho_all)\n",
        "\n",
        "            v_all = s[\"v_all\"]\n",
        "            v_sub = s[\"v_sub\"]  # fixed precomputed Fisher\n",
        "            v_protect = alpha_geom * v_all + beta_geom * v_sub\n",
        "\n",
        "            m_hat = s[\"m\"] / (1 - beta1**global_step)\n",
        "            denom = (v_protect + eps).pow(gamma_exp)\n",
        "            step = m_hat / denom\n",
        "            p.data.add_(step, alpha=-lr)\n",
        "\n",
        "    print(\"=== ProtectedAdam-γ with precomputed Retain Fisher ===\")\n",
        "    for epoch in range(num_epochs):\n",
        "        for step, batch in enumerate(update_loader):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            model.zero_grad()\n",
        "            out = model(**batch)\n",
        "            loss = out.loss\n",
        "            loss.backward()\n",
        "            protected_adam_step()\n",
        "\n",
        "        print(f\"Epoch {epoch} evaluation:\")\n",
        "        eval_ppl(model, update_test_ds, \"Update new (precomputed-Fisher)\")\n",
        "        eval_ppl(model, retain_test_ds,  \"Retain protected (precomputed-Fisher)\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# 1. Make a base model for Fisher estimation\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "\n",
        "# 2. Estimate Fisher on Retain ONCE\n",
        "mfisher_retain = estimate_model_fisher_retain(base_model, num_batches=200)\n",
        "\n",
        "# 3. Run Update finetuning using precomputed Fisher, no Retain batches\n",
        "protected_model_pre_mfisher = run_protected_adam_precomputed(\n",
        "    num_epochs=3,\n",
        "    lr=1e-5,\n",
        "    alpha_geom=1.0,\n",
        "    beta_geom=10.0,\n",
        "    gamma_exp=0.5,\n",
        "    rho_all=0.99,\n",
        "    fisher_sub=mfisher_retain,   # <- pass the dict here\n",
        ")\n",
        "\n",
        "\n",
        "# 2. Estimate Fisher on Retain ONCE\n",
        "fisher_retain = estimate_fisher_retain(base_model, num_batches=200)\n",
        "\n",
        "# 3. Run Update finetuning using precomputed Fisher, no Retain batches\n",
        "protected_model_pre = run_protected_adam_precomputed(\n",
        "    num_epochs=3,\n",
        "    lr=3e-4,\n",
        "    alpha_geom=1.0,\n",
        "    beta_geom=10.0,\n",
        "    gamma_exp=0.5,\n",
        "    rho_all=0.99,\n",
        "    fisher_sub=fisher_retain,   # <- pass the dict here\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "id": "Q7ouhy6W_9b7",
        "outputId": "36bb129d-ea7f-4e61-add9-e6baa4b09f51"
      },
      "outputs": [],
      "source": [
        "# does not work, ignore for now\n",
        "def run_protected_adam_precomputed2(\n",
        "    num_epochs=2,\n",
        "    lr=5e-5,\n",
        "    alpha_geom=1.0,      # scale for v_all (Adam geometry)\n",
        "    beta_geom=10.0,      # strength of protection retainom v_sub\n",
        "    gamma_exp=0.5,       # exponent applied only to normalized v_sub\n",
        "    rho_all=0.99,\n",
        "    fisher_sub=None,     # dict[name -> tensor], precomputed Fisher on Retain\n",
        "):\n",
        "    \"\"\"\n",
        "    Protected Adam with precomputed Fisher (additive version).\n",
        "\n",
        "    - v_all: EMA of grad^2 on Update (new task), like Adam.\n",
        "    - v_sub: fixed Fisher retainom Retain (protected capability), precomputed.\n",
        "    - v_sub is normalized globally once to be dimensionless.\n",
        "\n",
        "    Update (per-parameter i):\n",
        "        v_all_i ← EMA of g_i^2\n",
        "        v_sub_i ≈ Fisher_i\n",
        "\n",
        "        v_sub_scaled_i = v_sub_i / global_mean(v_sub)\n",
        "\n",
        "        base_rms_i   = sqrt(alpha_geom * v_all_i)\n",
        "        protect_i    = beta_geom * (v_sub_scaled_i ** gamma_exp)\n",
        "\n",
        "        denom_i = base_rms_i + protect_i + eps\n",
        "\n",
        "        Δθ_i = -lr * m_hat_i / denom_i\n",
        "\n",
        "    Properties:\n",
        "      - If fisher_sub is None or beta_geom = 0 -> exactly Adam.\n",
        "      - If v_sub is small -> denom ≈ base_rms -> Adam-like.\n",
        "      - If v_sub is large -> extra additive penalty in denom -> stronger protection.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1) Start retainom base GPT-2\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    model = copy.deepcopy(base_model).to(device)\n",
        "    model.train()\n",
        "\n",
        "    # 2) Collect named parameters to align with fisher_sub[name]\n",
        "    named_params = [\n",
        "        (name, p) for name, p in model.named_parameters()\n",
        "        if p.requires_grad\n",
        "    ]\n",
        "\n",
        "    # 3) Initialize state (m, v_all, v_sub)\n",
        "    state = {}\n",
        "    for name, p in named_params:\n",
        "        v_sub_init = torch.zeros_like(p.data)\n",
        "        if fisher_sub is not None and name in fisher_sub:\n",
        "            v_sub_init = fisher_sub[name].clone().to(p.data.device)\n",
        "        state[name] = {\n",
        "            \"m\": torch.zeros_like(p.data),\n",
        "            \"v_all\": torch.zeros_like(p.data),\n",
        "            \"v_sub\": v_sub_init,\n",
        "        }\n",
        "\n",
        "    # 4) Compute a global mean of v_sub for normalization (dimensionless)\n",
        "    if fisher_sub is not None:\n",
        "        total_sum = 0.0\n",
        "        total_count = 0\n",
        "        for name, p in named_params:\n",
        "            v_sub = state[name][\"v_sub\"]\n",
        "            if v_sub.numel() > 0:\n",
        "                total_sum += v_sub.sum().item()\n",
        "                total_count += v_sub.numel()\n",
        "        if total_count > 0:\n",
        "            global_vsub_mean = total_sum / total_count\n",
        "        else:\n",
        "            global_vsub_mean = 1.0\n",
        "    else:\n",
        "        global_vsub_mean = 1.0\n",
        "\n",
        "    beta1 = 0.9\n",
        "    eps = 1e-8\n",
        "    global_step = 0\n",
        "\n",
        "    print(\"=== ProtectedAdam-precomputed2 (additive): Adam base + Fisher protection ===\")\n",
        "    print(\n",
        "        f\"alpha_geom={alpha_geom}, beta_geom={beta_geom}, \"\n",
        "        f\"gamma_exp={gamma_exp}, rho_all={rho_all}, \"\n",
        "        f\"global_vsub_mean={global_vsub_mean:.3e}\"\n",
        "    )\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for step, batch in enumerate(update_loader):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            # ----- 1) Forward/backward on Update (new task) -----\n",
        "            model.zero_grad()\n",
        "            out = model(**batch)\n",
        "            loss = out.loss\n",
        "            loss.backward()\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "            # ----- 2) Protected Adam step (additive Fisher term) -----\n",
        "            with torch.no_grad():\n",
        "                for name, p in named_params:\n",
        "                    if p.grad is None:\n",
        "                        continue\n",
        "\n",
        "                    g = p.grad.data\n",
        "                    s = state[name]\n",
        "\n",
        "                    # First moment (Adam)\n",
        "                    s[\"m\"].mul_(beta1).add_(g, alpha=1 - beta1)\n",
        "\n",
        "                    # Second moment on \"all\" (new Update) data (Adam-style)\n",
        "                    s[\"v_all\"].mul_(rho_all).addcmul_(g, g, value=1 - rho_all)\n",
        "\n",
        "                    v_all = s[\"v_all\"]\n",
        "                    v_sub = s[\"v_sub\"]\n",
        "\n",
        "                    # Base Adam geometry: sqrt of v_all (scaled)\n",
        "                    base_rms = (alpha_geom * v_all).sqrt()\n",
        "\n",
        "                    # Normalized protective curvature retainom v_sub (dimensionless)\n",
        "                    if fisher_sub is not None and beta_geom != 0.0 and global_vsub_mean > 0.0:\n",
        "                        v_sub_scaled = v_sub / (global_vsub_mean + 1e-12)\n",
        "                        v_sub_scaled = torch.clamp(v_sub_scaled, min=0.0)  # safety\n",
        "                        protect_term = beta_geom * v_sub_scaled.pow(gamma_exp)\n",
        "                    else:\n",
        "                        protect_term = 0.0\n",
        "\n",
        "                    m_hat = s[\"m\"] / (1 - beta1**global_step)\n",
        "\n",
        "                    # ADDITIVE protection: denom = base_rms + protective term\n",
        "                    denom = base_rms + protect_term + eps\n",
        "                    step_dir = m_hat / denom\n",
        "\n",
        "                    p.data.add_(step_dir, alpha=-lr)\n",
        "\n",
        "            if (step + 1) % 100 == 0:\n",
        "                print(f\"[Epoch {epoch} Step {step+1}] loss_new = {loss.item():.4f}\")\n",
        "\n",
        "        # ----- 3) Epoch-end evaluation -----\n",
        "        print(f\"Epoch {epoch} evaluation:\")\n",
        "        update_ppl = eval_ppl(model, update_test_ds, \"Update new (ProtAdam-pre2-add)\")\n",
        "        retain_ppl  = eval_ppl(model, retain_test_ds,  \"Retain protected (ProtAdam-pre2-add)\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# 3. Run Update finetuning using precomputed Fisher, no Retain batches\n",
        "protected_model_pre = run_protected_adam_precomputed2(\n",
        "    num_epochs=3,\n",
        "    lr=3e-4,\n",
        "    alpha_geom=1.0,\n",
        "    beta_geom=0.1,\n",
        "    gamma_exp=0.5,\n",
        "    rho_all=0.99,\n",
        "    fisher_sub=fisher_retain,   # <- pass the dict here\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAgBd4Mko-n2",
        "outputId": "b721b0db-0453-4118-ce6b-5463fb7ae492"
      },
      "outputs": [],
      "source": [
        "def run_adam_with_fisher_trust_region(\n",
        "    num_epochs=2,\n",
        "    lr=1e-5,\n",
        "    beta1=0.9,\n",
        "    beta2=0.999,\n",
        "    eps=1e-8,\n",
        "    fisher_sub=None,    # dict[name -> tensor] retrain om estimate_fishe_retain_named(...)\n",
        "    delta_kl=1e-3,      # KL budget per step (approx)\n",
        "):\n",
        "    \"\"\"\n",
        "    Adam on Update, with a TRPO-style KL trust region on Retain capability:\n",
        "      1) Compute standard Adam step Δθ.\n",
        "      2) Estimate Retain KL ≈ 0.5 * Σ_i F_sub[i] * (Δθ_i)^2\n",
        "      3) If KL > delta_kl: scale Δθ by sqrt(delta_kl / KL).\n",
        "    \"\"\"\n",
        "\n",
        "    # Start retainom the same base model as elsewhere\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    base_model.resize_token_embeddings(len(tokenizer))\n",
        "    model = copy.deepcopy(base_model).to(device)\n",
        "    model.train()\n",
        "\n",
        "    # Named params for alignment with fisher_sub\n",
        "    named_params = [\n",
        "        (name, p) for name, p in model.named_parameters()\n",
        "        if p.requires_grad\n",
        "    ]\n",
        "\n",
        "    # Adam state\n",
        "    state = {}\n",
        "    for name, p in named_params:\n",
        "        state[name] = {\n",
        "            \"m\": torch.zeros_like(p.data),\n",
        "            \"v\": torch.zeros_like(p.data),\n",
        "        }\n",
        "\n",
        "    global_step = 0\n",
        "\n",
        "    print(\"=== Adam with Fisher KL trust region on Retain ===\")\n",
        "    print(f\"lr={lr}, delta_kl={delta_kl}\")\n",
        "    for epoch in range(num_epochs):\n",
        "        for step, batch in enumerate(update_loader):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            # 1) Forward/backward on Update batch\n",
        "            model.zero_grad()\n",
        "            out = model(**batch)\n",
        "            loss = out.loss\n",
        "            loss.backward()\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "            # 2) Compute Adam proposal step Δθ for each param (WITHOUT applying yet)\n",
        "            proposed_steps = {}  # name -> tensor (Δθ)\n",
        "            for name, p in named_params:\n",
        "                if p.grad is None:\n",
        "                    proposed_steps[name] = torch.zeros_like(p.data)\n",
        "                    continue\n",
        "\n",
        "                g = p.grad.data\n",
        "                s = state[name]\n",
        "\n",
        "                # Adam moments\n",
        "                s[\"m\"].mul_(beta1).add_(g, alpha=1 - beta1)\n",
        "                s[\"v\"].mul_(beta2).addcmul_(g, g, value=1 - beta2)\n",
        "\n",
        "                # Bias-corrected\n",
        "                m_hat = s[\"m\"] / (1 - beta1 ** global_step)\n",
        "                v_hat = s[\"v\"] / (1 - beta2 ** global_step)\n",
        "\n",
        "                # Classic Adam step (note: step is *direction*, no lr yet)\n",
        "                step_dir = m_hat / (v_hat.sqrt() + eps)\n",
        "\n",
        "                # Proposed parameter change Δθ = -lr * step_dir\n",
        "                delta_theta = -lr * step_dir\n",
        "                proposed_steps[name] = delta_theta\n",
        "\n",
        "            # 3) Estimate Retain KL for this joint step using precomputed Fisher\n",
        "            kl_est = 0.0\n",
        "            if fisher_sub is not None:\n",
        "                for name, p in named_params:\n",
        "                    if name not in fisher_sub:\n",
        "                        continue\n",
        "                    delta = proposed_steps[name]\n",
        "                    if delta is None:\n",
        "                        continue\n",
        "                    F = fisher_sub[name].to(delta.device)\n",
        "                    # 0.5 * sum_i F_i * (Δθ_i)^2\n",
        "                    kl_est += 0.5 * (F * (delta ** 2)).sum().item()\n",
        "\n",
        "            # 4) Compute scaling factor to enforce KL ≤ delta_kl\n",
        "            if fisher_sub is None or kl_est <= 0.0:\n",
        "                scale = 1.0\n",
        "            elif kl_est <= delta_kl:\n",
        "                scale = 1.0\n",
        "            else:\n",
        "                scale = (delta_kl / kl_est) ** 0.5\n",
        "\n",
        "            # 5) Apply scaled step\n",
        "            for name, p in named_params:\n",
        "                delta = proposed_steps[name]\n",
        "                if delta is None:\n",
        "                    continue\n",
        "                p.data.add_(delta * scale)\n",
        "\n",
        "            if (step + 1) % 100 == 0:\n",
        "                print(\n",
        "                    f\"[Epoch {epoch} Step {step+1}] \"\n",
        "                    f\"loss_new = {loss.item():.4f}, KL_est = {kl_est:.3e}, scale = {scale:.3f}\"\n",
        "                )\n",
        "\n",
        "        # 6) Evaluation at epoch end\n",
        "        print(f\"Epoch {epoch} evaluation:\")\n",
        "        update_ppl = eval_ppl(model, update_test_ds, \"Update new (Adam+KL)\")\n",
        "        retain_ppl  = eval_ppl(model, retain_test_ds,  \"Retain protected (Adam+KL)\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Precompute model Fisher on Retain (once)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "base_model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "fisher_retain = estimate_fisher_retain(base_model, num_batches=200)\n",
        "\n",
        "# Now run Update finetuning with TRPO-style KL trust region on Retain\n",
        "adam_trpo_model = run_adam_with_fisher_trust_region(\n",
        "    num_epochs=3,\n",
        "    lr=3e-4,\n",
        "    fisher_sub=fisher_retain,\n",
        "    delta_kl=1e-10,   # tune this up/down\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZW0AGZBHOA7O",
        "outputId": "128551b1-f88f-4f75-e6ef-94955a281746"
      },
      "outputs": [],
      "source": [
        "print(\"=== Final comparison ===\")\n",
        "\n",
        "print(\"Before training:\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "base_model.resize_token_embeddings(len(tokenizer))\n",
        "model = copy.deepcopy(base_model).to(device)\n",
        "\n",
        "eval_ppl(model, update_test_ds, \"Update new (before training)\")\n",
        "eval_ppl(model, retain_test_ds,  \"Retain protected (before training)\")\n",
        "\n",
        "print(\"Baseline Adam:\")\n",
        "eval_ppl(baseline_model, update_test_ds, \"Update new (baseline)\")\n",
        "eval_ppl(baseline_model, retain_test_ds,  \"Retain protected (baseline)\")\n",
        "\n",
        "print(\"\\nEWC:\")\n",
        "eval_ppl(ewc_model, update_test_ds, \"Update new (EWC)\")\n",
        "eval_ppl(ewc_model, retain_test_ds,  \"Retain protected (EWC)\")\n",
        "\n",
        "print(\"\\nProtectedAdam-γ:\")\n",
        "eval_ppl(protected_model, update_test_ds, \"Update new (ProtectedAdam-γ)\")\n",
        "eval_ppl(protected_model, retain_test_ds,  \"Retain protected (ProtectedAdam-γ)\")\n",
        "\n",
        "\n",
        "print(\"\\nProtectedAdam2-γ:\")\n",
        "eval_ppl(protected_model2, update_test_ds, \"Update new (ProtectedAdam-γ)\")\n",
        "eval_ppl(protected_model2, retain_test_ds,  \"Retain protected (ProtectedAdam-γ)\")\n",
        "\n",
        "print(\"\\nReplay:\")\n",
        "eval_ppl(replay_model, update_test_ds, \"Update new (ProtectedAdam-γ)\")\n",
        "eval_ppl(replay_model, retain_test_ds,  \"Retain protected (ProtectedAdam-γ)\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "openr1_v2 (local)",
      "language": "python",
      "name": "openr1_v2_local"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00a45ce331f049659ddc5a8d3a733689": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "019d2a1857fe46099f9fc01c6ba6b8d7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a9cd86371154105b73bcf2ede9feea1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4017aa81e3504205b797a47b4541d236",
              "IPY_MODEL_1afded3de67843e89d9ce05dacf97cf6",
              "IPY_MODEL_3d2e1581055140788c0638914354f8ed"
            ],
            "layout": "IPY_MODEL_b07617e6f8d34273a5929b6ed5998120"
          }
        },
        "11481ce4d7c54a20b328fe987088d02b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "149b331d4d87433398ee419064405baa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c48cfdbb01a6438087a163d31de89005",
            "placeholder": "​",
            "style": "IPY_MODEL_a8851e5f5d6f46e8860bc772fa5b81ea",
            "value": "model.safetensors: 100%"
          }
        },
        "17a88ca67a994b51b5f425842c0ab3ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7143d3bd72f4e1f9e119e1b29205cfb",
            "placeholder": "​",
            "style": "IPY_MODEL_cf6bf77b10dc4e04920eb2c05b5a4347",
            "value": "generation_config.json: 100%"
          }
        },
        "19a30732e19745b8a333de9eb6332a41": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_46a0746bf84440f38874b729d617f1df",
              "IPY_MODEL_9de635c4e355447e8e2a4d9f760fdfde",
              "IPY_MODEL_8998157212b8431ba8509dd0df98981b"
            ],
            "layout": "IPY_MODEL_9921b84c7f784e7ba9f32a97602546a7"
          }
        },
        "1afded3de67843e89d9ce05dacf97cf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a197dbff6da43a6b445dfe17b2c2522",
            "max": 26,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_412823a0f00e42059339c56fb4bd9c9f",
            "value": 26
          }
        },
        "1f6ddb9d63b442dcb6d2b772c5068212": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21357a168091432fbf46ec848df81842": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5f3adfc279949e58696997e532348ee",
            "placeholder": "​",
            "style": "IPY_MODEL_4eb8fcc217cf4b9e98b63c09077a17ec",
            "value": "tokenizer.json: 100%"
          }
        },
        "2846c79926f440708121d6b5efa2f901": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38706d9664be4357b79dd614fbac9b27": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3adc4be8a5e442cc8c4d230d8ec3ef85": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3c41a4d44ad349d2accb847f8ec65f41": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3d2e1581055140788c0638914354f8ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7bc5dd35c071495b82c20903d663b66c",
            "placeholder": "​",
            "style": "IPY_MODEL_91a9aec8a55f4f418af506009da10f35",
            "value": " 26.0/26.0 [00:00&lt;00:00, 3.32kB/s]"
          }
        },
        "3ee498aad5c54b4c956bb39619959b60": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6c8823a320447f7bae20c76e4070c0b",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8c9e710110a6423b84ade1d41b8a0529",
            "value": 1042301
          }
        },
        "4017aa81e3504205b797a47b4541d236": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c27c084743f643cc972967bcb0eb5753",
            "placeholder": "​",
            "style": "IPY_MODEL_6e69c53fb73d44caaf004f4a1baf5330",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "412823a0f00e42059339c56fb4bd9c9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4644d3f2f33d4f089ee5d788d8a2a0ef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46a0746bf84440f38874b729d617f1df": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9182aedceb2421b9a77a245d20a9a1e",
            "placeholder": "​",
            "style": "IPY_MODEL_95b18249d1884fa7895fb09393d2598b",
            "value": "config.json: 100%"
          }
        },
        "46ab261f4cc549e181eeae76c646abaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_21357a168091432fbf46ec848df81842",
              "IPY_MODEL_53dbd36dc7e4462f8d94a403a0d82875",
              "IPY_MODEL_c1bac9dec2244a1e9eef085386d047f1"
            ],
            "layout": "IPY_MODEL_ec9d2bfc9b82450199239de8eada416d"
          }
        },
        "4a197dbff6da43a6b445dfe17b2c2522": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a2226e944a4410aa53f1717e07169b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e9ea7427ef0479c9b0216fdb9604c8a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4eb8fcc217cf4b9e98b63c09077a17ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f1ed9d96a5a4693ac360e9f90e42b8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "503d3af66d1c4b47ac0f9fd4caf475ae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53dbd36dc7e4462f8d94a403a0d82875": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4644d3f2f33d4f089ee5d788d8a2a0ef",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_38706d9664be4357b79dd614fbac9b27",
            "value": 1355256
          }
        },
        "5df1c3945e404094b3f18c099b502a84": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00a45ce331f049659ddc5a8d3a733689",
            "placeholder": "​",
            "style": "IPY_MODEL_b7e41e3eac364027a5a1bf7342f7bd49",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 1.57MB/s]"
          }
        },
        "6099f135e0814a47822b3749934696b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2f3c8562c204b1d9222851eed5f106f",
            "placeholder": "​",
            "style": "IPY_MODEL_f521b4a4d7154de6ba16f3e6bdf59b51",
            "value": "vocab.json: 100%"
          }
        },
        "65ac5143b332424ca9424506b20cd40a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "675449c0ef484dc3b3baae5bbe8b5fd5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e69c53fb73d44caaf004f4a1baf5330": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7681bec193b248ad8beca1bd01baf4c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2846c79926f440708121d6b5efa2f901",
            "placeholder": "​",
            "style": "IPY_MODEL_3adc4be8a5e442cc8c4d230d8ec3ef85",
            "value": " 124/124 [00:00&lt;00:00, 16.4kB/s]"
          }
        },
        "7805724c2c5448d4b29119483f055636": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7bc5dd35c071495b82c20903d663b66c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dddedc6cfcb46848461c6aa231650ec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8998157212b8431ba8509dd0df98981b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_675449c0ef484dc3b3baae5bbe8b5fd5",
            "placeholder": "​",
            "style": "IPY_MODEL_65ac5143b332424ca9424506b20cd40a",
            "value": " 665/665 [00:00&lt;00:00, 85.9kB/s]"
          }
        },
        "8bdf857cc0fb474f9bf8c73fa0ea2257": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_17a88ca67a994b51b5f425842c0ab3ff",
              "IPY_MODEL_ecdf1063c4cc48cab41115fbda94cc85",
              "IPY_MODEL_7681bec193b248ad8beca1bd01baf4c6"
            ],
            "layout": "IPY_MODEL_1f6ddb9d63b442dcb6d2b772c5068212"
          }
        },
        "8c9e710110a6423b84ade1d41b8a0529": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8d5615ddaac748d7a1ba5a26726aa6fb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ebe0ad1bb4e456a97821b31d8dfb9f6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ee340f1e44442aca9ad56eaf28e3ec6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91a9aec8a55f4f418af506009da10f35": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92bf0487d45e4c06869ae88ee0bb877c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95b18249d1884fa7895fb09393d2598b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9921b84c7f784e7ba9f32a97602546a7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9de635c4e355447e8e2a4d9f760fdfde": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11481ce4d7c54a20b328fe987088d02b",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e97ed5800cff4055900545ff376979be",
            "value": 665
          }
        },
        "a1fc267bd68947dfbafa9ed3e6fb56c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a2f3c8562c204b1d9222851eed5f106f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8851e5f5d6f46e8860bc772fa5b81ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b07617e6f8d34273a5929b6ed5998120": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0fa9161ed064445aece8d321b3d88a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ee340f1e44442aca9ad56eaf28e3ec6",
            "max": 548105171,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3c41a4d44ad349d2accb847f8ec65f41",
            "value": 548105171
          }
        },
        "b5f3adfc279949e58696997e532348ee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7e41e3eac364027a5a1bf7342f7bd49": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bbe725cd4bf4424490b1b6444fd128a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_149b331d4d87433398ee419064405baa",
              "IPY_MODEL_b0fa9161ed064445aece8d321b3d88a7",
              "IPY_MODEL_cf32259002024b7da355cc4eb0a752e8"
            ],
            "layout": "IPY_MODEL_503d3af66d1c4b47ac0f9fd4caf475ae"
          }
        },
        "c199e25df4d54001919f0481edc0b358": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e7c2de72aff4428db80fd3ddd8d82557",
              "IPY_MODEL_cb45de6c1d7d4dc7809c42da301e9fba",
              "IPY_MODEL_d8e0603c141f4674a1f57399cee4ef45"
            ],
            "layout": "IPY_MODEL_8d5615ddaac748d7a1ba5a26726aa6fb"
          }
        },
        "c1bac9dec2244a1e9eef085386d047f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ebe0ad1bb4e456a97821b31d8dfb9f6",
            "placeholder": "​",
            "style": "IPY_MODEL_4a2226e944a4410aa53f1717e07169b5",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 6.35MB/s]"
          }
        },
        "c27c084743f643cc972967bcb0eb5753": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c48cfdbb01a6438087a163d31de89005": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb45de6c1d7d4dc7809c42da301e9fba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d928ee45c23f436a9aa89ca5ef53df92",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ff49ae10aa48498dbb0cc35ab638c298",
            "value": 456318
          }
        },
        "ce768476c2144ff58e413e5195528b39": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf32259002024b7da355cc4eb0a752e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce768476c2144ff58e413e5195528b39",
            "placeholder": "​",
            "style": "IPY_MODEL_4f1ed9d96a5a4693ac360e9f90e42b8a",
            "value": " 548M/548M [00:01&lt;00:00, 360MB/s]"
          }
        },
        "cf6bf77b10dc4e04920eb2c05b5a4347": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2e42c54ade243779ec71eb5fd4a51bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6099f135e0814a47822b3749934696b3",
              "IPY_MODEL_3ee498aad5c54b4c956bb39619959b60",
              "IPY_MODEL_5df1c3945e404094b3f18c099b502a84"
            ],
            "layout": "IPY_MODEL_7dddedc6cfcb46848461c6aa231650ec"
          }
        },
        "d8e0603c141f4674a1f57399cee4ef45": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_019d2a1857fe46099f9fc01c6ba6b8d7",
            "placeholder": "​",
            "style": "IPY_MODEL_92bf0487d45e4c06869ae88ee0bb877c",
            "value": " 456k/456k [00:00&lt;00:00, 1.07MB/s]"
          }
        },
        "d928ee45c23f436a9aa89ca5ef53df92": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6c8823a320447f7bae20c76e4070c0b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7c2de72aff4428db80fd3ddd8d82557": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e9ea7427ef0479c9b0216fdb9604c8a",
            "placeholder": "​",
            "style": "IPY_MODEL_7805724c2c5448d4b29119483f055636",
            "value": "merges.txt: 100%"
          }
        },
        "e97ed5800cff4055900545ff376979be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ec9d2bfc9b82450199239de8eada416d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecdf1063c4cc48cab41115fbda94cc85": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed40a4f1820246b99ca964b03244c906",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a1fc267bd68947dfbafa9ed3e6fb56c9",
            "value": 124
          }
        },
        "ed40a4f1820246b99ca964b03244c906": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f521b4a4d7154de6ba16f3e6bdf59b51": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f7143d3bd72f4e1f9e119e1b29205cfb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9182aedceb2421b9a77a245d20a9a1e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff49ae10aa48498dbb0cc35ab638c298": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
