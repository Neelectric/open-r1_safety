{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiments with Fisher\n",
        "\n",
        "This whole script takes ~45mins to run with an H100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2mUsing Python 3.12.12 environment at: /pvc/repos/open-r1_safety/openr1_v2\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m2 packages\u001b[0m \u001b[2min 17ms\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!uv pip install ipykernel jupyter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup 'Update' and 'Protect' texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import copy\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from tqdm.notebook import tqdm # this makes tqdm.write() work with notebooks!\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, get_scheduler\n",
        "from datasets import load_dataset, load_from_disk\n",
        "\n",
        "from trl.trainer.sft_trainer import DataCollatorForLanguageModeling\n",
        "from copy import deepcopy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_generation_chat_template(tokenizer):\n",
        "    if \"qwen\" in tokenizer.name_or_path.lower():\n",
        "        # we have to use DataCollatorForLanguageModeling with completion_only_loss=True\n",
        "        # however, for that tokenizer needs to have return_assistant_tokens_mask=True, and qwen decided against adding support for {% generation %} / {% endgeneration %} functionality\n",
        "        # so we download a community qwen3 chat template that has it\n",
        "        !wget -O all_assistant.jinja --no-check-certificate https://raw.githubusercontent.com/HarryMayne/qwen_3_chat_templates/refs/heads/main/all_assistant.jinja\n",
        "        !mv all_assistant.jinja chat_templates/qwen_all_assistant.jinja\n",
        "        with open('chat_templates/qwen_all_assistant.jinja', 'r') as f:\n",
        "            tokenizer.chat_template = f.read()\n",
        "    if \"smollm2\" in tokenizer.name_or_path.lower():\n",
        "        with open('chat_templates/smollm2_all_assistant.jinja', 'r') as f:\n",
        "            tokenizer.chat_template = f.read()\n",
        "    if \"llama\" in tokenizer.name_or_path.lower():\n",
        "        with open('chat_templates/llama3_all_assistant.jinja', 'r') as f:\n",
        "            tokenizer.chat_template = f.read()\n",
        "    return tokenizer\n",
        "\n",
        "def load_or_preprocess_dataset(model_id, dataset_id, tokenizer, max_length=4096):\n",
        "    local_ds_id = f\"datasets/{model_id}/{dataset_id}\"\n",
        "    num_proc = 16\n",
        "    if True:\n",
        "        print(f\"Dataset not found locally, processing and caching...\")\n",
        "        raw_dataset = load_dataset(dataset_id)[\"train\"]\n",
        "        def preprocess(example):\n",
        "            tokenized = tokenizer.apply_chat_template(\n",
        "                example[\"messages\"],\n",
        "                tokenize=True,\n",
        "                return_assistant_tokens_mask=True,\n",
        "                return_dict=True,\n",
        "            )\n",
        "            return {\n",
        "                \"input_ids\": tokenized[\"input_ids\"],\n",
        "                \"assistant_masks\": tokenized[\"assistant_masks\"],\n",
        "            }\n",
        "        \n",
        "        tokenized_dataset = raw_dataset.map(preprocess, remove_columns=raw_dataset.column_names, num_proc=num_proc, desc=\"Tokenizing\")\n",
        "        def shorter_than(example):\n",
        "            return len(example[\"input_ids\"]) <= max_length\n",
        "        final_dataset = tokenized_dataset.filter(shorter_than, num_proc=num_proc, desc=f\"Filtering to max length {max_length}\")\n",
        "        print(f\"Tokenized: {len(tokenized_dataset)}, After filtering: {len(final_dataset)}\")\n",
        "        final_dataset.save_to_disk(local_ds_id)\n",
        "    return final_dataset\n",
        "\n",
        "def create_dataloader(tokenized_dataset, batch_size):\n",
        "    collator = DataCollatorForLanguageModeling(pad_token_id=tokenizer.pad_token_id, completion_only_loss=True)\n",
        "    dataloader = DataLoader(\n",
        "        tokenized_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=collator,\n",
        "    )\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import set_seed\n",
        "random_seed = 42\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "{{- bos_token }}\n",
            "{%- if custom_tools is defined %}\n",
            "    {%- set tools = custom_tools %}\n",
            "{%- endif %}\n",
            "{%- if not tools_in_user_message is defined %}\n",
            "    {%- set tools_in_user_message = true %}\n",
            "{%- endif %}\n",
            "{%- if not date_string is defined %}\n",
            "    {%- set date_string = \"26 Jul 2024\" %}\n",
            "{%- endif %}\n",
            "{%- if not tools is defined %}\n",
            "    {%- set tools = none %}\n",
            "{%- endif %}\n",
            "\n",
            "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
            "{%- if messages[0]['role'] == 'system' %}\n",
            "    {%- set system_message = messages[0]['content']|trim %}\n",
            "    {%- set messages = messages[1:] %}\n",
            "{%- else %}\n",
            "    {%- set system_message = \"You are a helpful AI Assistant that provides well-reasoned and detailed responses. You first think about the reasoning process as an internal monologue and then provide the user with the answer. Respond in the following format: <think>\n",
            "...\n",
            "</think>\n",
            "<answer>\n",
            "...\n",
            "</answer>\" %}\n",
            "{%- endif %}\n",
            "\n",
            "{#- System message + builtin tools #}\n",
            "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
            "{%- if builtin_tools is defined or tools is not none %}\n",
            "    {{- \"Environment: ipython\\n\" }}\n",
            "{%- endif %}\n",
            "{%- if builtin_tools is defined %}\n",
            "    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}\n",
            "{%- endif %}\n",
            "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
            "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
            "{%- if tools is not none and not tools_in_user_message %}\n",
            "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
            "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
            "    {{- \"Do not use variables.\\n\\n\" }}\n",
            "    {%- for t in tools %}\n",
            "        {{- t | tojson(indent=4) }}\n",
            "        {{- \"\\n\\n\" }}\n",
            "    {%- endfor %}\n",
            "{%- endif %}\n",
            "{{- system_message }}\n",
            "{{- \"<|eot_id|>\" }}\n",
            "\n",
            "{#- Custom tools are passed in a user message with some extra guidance #}\n",
            "{%- if tools_in_user_message and not tools is none %}\n",
            "    {#- Extract the first user message so we can plug it in here #}\n",
            "    {%- if messages | length != 0 %}\n",
            "        {%- set first_user_message = messages[0]['content']|trim %}\n",
            "        {%- set messages = messages[1:] %}\n",
            "    {%- else %}\n",
            "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
            "    {%- endif %}\n",
            "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
            "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
            "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
            "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
            "    {{- \"Do not use variables.\\n\\n\" }}\n",
            "    {%- for t in tools %}\n",
            "        {{- t | tojson(indent=4) }}\n",
            "        {{- \"\\n\\n\" }}\n",
            "    {%- endfor %}\n",
            "    {{- first_user_message + \"<|eot_id|>\"}}\n",
            "{%- endif %}\n",
            "\n",
            "{%- for message in messages %}\n",
            "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
            "        {%- if message['role'] == 'assistant' %}\n",
            "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
            "            {% generation %}\n",
            "            {{- message['content'] | trim + '<|eot_id|>' }}\n",
            "            {% endgeneration %}\n",
            "        {%- else %}\n",
            "            {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
            "        {%- endif %}\n",
            "    {%- elif 'tool_calls' in message %}\n",
            "        {%- if not message.tool_calls|length == 1 %}\n",
            "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
            "        {%- endif %}\n",
            "        {%- set tool_call = message.tool_calls[0].function %}\n",
            "        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n",
            "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
            "            {% generation %}\n",
            "            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n",
            "            {%- for arg_name, arg_val in tool_call.arguments | items %}\n",
            "                {{- arg_name + '=\"' + arg_val + '\"' }}\n",
            "                {%- if not loop.last %}\n",
            "                    {{- \", \" }}\n",
            "                {%- endif %}\n",
            "            {%- endfor %}\n",
            "            {{- \")<|eom_id|>\" }}\n",
            "            {% endgeneration %}\n",
            "        {%- else %}\n",
            "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
            "            {% generation %}\n",
            "            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
            "            {{- '\"parameters\": ' }}\n",
            "            {{- tool_call.arguments | tojson }}\n",
            "            {{- \"}<|eot_id|>\" }}\n",
            "            {% endgeneration %}\n",
            "        {%- endif %}\n",
            "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
            "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
            "        {%- if message.content is mapping or message.content is iterable %}\n",
            "            {{- message.content | tojson }}\n",
            "        {%- else %}\n",
            "            {{- message.content }}\n",
            "        {%- endif %}\n",
            "        {{- \"<|eot_id|>\" }}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "{%- if add_generation_prompt %}\n",
            "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
            "{%- endif %}\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# model_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
        "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token  # for batching\n",
        "tokenizer = add_generation_chat_template(tokenizer)\n",
        "print(tokenizer.chat_template)\n",
        "\n",
        "batch_size = 1 #8 for smollm2, 1 for llama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset not found locally, processing and caching...\n",
            "Tokenized: 86158, After filtering: 3608\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fda5c07220ff41feabdb55d7920c360b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/3608 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "update_id = \"Neelectric/OpenR1-Math-220k_extended_Llama3_4096toks\"\n",
        "update_ds = load_or_preprocess_dataset(model_name, update_id, tokenizer, 1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset not found locally, processing and caching...\n",
            "Tokenized: 86745, After filtering: 74766\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9d7c669409e944e0bd9990eea4122067",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/2 shards):   0%|          | 0/74766 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "retain_id = \"Neelectric/wildguardmix_Llama-3.1-8B-Instruct_4096toks\"\n",
        "retain_ds = load_or_preprocess_dataset(model_name, retain_id, tokenizer, 1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1804\n",
            "1804\n"
          ]
        }
      ],
      "source": [
        "full_length = len(update_ds) //2\n",
        "print(full_length)\n",
        "update_ds = update_ds.select(range(full_length))\n",
        "retain_ds = retain_ds.select(range(full_length))\n",
        "print(len(retain_ds))\n",
        "update_ds = update_ds.shuffle(seed=random_seed)\n",
        "retain_ds = retain_ds.shuffle(seed=random_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322,
          "referenced_widgets": [
            "0a9cd86371154105b73bcf2ede9feea1",
            "4017aa81e3504205b797a47b4541d236",
            "1afded3de67843e89d9ce05dacf97cf6",
            "3d2e1581055140788c0638914354f8ed",
            "b07617e6f8d34273a5929b6ed5998120",
            "c27c084743f643cc972967bcb0eb5753",
            "6e69c53fb73d44caaf004f4a1baf5330",
            "4a197dbff6da43a6b445dfe17b2c2522",
            "412823a0f00e42059339c56fb4bd9c9f",
            "7bc5dd35c071495b82c20903d663b66c",
            "91a9aec8a55f4f418af506009da10f35",
            "d2e42c54ade243779ec71eb5fd4a51bf",
            "6099f135e0814a47822b3749934696b3",
            "3ee498aad5c54b4c956bb39619959b60",
            "5df1c3945e404094b3f18c099b502a84",
            "7dddedc6cfcb46848461c6aa231650ec",
            "a2f3c8562c204b1d9222851eed5f106f",
            "f521b4a4d7154de6ba16f3e6bdf59b51",
            "e6c8823a320447f7bae20c76e4070c0b",
            "8c9e710110a6423b84ade1d41b8a0529",
            "00a45ce331f049659ddc5a8d3a733689",
            "b7e41e3eac364027a5a1bf7342f7bd49",
            "c199e25df4d54001919f0481edc0b358",
            "e7c2de72aff4428db80fd3ddd8d82557",
            "cb45de6c1d7d4dc7809c42da301e9fba",
            "d8e0603c141f4674a1f57399cee4ef45",
            "8d5615ddaac748d7a1ba5a26726aa6fb",
            "4e9ea7427ef0479c9b0216fdb9604c8a",
            "7805724c2c5448d4b29119483f055636",
            "d928ee45c23f436a9aa89ca5ef53df92",
            "ff49ae10aa48498dbb0cc35ab638c298",
            "019d2a1857fe46099f9fc01c6ba6b8d7",
            "92bf0487d45e4c06869ae88ee0bb877c",
            "46ab261f4cc549e181eeae76c646abaa",
            "21357a168091432fbf46ec848df81842",
            "53dbd36dc7e4462f8d94a403a0d82875",
            "c1bac9dec2244a1e9eef085386d047f1",
            "ec9d2bfc9b82450199239de8eada416d",
            "b5f3adfc279949e58696997e532348ee",
            "4eb8fcc217cf4b9e98b63c09077a17ec",
            "4644d3f2f33d4f089ee5d788d8a2a0ef",
            "38706d9664be4357b79dd614fbac9b27",
            "8ebe0ad1bb4e456a97821b31d8dfb9f6",
            "4a2226e944a4410aa53f1717e07169b5",
            "19a30732e19745b8a333de9eb6332a41",
            "46a0746bf84440f38874b729d617f1df",
            "9de635c4e355447e8e2a4d9f760fdfde",
            "8998157212b8431ba8509dd0df98981b",
            "9921b84c7f784e7ba9f32a97602546a7",
            "f9182aedceb2421b9a77a245d20a9a1e",
            "95b18249d1884fa7895fb09393d2598b",
            "11481ce4d7c54a20b328fe987088d02b",
            "e97ed5800cff4055900545ff376979be",
            "675449c0ef484dc3b3baae5bbe8b5fd5",
            "65ac5143b332424ca9424506b20cd40a"
          ]
        },
        "id": "-KxrbhNtq7bc",
        "outputId": "bd4e7ec8-ef80-4cb7-d16a-808a85c8fd6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1443\n",
            "1443\n",
            "361\n",
            "1443\n",
            "361\n"
          ]
        }
      ],
      "source": [
        "# Train / test splits\n",
        "num_train = int(0.8 * full_length)\n",
        "print(num_train)\n",
        "retain_train_ds = retain_ds.select(range(num_train))\n",
        "retain_test_ds = retain_ds.select(range(num_train, full_length))\n",
        "print(len(retain_train_ds))\n",
        "print(len(retain_test_ds))\n",
        "\n",
        "update_train_ds = update_ds.select(range(num_train))\n",
        "update_test_ds = update_ds.select(range(num_train, full_length))\n",
        "print(len(update_train_ds))\n",
        "print(len(update_test_ds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynTogCxlD6qe",
        "outputId": "2fa89744-95db-4461-bd1f-e9bc372e1f99"
      },
      "outputs": [],
      "source": [
        "# block_size = 64\n",
        "# batch_size = 8\n",
        "\n",
        "# retain_train_ds  = LineByLineLMDataset(retain_train, tokenizer, block_size)\n",
        "# retain_test_ds   = LineByLineLMDataset(retain_test,  tokenizer, block_size)\n",
        "# update_train_ds = LineByLineLMDataset(update_train, tokenizer, block_size)\n",
        "# update_test_ds  = LineByLineLMDataset(update_test,  tokenizer, block_size)\n",
        "\n",
        "# update_loader = DataLoader(update_train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "# ba_loader  = DataLoader(retain_train_ds,  batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# len(update_train_ds), len(retain_train_ds)\n",
        "\n",
        "update_loader = create_dataloader(update_train_ds, batch_size)\n",
        "retain_loader = create_dataloader(update_test_ds, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3COzmCNrD8_u"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def eval_ppl(model, dataset, name, batch_size_eval=8, disable_tqdm=True):\n",
        "    model.eval()\n",
        "    loader = create_dataloader(dataset, batch_size_eval)\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "    for batch in tqdm(loader, disable=disable_tqdm):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        out = model(**batch)\n",
        "        loss = out.loss * batch[\"attention_mask\"].sum()\n",
        "        total_loss += loss.item()\n",
        "        total_tokens += batch[\"attention_mask\"].sum().item()\n",
        "    ppl = math.exp(total_loss / total_tokens)\n",
        "    print(f\"{name} perplexity: {ppl:.3f}\")\n",
        "    model.train()\n",
        "    return ppl\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Before any optim: ppl on train and protect before fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ec6f49894c494f7bb61386af4c35c88a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "model = copy.deepcopy(base_model).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "09efba3102924504bee7c7d43c27be42",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/46 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline update before optim perplexity: 2.866\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "48344a1bb0ba46adab22a24a7223201c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/46 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline retain before optim perplexity: 1.734\n"
          ]
        }
      ],
      "source": [
        "eng_ppl = eval_ppl(model, update_test_ds, \"Baseline update before optim\", batch_size_eval=8, disable_tqdm=False)\n",
        "ba_ppl = eval_ppl(model, retain_test_ds, \"Baseline retain before optim\", batch_size_eval=8, disable_tqdm=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527,
          "referenced_widgets": [
            "bbe725cd4bf4424490b1b6444fd128a2",
            "149b331d4d87433398ee419064405baa",
            "b0fa9161ed064445aece8d321b3d88a7",
            "cf32259002024b7da355cc4eb0a752e8",
            "503d3af66d1c4b47ac0f9fd4caf475ae",
            "c48cfdbb01a6438087a163d31de89005",
            "a8851e5f5d6f46e8860bc772fa5b81ea",
            "8ee340f1e44442aca9ad56eaf28e3ec6",
            "3c41a4d44ad349d2accb847f8ec65f41",
            "ce768476c2144ff58e413e5195528b39",
            "4f1ed9d96a5a4693ac360e9f90e42b8a",
            "8bdf857cc0fb474f9bf8c73fa0ea2257",
            "17a88ca67a994b51b5f425842c0ab3ff",
            "ecdf1063c4cc48cab41115fbda94cc85",
            "7681bec193b248ad8beca1bd01baf4c6",
            "1f6ddb9d63b442dcb6d2b772c5068212",
            "f7143d3bd72f4e1f9e119e1b29205cfb",
            "cf6bf77b10dc4e04920eb2c05b5a4347",
            "ed40a4f1820246b99ca964b03244c906",
            "a1fc267bd68947dfbafa9ed3e6fb56c9",
            "2846c79926f440708121d6b5efa2f901",
            "3adc4be8a5e442cc8c4d230d8ec3ef85"
          ]
        },
        "id": "YCnIJL95D_xR",
        "outputId": "e54bb78b-1a96-43cb-d896-271084ac0983"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "da661df416d0431f891589ad12965564",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# SmolLM2 report mentions they use lr=3e-4 throughout SFT (page 8 section 5.2) https://arxiv.org/pdf/2502.02737\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m baseline_model = \u001b[43mrun_baseline_adam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3e-4\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mrun_baseline_adam\u001b[39m\u001b[34m(num_epochs, lr)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_baseline_adam\u001b[39m(num_epochs=\u001b[32m2\u001b[39m, lr=\u001b[32m1e-5\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     base_model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     model = copy.deepcopy(base_model).to(device)\n\u001b[32m      6\u001b[39m     optimizer = AdamW(model.parameters(), lr=lr)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/pvc/repos/open-r1_safety/openr1_v2/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:604\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    603\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    608\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    610\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/pvc/repos/open-r1_safety/openr1_v2/lib/python3.12/site-packages/transformers/modeling_utils.py:277\u001b[39m, in \u001b[36mrestore_default_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    279\u001b[39m     torch.set_default_dtype(old_dtype)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/pvc/repos/open-r1_safety/openr1_v2/lib/python3.12/site-packages/transformers/modeling_utils.py:5048\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   5038\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5039\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   5041\u001b[39m     (\n\u001b[32m   5042\u001b[39m         model,\n\u001b[32m   5043\u001b[39m         missing_keys,\n\u001b[32m   5044\u001b[39m         unexpected_keys,\n\u001b[32m   5045\u001b[39m         mismatched_keys,\n\u001b[32m   5046\u001b[39m         offload_index,\n\u001b[32m   5047\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m5048\u001b[39m     ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5049\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5050\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5051\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5052\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5053\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5054\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5055\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5056\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5057\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5058\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5059\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5060\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5061\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5062\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5063\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5064\u001b[39m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[32m   5065\u001b[39m model.tie_weights()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/pvc/repos/open-r1_safety/openr1_v2/lib/python3.12/site-packages/transformers/modeling_utils.py:5468\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[39m\n\u001b[32m   5465\u001b[39m         args_list = logging.tqdm(args_list, desc=\u001b[33m\"\u001b[39m\u001b[33mLoading checkpoint shards\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   5467\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m args_list:\n\u001b[32m-> \u001b[39m\u001b[32m5468\u001b[39m         _error_msgs, disk_offload_index = \u001b[43mload_shard_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5469\u001b[39m         error_msgs += _error_msgs\n\u001b[32m   5471\u001b[39m \u001b[38;5;66;03m# Save offloaded index if needed\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/pvc/repos/open-r1_safety/openr1_v2/lib/python3.12/site-packages/transformers/modeling_utils.py:843\u001b[39m, in \u001b[36mload_shard_file\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    841\u001b[39m \u001b[38;5;66;03m# Skip it with fsdp on ranks other than 0\u001b[39;00m\n\u001b[32m    842\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized):\n\u001b[32m--> \u001b[39m\u001b[32m843\u001b[39m     disk_offload_index = \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreverse_key_renaming_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    849\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    850\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    851\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    852\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    853\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    854\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    856\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m error_msgs, disk_offload_index\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/pvc/repos/open-r1_safety/openr1_v2/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/pvc/repos/open-r1_safety/openr1_v2/lib/python3.12/site-packages/transformers/modeling_utils.py:750\u001b[39m, in \u001b[36m_load_state_dict_into_meta_model\u001b[39m\u001b[34m(model, state_dict, shard_file, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, hf_quantizer, keep_in_fp32_regex, device_mesh)\u001b[39m\n\u001b[32m    748\u001b[39m param = param[...]\n\u001b[32m    749\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m casting_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m750\u001b[39m     param = \u001b[43mparam\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasting_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m to_contiguous:\n\u001b[32m    752\u001b[39m     param = param.contiguous()\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "def run_baseline_adam(num_epochs=2, lr=1e-5):\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    model = copy.deepcopy(base_model).to(device)\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "    print(f\"Before opt:\")\n",
        "    update_ppl = eval_ppl(model, update_test_ds, \"Update new\")\n",
        "    retain_ppl  = eval_ppl(model, retain_test_ds,  \"Retain protected\")\n",
        "\n",
        "    print(\"=== Baseline Adam: train on Update only ===\")\n",
        "    for epoch in range(num_epochs):\n",
        "        for step, batch in enumerate(update_loader):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            model.zero_grad()\n",
        "            out = model(**batch)\n",
        "            loss = out.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if (step + 1) % 100 == 0:\n",
        "                print(f\"[Epoch {epoch} Step {step+1}] loss_new = {loss.item():.4f}\")\n",
        "\n",
        "        print(f\"Epoch {epoch} evaluation:\")\n",
        "        update_ppl = eval_ppl(model, update_test_ds, \"Update new\")\n",
        "        retain_ppl  = eval_ppl(model, retain_test_ds,  \"Retain protected\")\n",
        "    return model\n",
        "# SmolLM2 report mentions they use lr=3e-4 throughout SFT (page 8 section 5.2) https://arxiv.org/pdf/2502.02737\n",
        "baseline_model = run_baseline_adam(num_epochs=4, lr=3e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lz-u9NgkECO1",
        "outputId": "ed415c53-b8ac-47e4-822d-23c48409b63a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6e1090752dc04d55b46779e6736e20a6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Estimating Fisher on Retain (protected) ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c06e6e8e1c0408ca8389d400dabf5b5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== EWC: train on Update with Retain EWC penalty ===\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 1.96 GiB. GPU 0 has a total capacity of 139.81 GiB of which 1.91 GiB is free. Process 453783 has 137.89 GiB memory in use. Of the allocated memory 135.57 GiB is allocated by PyTorch, and 1.60 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 65\u001b[39m\n\u001b[32m     62\u001b[39m         ba_ppl  = eval_ppl(model, retain_test_ds,  \u001b[33m\"\u001b[39m\u001b[33mRetain protected (EWC)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m ewc_model = \u001b[43mrun_ewc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mewc_lambda\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50.0\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mrun_ewc\u001b[39m\u001b[34m(num_epochs, lr, ewc_lambda)\u001b[39m\n\u001b[32m     49\u001b[39m ewc_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p, p0 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(params, theta0.parameters()):\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     ewc_loss = ewc_loss + (\u001b[43mfisher\u001b[49m\u001b[43m[\u001b[49m\u001b[43mp\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mp0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m).sum()\n\u001b[32m     52\u001b[39m total_loss = loss_new + \u001b[32m0.5\u001b[39m * ewc_lambda * ewc_loss\n\u001b[32m     54\u001b[39m total_loss.backward()\n",
            "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 1.96 GiB. GPU 0 has a total capacity of 139.81 GiB of which 1.91 GiB is free. Process 453783 has 137.89 GiB memory in use. Of the allocated memory 135.57 GiB is allocated by PyTorch, and 1.60 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "def estimate_fisher_on_retain(model, num_batches=200):\n",
        "    model.eval()\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    fisher = {p: torch.zeros_like(p.data) for p in params}\n",
        "\n",
        "    # loader = DataLoader(retain_train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "    loader = create_dataloader(retain_train_ds, batch_size)\n",
        "    it = iter(loader)\n",
        "    for i in tqdm(range(num_batches)):\n",
        "        try:\n",
        "            batch = next(it)\n",
        "        except StopIteration:\n",
        "            it = iter(loader)\n",
        "            batch = next(it)\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        model.zero_grad()\n",
        "        out = model(**batch)\n",
        "        loss = out.loss\n",
        "        loss.backward()\n",
        "        for p in params:\n",
        "            if p.grad is None:\n",
        "                continue\n",
        "            fisher[p] += p.grad.data.pow(2)\n",
        "    for p in params:\n",
        "        fisher[p] /= num_batches\n",
        "    model.train()\n",
        "    return fisher\n",
        "\n",
        "def run_ewc(num_epochs=2, lr=5e-5, ewc_lambda=50.0):\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    model = copy.deepcopy(base_model).to(device)\n",
        "\n",
        "    print(\"Estimating Fisher on Retain (protected) ...\")\n",
        "    fisher = estimate_fisher_on_retain(model, num_batches=100)\n",
        "    theta0 = copy.deepcopy(model).to(device)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "\n",
        "    print(\"=== EWC: train on Update with Retain EWC penalty ===\")\n",
        "    for epoch in range(num_epochs):\n",
        "        for step, batch in enumerate(update_loader):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            model.zero_grad()\n",
        "            out = model(**batch)\n",
        "            loss_new = out.loss\n",
        "\n",
        "            ewc_loss = 0.0\n",
        "            for p, p0 in zip(params, theta0.parameters()):\n",
        "                ewc_loss = ewc_loss + (fisher[p] * (p - p0).pow(2)).sum()\n",
        "            total_loss = loss_new + 0.5 * ewc_lambda * ewc_loss\n",
        "\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if (step + 1) % 100 == 0:\n",
        "                print(f\"[Epoch {epoch} Step {step+1}] loss_new={loss_new.item():.4f}, ewc_loss={ewc_loss.item():.4f}\")\n",
        "\n",
        "        print(f\"Epoch {epoch} evaluation:\")\n",
        "        update_ppl = eval_ppl(model, update_test_ds, \"Update new (EWC)\")\n",
        "        ba_ppl  = eval_ppl(model, retain_test_ds,  \"Retain protected (EWC)\")\n",
        "    return model\n",
        "\n",
        "ewc_model = run_ewc(num_epochs=3, lr=3e-4, ewc_lambda=50.0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vS3js6UEEs8",
        "outputId": "50c1adf8-2795-458a-bc1a-6ff40a432ec3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== ProtectedAdam-: geometry shaped by Retain subset ===\n",
            "alpha_geom=1.0, beta_geom=10.0, gamma_exp=0.5\n",
            "[Epoch 0 Step 100] loss_new = 1.1522\n",
            "Epoch 0 evaluation:\n",
            "Update new (ProtectedAdam-) perplexity: 2.854\n",
            "Retain protected (ProtectedAdam-) perplexity: 4.918\n",
            "[Epoch 1 Step 100] loss_new = 0.9689\n",
            "Epoch 1 evaluation:\n",
            "Update new (ProtectedAdam-) perplexity: 2.734\n",
            "Retain protected (ProtectedAdam-) perplexity: 5.008\n",
            "[Epoch 2 Step 100] loss_new = 0.8218\n",
            "Epoch 2 evaluation:\n",
            "Update new (ProtectedAdam-) perplexity: 2.670\n",
            "Retain protected (ProtectedAdam-) perplexity: 5.125\n"
          ]
        }
      ],
      "source": [
        "def run_protected_adam(\n",
        "    num_epochs=2,\n",
        "    lr=5e-5,\n",
        "    alpha_geom=1.0,\n",
        "    beta_geom=10.0,\n",
        "    gamma_exp=0.5,\n",
        "    subset_update_every=5,\n",
        "    rho_all=0.99,\n",
        "    rho_sub=0.99,\n",
        "):\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    model = copy.deepcopy(base_model).to(device)\n",
        "    model.train()\n",
        "\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "\n",
        "    state = {}\n",
        "    for p in params:\n",
        "        state[p] = {\n",
        "            \"m\": torch.zeros_like(p.data),\n",
        "            \"v_all\": torch.zeros_like(p.data),\n",
        "            \"v_sub\": torch.zeros_like(p.data),\n",
        "        }\n",
        "\n",
        "    beta1 = 0.9\n",
        "    eps = 1e-6\n",
        "    global_step = 0\n",
        "\n",
        "    def protected_adam_step():\n",
        "        nonlocal global_step\n",
        "        global_step += 1\n",
        "        for p in params:\n",
        "            if p.grad is None:\n",
        "                continue\n",
        "            grad = p.grad.data\n",
        "            s = state[p]\n",
        "\n",
        "            # first moment\n",
        "            s[\"m\"].mul_(beta1).add_(grad, alpha=1 - beta1)\n",
        "\n",
        "            # second moment on \"all\" (new Update) data\n",
        "            s[\"v_all\"].mul_(rho_all).addcmul_(grad, grad, value=1 - rho_all)\n",
        "\n",
        "            v_all = s[\"v_all\"]\n",
        "            v_sub = s[\"v_sub\"]\n",
        "            v_protect = alpha_geom * v_all + beta_geom * v_sub\n",
        "\n",
        "            m_hat = s[\"m\"] / (1 - beta1**global_step)\n",
        "            denom = (v_protect + eps).pow(gamma_exp)\n",
        "            step = m_hat / denom\n",
        "            p.data.add_(step, alpha=-lr)\n",
        "\n",
        "    def update_subset_curvature():\n",
        "        for p in params:\n",
        "            if p.grad is None:\n",
        "                continue\n",
        "            grad = p.grad.data\n",
        "            s = state[p]\n",
        "            s[\"v_sub\"].mul_(rho_sub).addcmul_(grad, grad, value=1 - rho_sub)\n",
        "\n",
        "    retain_iter = iter(retain_loader)\n",
        "\n",
        "    print(\"=== ProtectedAdam-: geometry shaped by Retain subset ===\")\n",
        "    print(f\"alpha_geom={alpha_geom}, beta_geom={beta_geom}, gamma_exp={gamma_exp}\")\n",
        "    for epoch in range(num_epochs):\n",
        "        for step, batch in enumerate(update_loader):\n",
        "            # 1) Update batch: gradient for new task\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            model.zero_grad()\n",
        "            out = model(**batch)\n",
        "            loss = out.loss\n",
        "            loss.backward()\n",
        "\n",
        "            # 2) Take ProtectedAdam step (updates v_all + params)\n",
        "            protected_adam_step()\n",
        "\n",
        "            # 3) Occasionally update subset curvature using Retain\n",
        "            if (step + 1) % subset_update_every == 0:\n",
        "                try:\n",
        "                    retain_batch = next(retain_iter)\n",
        "                except StopIteration:\n",
        "                    retain_iter = iter(retain_loader)\n",
        "                    retain_batch = next(retain_iter)\n",
        "                retain_batch = {k: v.to(device) for k, v in retain_batch.items()}\n",
        "                model.zero_grad()\n",
        "                retain_out = model(**retain_batch)\n",
        "                retain_loss = retain_out.loss\n",
        "                retain_loss.backward()\n",
        "                update_subset_curvature()\n",
        "                model.zero_grad()\n",
        "\n",
        "            if (step + 1) % 100 == 0:\n",
        "                print(f\"[Epoch {epoch} Step {step+1}] loss_new = {loss.item():.4f}\")\n",
        "\n",
        "        print(f\"Epoch {epoch} evaluation:\")\n",
        "        update_ppl = eval_ppl(model, update_test_ds, \"Update new (ProtectedAdam-)\")\n",
        "        retain_ppl  = eval_ppl(model, retain_test_ds,  \"Retain protected (ProtectedAdam-)\")\n",
        "\n",
        "    return model\n",
        "\n",
        "protected_model = run_protected_adam(\n",
        "    num_epochs=3,\n",
        "    lr=3e-4,\n",
        "    alpha_geom=1.0,\n",
        "    beta_geom=10.0,   # strength of protected geometry\n",
        "    gamma_exp=0.5,    # between 0.5 (Adam) and 1.0 (diag NGD)\n",
        "    subset_update_every=5,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHTG9YTgOOAf",
        "outputId": "f4d4a82a-4533-46c7-c6a2-cbc5f86580ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before opt:\n",
            "Update new (ProtectedAdam-) perplexity: 4.730\n",
            "Retain protected (ProtectedAdam-) perplexity: 4.484\n",
            "=== ProtectedAdam-: geometry shaped by Retain subset ===\n",
            "alpha_geom=1.0, beta_geom=10.0, gamma_exp=0.5\n",
            "[Epoch 0 Step 100] loss_new = 1.3416\n",
            "Epoch 0 evaluation:\n",
            "Update new (ProtectedAdam-) perplexity: 2.853\n",
            "Retain protected (ProtectedAdam-) perplexity: 4.900\n",
            "[Epoch 1 Step 100] loss_new = 1.0404\n",
            "Epoch 1 evaluation:\n",
            "Update new (ProtectedAdam-) perplexity: 2.734\n",
            "Retain protected (ProtectedAdam-) perplexity: 4.990\n",
            "[Epoch 2 Step 100] loss_new = 1.0139\n",
            "Epoch 2 evaluation:\n",
            "Update new (ProtectedAdam-) perplexity: 2.670\n",
            "Retain protected (ProtectedAdam-) perplexity: 5.085\n"
          ]
        }
      ],
      "source": [
        "def run_protected_adam2(\n",
        "    num_epochs=3,\n",
        "    lr=5e-5,\n",
        "    alpha_geom=1.0,\n",
        "    beta_geom=10.0,\n",
        "    gamma_exp=0.5,\n",
        "    subset_update_every=5,\n",
        "    rho_all=0.99,\n",
        "    rho_sub=0.99,\n",
        "):\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    model = copy.deepcopy(base_model).to(device)\n",
        "    model.train()\n",
        "\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "\n",
        "    state = {}\n",
        "    for p in params:\n",
        "        state[p] = {\n",
        "            \"m\": torch.zeros_like(p.data),\n",
        "            \"v_all\": torch.zeros_like(p.data),\n",
        "            \"v_sub\": torch.zeros_like(p.data),\n",
        "        }\n",
        "\n",
        "    beta1 = 0.9\n",
        "    eps = 1e-6\n",
        "    global_step = 0\n",
        "\n",
        "    def protected_adam_step():\n",
        "        nonlocal global_step\n",
        "        global_step += 1\n",
        "\n",
        "        # First pass: update moments, compute v_protect, and accumulate\n",
        "        # the mean denominators for =0.5 (baseline) and =gamma_exp\n",
        "        temp = {}\n",
        "        sum_baseline = 0.0\n",
        "        sum_gamma = 0.0\n",
        "        count_tensors = 0\n",
        "\n",
        "        for p in params:\n",
        "            if p.grad is None:\n",
        "                continue\n",
        "            grad = p.grad.data\n",
        "            s = state[p]\n",
        "\n",
        "            # First moment\n",
        "            s[\"m\"].mul_(beta1).add_(grad, alpha=1 - beta1)\n",
        "\n",
        "            # Second moment on \"all\" (new Update) data\n",
        "            s[\"v_all\"].mul_(rho_all).addcmul_(grad, grad, value=1 - rho_all)\n",
        "\n",
        "            v_all = s[\"v_all\"]\n",
        "            v_sub = s[\"v_sub\"]\n",
        "            v_protect = alpha_geom * v_all + beta_geom * v_sub\n",
        "\n",
        "            # Bias-corrected first moment (optional but keeps Adam-like behaviour)\n",
        "            m_hat = s[\"m\"] / (1 - beta1**global_step)\n",
        "\n",
        "            denom_baseline = (v_protect + eps).pow(0.5)\n",
        "            denom_gamma = (v_protect + eps).pow(gamma_exp)\n",
        "\n",
        "            sum_baseline += denom_baseline.mean()\n",
        "            sum_gamma += denom_gamma.mean()\n",
        "            count_tensors += 1\n",
        "\n",
        "            temp[p] = {\n",
        "                \"m_hat\": m_hat,\n",
        "                \"v_protect\": v_protect,\n",
        "            }\n",
        "\n",
        "        if count_tensors == 0:\n",
        "            return\n",
        "\n",
        "        # Renormalization factor so that average step size matches =0.5 case\n",
        "        scale = (sum_baseline / sum_gamma).detach()\n",
        "\n",
        "        # Second pass: apply update with renormalized step size\n",
        "        for p in params:\n",
        "            if p.grad is None or p not in temp:\n",
        "                continue\n",
        "            buf = temp[p]\n",
        "            m_hat = buf[\"m_hat\"]\n",
        "            v_protect = buf[\"v_protect\"]\n",
        "\n",
        "            denom_gamma = (v_protect + eps).pow(gamma_exp)\n",
        "            step = (m_hat / denom_gamma) * scale\n",
        "            p.data.add_(step, alpha=-lr)\n",
        "\n",
        "    def update_subset_curvature():\n",
        "        for p in params:\n",
        "            if p.grad is None:\n",
        "                continue\n",
        "            grad = p.grad.data\n",
        "            s = state[p]\n",
        "            s[\"v_sub\"].mul_(rho_sub).addcmul_(grad, grad, value=1 - rho_sub)\n",
        "\n",
        "    retain_iter = iter(retain_loader)\n",
        "\n",
        "    print(f\"Before opt:\")\n",
        "    update_ppl = eval_ppl(model, update_test_ds, \"Update new (ProtectedAdam-)\")\n",
        "    retain_ppl  = eval_ppl(model, retain_test_ds,  \"Retain protected (ProtectedAdam-)\")\n",
        "\n",
        "\n",
        "    print(\"=== ProtectedAdam-: geometry shaped by Retain subset ===\")\n",
        "    print(f\"alpha_geom={alpha_geom}, beta_geom={beta_geom}, gamma_exp={gamma_exp}\")\n",
        "    for epoch in range(num_epochs):\n",
        "        for step, batch in enumerate(update_loader):\n",
        "            # 1) Update batch: gradient for new task\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            model.zero_grad()\n",
        "            out = model(**batch)\n",
        "            loss = out.loss\n",
        "            loss.backward()\n",
        "\n",
        "            # 2) Take ProtectedAdam step (updates v_all + params)\n",
        "            protected_adam_step()\n",
        "\n",
        "            # 3) Occasionally update subset curvature using Retain\n",
        "            if (step + 1) % subset_update_every == 0:\n",
        "                try:\n",
        "                    retain_batch = next(retain_iter)\n",
        "                except StopIteration:\n",
        "                    retain_iter = iter(retain_loader)\n",
        "                    retain_batch = next(retain_iter)\n",
        "                retain_batch = {k: v.to(device) for k, v in retain_batch.items()}\n",
        "                model.zero_grad()\n",
        "                retain_out = model(**retain_batch)\n",
        "                retain_loss = retain_out.loss\n",
        "                retain_loss.backward()\n",
        "                update_subset_curvature()\n",
        "                model.zero_grad()\n",
        "\n",
        "            if (step + 1) % 100 == 0:\n",
        "                print(f\"[Epoch {epoch} Step {step+1}] loss_new = {loss.item():.4f}\")\n",
        "\n",
        "        print(f\"Epoch {epoch} evaluation:\")\n",
        "        update_ppl = eval_ppl(model, update_test_ds, \"Update new (ProtectedAdam-)\")\n",
        "        retain_ppl  = eval_ppl(model, retain_test_ds,  \"Retain protected (ProtectedAdam-)\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "protected_model2 = run_protected_adam2(\n",
        "    num_epochs=3,\n",
        "    lr=3e-4,\n",
        "    alpha_geom=1.0,\n",
        "    beta_geom=10.0,   # strength of protected geometry\n",
        "    gamma_exp=0.5,    # between 0.5 (Adam) and 1.0 (diag NGD)\n",
        "    subset_update_every=5,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZnYsSOoViX2",
        "outputId": "3a83e68c-b724-4a8a-8b25-c1b0b3bb8e87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Replay baseline: Update training + Retain replay ===\n",
            "subset_update_every=5, replay_weight=1.0\n",
            "Before opt:\n",
            "Update new (replay) perplexity: 4.728\n",
            "Retain protected (replay) perplexity: 4.502\n",
            "[Epoch 0 Step 100] loss_new=0.9579, loss_replay=0.9523, total=1.9102\n",
            "Epoch 0 evaluation:\n",
            "Update new (replay) perplexity: 2.200\n",
            "Retain protected (replay) perplexity: 5.595\n",
            "[Epoch 1 Step 100] loss_new=0.7064, loss_replay=0.7346, total=1.4410\n",
            "Epoch 1 evaluation:\n",
            "Update new (replay) perplexity: 1.863\n",
            "Retain protected (replay) perplexity: 6.173\n",
            "[Epoch 2 Step 100] loss_new=0.5434, loss_replay=0.5460, total=1.0894\n",
            "Epoch 2 evaluation:\n",
            "Update new (replay) perplexity: 1.609\n",
            "Retain protected (replay) perplexity: 6.981\n"
          ]
        }
      ],
      "source": [
        "def run_replay(\n",
        "    num_epochs=2,\n",
        "    lr=5e-5,\n",
        "    subset_update_every=5,\n",
        "    replay_weight=1.0,   # : strength of Retain replay loss\n",
        "):\n",
        "    \"\"\"\n",
        "    Experience Replay baseline.\n",
        "\n",
        "    - Optimizes Update CE loss every step.\n",
        "    - Every `subset_update_every` steps, also optimizes Retain CE.\n",
        "    - Total loss = CE_update + replay_weight * CE_Retain.\n",
        "    - Uses plain AdamW.\n",
        "    - No curvature, no shielding, no geometry.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load model\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    model = copy.deepcopy(base_model).to(device)\n",
        "    model.train()\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "    # Retain iterator for replay\n",
        "    retain_iter = iter(retain_loader)\n",
        "\n",
        "    print(\"=== Replay baseline: Update training + Retain replay ===\")\n",
        "    print(f\"subset_update_every={subset_update_every}, replay_weight={replay_weight}\")\n",
        "\n",
        "    print(\"Before opt:\")\n",
        "    eval_ppl(model, update_test_ds, \"Update new (replay)\")\n",
        "    eval_ppl(model, retain_test_ds,  \"Retain protected (replay)\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for step, batch in enumerate(update_loader):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            # Update forward/backward\n",
        "            model.zero_grad()\n",
        "            out = model(**batch)\n",
        "            loss_new = out.loss\n",
        "            total_loss = loss_new\n",
        "\n",
        "            # Retain replay every N steps\n",
        "            if (step + 1) % subset_update_every == 0:\n",
        "                try:\n",
        "                    ba_batch = next(retain_iter)\n",
        "                except StopIteration:\n",
        "                    retain_iter = iter(retain_loader)\n",
        "                    ba_batch = next(retain_iter)\n",
        "                ba_batch = {k: v.to(device) for k, v in ba_batch.items()}\n",
        "\n",
        "                ba_out = model(**ba_batch)\n",
        "                ba_loss = ba_out.loss\n",
        "\n",
        "                total_loss = loss_new + replay_weight * ba_loss\n",
        "\n",
        "            # Backprop + update\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Logging\n",
        "            if (step + 1) % 100 == 0:\n",
        "                if (step + 1) % subset_update_every == 0:\n",
        "                    print(\n",
        "                        f\"[Epoch {epoch} Step {step+1}] \"\n",
        "                        f\"loss_new={loss_new.item():.4f}, \"\n",
        "                        f\"loss_replay={ba_loss.item():.4f}, \"\n",
        "                        f\"total={total_loss.item():.4f}\"\n",
        "                    )\n",
        "                else:\n",
        "                    print(f\"[Epoch {epoch} Step {step+1}] loss_new={loss_new.item():.4f}\")\n",
        "\n",
        "        # End epoch eval\n",
        "        print(f\"Epoch {epoch} evaluation:\")\n",
        "        eval_ppl(model, update_test_ds, \"Update new (replay)\")\n",
        "        eval_ppl(model, retain_test_ds,  \"Retain protected (replay)\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "replay_model = run_replay(\n",
        "    num_epochs=3,\n",
        "    lr=3e-4,\n",
        "    subset_update_every=5,\n",
        "    replay_weight=1.0,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "jeeW4bw4ZbWo"
      },
      "outputs": [],
      "source": [
        "def estimate_fisher_retain(model, num_batches=200):\n",
        "    model.eval()\n",
        "    fisher = {\n",
        "        name: torch.zeros_like(p.data)\n",
        "        for name, p in model.named_parameters()\n",
        "        if p.requires_grad\n",
        "    }\n",
        "\n",
        "    loader = create_dataloader(retain_train_ds, batch_size)\n",
        "    it = iter(loader)\n",
        "\n",
        "    for i in range(num_batches):\n",
        "        try:\n",
        "            batch = next(it)\n",
        "        except StopIteration:\n",
        "            it = iter(loader)\n",
        "            batch = next(it)\n",
        "\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        model.zero_grad()\n",
        "        out = model(**batch)\n",
        "        loss = out.loss\n",
        "        loss.backward()\n",
        "\n",
        "        for name, p in model.named_parameters():\n",
        "            if not p.requires_grad or p.grad is None:\n",
        "                continue\n",
        "            fisher[name] += p.grad.data.pow(2)\n",
        "\n",
        "    for name in fisher:\n",
        "        fisher[name] /= num_batches\n",
        "\n",
        "    model.train()\n",
        "    return fisher\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "sIlC7y1Vd0Gj"
      },
      "outputs": [],
      "source": [
        "def estimate_model_fisher_retain(model, num_batches=200, top_k=100):\n",
        "    \"\"\"\n",
        "    Compute *model Fisher* diagonal using KL(p_ref || p_model),\n",
        "    with optional top-K truncation of the reference distribution.\n",
        "\n",
        "    top_k < 0   use full distribution (no truncation)\n",
        "    top_k > 0   keep only top_k tokens in reference distribution\n",
        "    \"\"\"\n",
        "\n",
        "    # Freeze reference model 0\n",
        "    ref_model = copy.deepcopy(model).eval().to(device)\n",
        "    for p in ref_model.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    fisher = {\n",
        "        name: torch.zeros_like(p.data)\n",
        "        for name, p in model.named_parameters()\n",
        "        if p.requires_grad\n",
        "    }\n",
        "\n",
        "    loader = create_dataloader(retain_train_ds, batch_size)\n",
        "    it = iter(loader)\n",
        "\n",
        "    for i in range(num_batches):\n",
        "        try:\n",
        "            batch = next(it)\n",
        "        except StopIteration:\n",
        "            it = iter(loader)\n",
        "            batch = next(it)\n",
        "\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        # ---- 1. Reference distribution ----\n",
        "        with torch.no_grad():\n",
        "            ref_logits = ref_model(**batch).logits\n",
        "            ref_probs_full = ref_logits.softmax(dim=-1)  # shape [B, T, V]\n",
        "\n",
        "        # ---- 2. Possibly truncate to top-K ----\n",
        "        if top_k is not None and top_k > 0:\n",
        "            # Get top-K indices for each token\n",
        "            top_vals, top_idx = torch.topk(ref_probs_full, k=top_k, dim=-1)\n",
        "            # Renormalize probs over top-K\n",
        "            ref_probs = top_vals / top_vals.sum(dim=-1, keepdim=True)\n",
        "            # Make a tensor of zeros [B,T,V]\n",
        "            ref_probs_k = torch.zeros_like(ref_probs_full)\n",
        "            # Scatter top-K probabilities back into vocab dimension\n",
        "            ref_probs_k.scatter_(-1, top_idx, ref_probs)\n",
        "            ref_probs = ref_probs_k\n",
        "        else:\n",
        "            # use full distribution\n",
        "            ref_probs = ref_probs_full\n",
        "\n",
        "        # ---- 3. Model logits ----\n",
        "        logits = model(**batch).logits\n",
        "        log_probs = logits.log_softmax(dim=-1)\n",
        "\n",
        "        # ---- 4. KL(p_ref || p_model) ----\n",
        "        # KL per token: _i q_i log(q_i/p_i)\n",
        "        kl = (ref_probs * (ref_probs.log() - log_probs)).sum(dim=-1)\n",
        "        loss = kl.mean()\n",
        "\n",
        "        # ---- 5. Backprop = model Fisher at 0 ----\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # ---- 6. Accumulate grad^2 ----\n",
        "        for name, p in model.named_parameters():\n",
        "            if not p.requires_grad or p.grad is None:\n",
        "                continue\n",
        "            fisher[name] += p.grad.data.pow(2)\n",
        "\n",
        "    # Average\n",
        "    for name in fisher:\n",
        "        fisher[name] /= num_batches\n",
        "\n",
        "    model.train()\n",
        "    return fisher\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmCW04VjZjuS",
        "outputId": "ea63d295-eeb4-4679-e1ba-54b5551efcf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== ProtectedAdam- with precomputed Retain Fisher ===\n",
            "Epoch 0 evaluation:\n",
            "Update new (precomputed-Fisher) perplexity: 3.732\n",
            "Retain protected (precomputed-Fisher) perplexity: 4.396\n",
            "Epoch 1 evaluation:\n",
            "Update new (precomputed-Fisher) perplexity: 3.462\n",
            "Retain protected (precomputed-Fisher) perplexity: 4.436\n",
            "Epoch 2 evaluation:\n",
            "Update new (precomputed-Fisher) perplexity: 3.329\n",
            "Retain protected (precomputed-Fisher) perplexity: 4.441\n",
            "=== ProtectedAdam- with precomputed Retain Fisher ===\n",
            "Epoch 0 evaluation:\n",
            "Update new (precomputed-Fisher) perplexity: 2.894\n",
            "Retain protected (precomputed-Fisher) perplexity: 4.497\n",
            "Epoch 1 evaluation:\n",
            "Update new (precomputed-Fisher) perplexity: 2.758\n",
            "Retain protected (precomputed-Fisher) perplexity: 4.599\n",
            "Epoch 2 evaluation:\n",
            "Update new (precomputed-Fisher) perplexity: 2.690\n",
            "Retain protected (precomputed-Fisher) perplexity: 4.679\n"
          ]
        }
      ],
      "source": [
        "def run_protected_adam_precomputed(\n",
        "    num_epochs=2,\n",
        "    lr=5e-5,\n",
        "    alpha_geom=1.0,\n",
        "    beta_geom=10.0,\n",
        "    gamma_exp=0.5,\n",
        "    rho_all=0.99,\n",
        "    fisher_sub=None,   # dict[name -> tensor]\n",
        "):\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    model = copy.deepcopy(base_model).to(device)\n",
        "    model.train()\n",
        "\n",
        "    # We'll work with named parameters for alignment\n",
        "    named_params = [\n",
        "        (name, p) for name, p in model.named_parameters()\n",
        "        if p.requires_grad\n",
        "    ]\n",
        "\n",
        "    state = {}\n",
        "    for name, p in named_params:\n",
        "        if fisher_sub is not None and name in fisher_sub:\n",
        "            v_sub_init = fisher_sub[name].clone().to(device)\n",
        "        else:\n",
        "            v_sub_init = torch.zeros_like(p.data)\n",
        "\n",
        "        state[name] = {\n",
        "            \"m\": torch.zeros_like(p.data),\n",
        "            \"v_all\": torch.zeros_like(p.data),\n",
        "            \"v_sub\": v_sub_init,\n",
        "        }\n",
        "\n",
        "    beta1 = 0.9\n",
        "    eps = 1e-6\n",
        "    global_step = 0\n",
        "\n",
        "    def protected_adam_step():\n",
        "        nonlocal global_step\n",
        "        global_step += 1\n",
        "        for name, p in named_params:\n",
        "            if p.grad is None:\n",
        "                continue\n",
        "            grad = p.grad.data\n",
        "            s = state[name]\n",
        "\n",
        "            # first moment\n",
        "            s[\"m\"].mul_(beta1).add_(grad, alpha=1 - beta1)\n",
        "\n",
        "            # second moment on \"all\" (new Update) data\n",
        "            s[\"v_all\"].mul_(rho_all).addcmul_(grad, grad, value=1 - rho_all)\n",
        "\n",
        "            v_all = s[\"v_all\"]\n",
        "            v_sub = s[\"v_sub\"]  # fixed precomputed Fisher\n",
        "            v_protect = alpha_geom * v_all + beta_geom * v_sub\n",
        "\n",
        "            m_hat = s[\"m\"] / (1 - beta1**global_step)\n",
        "            denom = (v_protect + eps).pow(gamma_exp)\n",
        "            step = m_hat / denom\n",
        "            p.data.add_(step, alpha=-lr)\n",
        "\n",
        "    print(\"=== ProtectedAdam- with precomputed Retain Fisher ===\")\n",
        "    for epoch in range(num_epochs):\n",
        "        for step, batch in enumerate(update_loader):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            model.zero_grad()\n",
        "            out = model(**batch)\n",
        "            loss = out.loss\n",
        "            loss.backward()\n",
        "            protected_adam_step()\n",
        "\n",
        "        print(f\"Epoch {epoch} evaluation:\")\n",
        "        eval_ppl(model, update_test_ds, \"Update new (precomputed-Fisher)\")\n",
        "        eval_ppl(model, retain_test_ds,  \"Retain protected (precomputed-Fisher)\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# 1. Make a base model for Fisher estimation\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "\n",
        "# 2. Estimate Fisher on Retain ONCE\n",
        "mfisher_retain = estimate_model_fisher_retain(base_model, num_batches=200)\n",
        "\n",
        "# 3. Run Update finetuning using precomputed Fisher, no Retain batches\n",
        "protected_model_pre_mfisher = run_protected_adam_precomputed(\n",
        "    num_epochs=3,\n",
        "    lr=1e-5,\n",
        "    alpha_geom=1.0,\n",
        "    beta_geom=10.0,\n",
        "    gamma_exp=0.5,\n",
        "    rho_all=0.99,\n",
        "    fisher_sub=mfisher_retain,   # <- pass the dict here\n",
        ")\n",
        "\n",
        "\n",
        "# 2. Estimate Fisher on Retain ONCE\n",
        "fisher_retain = estimate_fisher_retain(base_model, num_batches=200)\n",
        "\n",
        "# 3. Run Update finetuning using precomputed Fisher, no Retain batches\n",
        "protected_model_pre = run_protected_adam_precomputed(\n",
        "    num_epochs=3,\n",
        "    lr=3e-4,\n",
        "    alpha_geom=1.0,\n",
        "    beta_geom=10.0,\n",
        "    gamma_exp=0.5,\n",
        "    rho_all=0.99,\n",
        "    fisher_sub=fisher_retain,   # <- pass the dict here\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "id": "Q7ouhy6W_9b7",
        "outputId": "36bb129d-ea7f-4e61-add9-e6baa4b09f51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== ProtectedAdam-precomputed2 (additive): Adam base + Fisher protection ===\n",
            "alpha_geom=1.0, beta_geom=0.1, gamma_exp=0.5, rho_all=0.99, global_vsub_mean=2.467e-08\n",
            "[Epoch 0 Step 100] loss_new = 1.6104\n",
            "Epoch 0 evaluation:\n",
            "Update new (ProtAdam-pre2-add) perplexity: 4.041\n",
            "Retain protected (ProtAdam-pre2-add) perplexity: 4.445\n",
            "[Epoch 1 Step 100] loss_new = 1.2619\n",
            "Epoch 1 evaluation:\n",
            "Update new (ProtAdam-pre2-add) perplexity: 3.767\n",
            "Retain protected (ProtAdam-pre2-add) perplexity: 4.421\n",
            "[Epoch 2 Step 100] loss_new = 1.3105\n",
            "Epoch 2 evaluation:\n",
            "Update new (ProtAdam-pre2-add) perplexity: 3.594\n",
            "Retain protected (ProtAdam-pre2-add) perplexity: 4.397\n"
          ]
        }
      ],
      "source": [
        "# does not work, ignore for now\n",
        "def run_protected_adam_precomputed2(\n",
        "    num_epochs=2,\n",
        "    lr=5e-5,\n",
        "    alpha_geom=1.0,      # scale for v_all (Adam geometry)\n",
        "    beta_geom=10.0,      # strength of protection retainom v_sub\n",
        "    gamma_exp=0.5,       # exponent applied only to normalized v_sub\n",
        "    rho_all=0.99,\n",
        "    fisher_sub=None,     # dict[name -> tensor], precomputed Fisher on Retain\n",
        "):\n",
        "    \"\"\"\n",
        "    Protected Adam with precomputed Fisher (additive version).\n",
        "\n",
        "    - v_all: EMA of grad^2 on Update (new task), like Adam.\n",
        "    - v_sub: fixed Fisher retainom Retain (protected capability), precomputed.\n",
        "    - v_sub is normalized globally once to be dimensionless.\n",
        "\n",
        "    Update (per-parameter i):\n",
        "        v_all_i  EMA of g_i^2\n",
        "        v_sub_i  Fisher_i\n",
        "\n",
        "        v_sub_scaled_i = v_sub_i / global_mean(v_sub)\n",
        "\n",
        "        base_rms_i   = sqrt(alpha_geom * v_all_i)\n",
        "        protect_i    = beta_geom * (v_sub_scaled_i ** gamma_exp)\n",
        "\n",
        "        denom_i = base_rms_i + protect_i + eps\n",
        "\n",
        "        _i = -lr * m_hat_i / denom_i\n",
        "\n",
        "    Properties:\n",
        "      - If fisher_sub is None or beta_geom = 0 -> exactly Adam.\n",
        "      - If v_sub is small -> denom  base_rms -> Adam-like.\n",
        "      - If v_sub is large -> extra additive penalty in denom -> stronger protection.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1) Start retainom base GPT-2\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    model = copy.deepcopy(base_model).to(device)\n",
        "    model.train()\n",
        "\n",
        "    # 2) Collect named parameters to align with fisher_sub[name]\n",
        "    named_params = [\n",
        "        (name, p) for name, p in model.named_parameters()\n",
        "        if p.requires_grad\n",
        "    ]\n",
        "\n",
        "    # 3) Initialize state (m, v_all, v_sub)\n",
        "    state = {}\n",
        "    for name, p in named_params:\n",
        "        v_sub_init = torch.zeros_like(p.data)\n",
        "        if fisher_sub is not None and name in fisher_sub:\n",
        "            v_sub_init = fisher_sub[name].clone().to(p.data.device)\n",
        "        state[name] = {\n",
        "            \"m\": torch.zeros_like(p.data),\n",
        "            \"v_all\": torch.zeros_like(p.data),\n",
        "            \"v_sub\": v_sub_init,\n",
        "        }\n",
        "\n",
        "    # 4) Compute a global mean of v_sub for normalization (dimensionless)\n",
        "    if fisher_sub is not None:\n",
        "        total_sum = 0.0\n",
        "        total_count = 0\n",
        "        for name, p in named_params:\n",
        "            v_sub = state[name][\"v_sub\"]\n",
        "            if v_sub.numel() > 0:\n",
        "                total_sum += v_sub.sum().item()\n",
        "                total_count += v_sub.numel()\n",
        "        if total_count > 0:\n",
        "            global_vsub_mean = total_sum / total_count\n",
        "        else:\n",
        "            global_vsub_mean = 1.0\n",
        "    else:\n",
        "        global_vsub_mean = 1.0\n",
        "\n",
        "    beta1 = 0.9\n",
        "    eps = 1e-8\n",
        "    global_step = 0\n",
        "\n",
        "    print(\"=== ProtectedAdam-precomputed2 (additive): Adam base + Fisher protection ===\")\n",
        "    print(\n",
        "        f\"alpha_geom={alpha_geom}, beta_geom={beta_geom}, \"\n",
        "        f\"gamma_exp={gamma_exp}, rho_all={rho_all}, \"\n",
        "        f\"global_vsub_mean={global_vsub_mean:.3e}\"\n",
        "    )\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for step, batch in enumerate(update_loader):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            # ----- 1) Forward/backward on Update (new task) -----\n",
        "            model.zero_grad()\n",
        "            out = model(**batch)\n",
        "            loss = out.loss\n",
        "            loss.backward()\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "            # ----- 2) Protected Adam step (additive Fisher term) -----\n",
        "            with torch.no_grad():\n",
        "                for name, p in named_params:\n",
        "                    if p.grad is None:\n",
        "                        continue\n",
        "\n",
        "                    g = p.grad.data\n",
        "                    s = state[name]\n",
        "\n",
        "                    # First moment (Adam)\n",
        "                    s[\"m\"].mul_(beta1).add_(g, alpha=1 - beta1)\n",
        "\n",
        "                    # Second moment on \"all\" (new Update) data (Adam-style)\n",
        "                    s[\"v_all\"].mul_(rho_all).addcmul_(g, g, value=1 - rho_all)\n",
        "\n",
        "                    v_all = s[\"v_all\"]\n",
        "                    v_sub = s[\"v_sub\"]\n",
        "\n",
        "                    # Base Adam geometry: sqrt of v_all (scaled)\n",
        "                    base_rms = (alpha_geom * v_all).sqrt()\n",
        "\n",
        "                    # Normalized protective curvature retainom v_sub (dimensionless)\n",
        "                    if fisher_sub is not None and beta_geom != 0.0 and global_vsub_mean > 0.0:\n",
        "                        v_sub_scaled = v_sub / (global_vsub_mean + 1e-12)\n",
        "                        v_sub_scaled = torch.clamp(v_sub_scaled, min=0.0)  # safety\n",
        "                        protect_term = beta_geom * v_sub_scaled.pow(gamma_exp)\n",
        "                    else:\n",
        "                        protect_term = 0.0\n",
        "\n",
        "                    m_hat = s[\"m\"] / (1 - beta1**global_step)\n",
        "\n",
        "                    # ADDITIVE protection: denom = base_rms + protective term\n",
        "                    denom = base_rms + protect_term + eps\n",
        "                    step_dir = m_hat / denom\n",
        "\n",
        "                    p.data.add_(step_dir, alpha=-lr)\n",
        "\n",
        "            if (step + 1) % 100 == 0:\n",
        "                print(f\"[Epoch {epoch} Step {step+1}] loss_new = {loss.item():.4f}\")\n",
        "\n",
        "        # ----- 3) Epoch-end evaluation -----\n",
        "        print(f\"Epoch {epoch} evaluation:\")\n",
        "        update_ppl = eval_ppl(model, update_test_ds, \"Update new (ProtAdam-pre2-add)\")\n",
        "        retain_ppl  = eval_ppl(model, retain_test_ds,  \"Retain protected (ProtAdam-pre2-add)\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# 3. Run Update finetuning using precomputed Fisher, no Retain batches\n",
        "protected_model_pre = run_protected_adam_precomputed2(\n",
        "    num_epochs=3,\n",
        "    lr=3e-4,\n",
        "    alpha_geom=1.0,\n",
        "    beta_geom=0.1,\n",
        "    gamma_exp=0.5,\n",
        "    rho_all=0.99,\n",
        "    fisher_sub=fisher_retain,   # <- pass the dict here\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAgBd4Mko-n2",
        "outputId": "b721b0db-0453-4118-ce6b-5463fb7ae492"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Adam with Fisher KL trust region on Retain ===\n",
            "lr=0.0003, delta_kl=1e-10\n",
            "[Epoch 0 Step 100] loss_new = 1.0596, KL_est = 6.945e-09, scale = 0.120\n",
            "Epoch 0 evaluation:\n",
            "Update new (Adam+KL) perplexity: 2.775\n",
            "Retain protected (Adam+KL) perplexity: 4.491\n",
            "[Epoch 1 Step 100] loss_new = 1.0130, KL_est = 6.120e-09, scale = 0.128\n",
            "Epoch 1 evaluation:\n",
            "Update new (Adam+KL) perplexity: 2.618\n",
            "Retain protected (Adam+KL) perplexity: 4.646\n",
            "[Epoch 2 Step 100] loss_new = 0.8632, KL_est = 6.717e-09, scale = 0.122\n",
            "Epoch 2 evaluation:\n",
            "Update new (Adam+KL) perplexity: 2.552\n",
            "Retain protected (Adam+KL) perplexity: 4.678\n"
          ]
        }
      ],
      "source": [
        "def run_adam_with_fisher_trust_region(\n",
        "    num_epochs=2,\n",
        "    lr=1e-5,\n",
        "    beta1=0.9,\n",
        "    beta2=0.999,\n",
        "    eps=1e-8,\n",
        "    fisher_sub=None,    # dict[name -> tensor] retrain om estimate_fishe_retain_named(...)\n",
        "    delta_kl=1e-3,      # KL budget per step (approx)\n",
        "):\n",
        "    \"\"\"\n",
        "    Adam on Update, with a TRPO-style KL trust region on Retain capability:\n",
        "      1) Compute standard Adam step .\n",
        "      2) Estimate Retain KL  0.5 * _i F_sub[i] * (_i)^2\n",
        "      3) If KL > delta_kl: scale  by sqrt(delta_kl / KL).\n",
        "    \"\"\"\n",
        "\n",
        "    # Start retainom the same base model as elsewhere\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    base_model.resize_token_embeddings(len(tokenizer))\n",
        "    model = copy.deepcopy(base_model).to(device)\n",
        "    model.train()\n",
        "\n",
        "    # Named params for alignment with fisher_sub\n",
        "    named_params = [\n",
        "        (name, p) for name, p in model.named_parameters()\n",
        "        if p.requires_grad\n",
        "    ]\n",
        "\n",
        "    # Adam state\n",
        "    state = {}\n",
        "    for name, p in named_params:\n",
        "        state[name] = {\n",
        "            \"m\": torch.zeros_like(p.data),\n",
        "            \"v\": torch.zeros_like(p.data),\n",
        "        }\n",
        "\n",
        "    global_step = 0\n",
        "\n",
        "    print(\"=== Adam with Fisher KL trust region on Retain ===\")\n",
        "    print(f\"lr={lr}, delta_kl={delta_kl}\")\n",
        "    for epoch in range(num_epochs):\n",
        "        for step, batch in enumerate(update_loader):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            # 1) Forward/backward on Update batch\n",
        "            model.zero_grad()\n",
        "            out = model(**batch)\n",
        "            loss = out.loss\n",
        "            loss.backward()\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "            # 2) Compute Adam proposal step  for each param (WITHOUT applying yet)\n",
        "            proposed_steps = {}  # name -> tensor ()\n",
        "            for name, p in named_params:\n",
        "                if p.grad is None:\n",
        "                    proposed_steps[name] = torch.zeros_like(p.data)\n",
        "                    continue\n",
        "\n",
        "                g = p.grad.data\n",
        "                s = state[name]\n",
        "\n",
        "                # Adam moments\n",
        "                s[\"m\"].mul_(beta1).add_(g, alpha=1 - beta1)\n",
        "                s[\"v\"].mul_(beta2).addcmul_(g, g, value=1 - beta2)\n",
        "\n",
        "                # Bias-corrected\n",
        "                m_hat = s[\"m\"] / (1 - beta1 ** global_step)\n",
        "                v_hat = s[\"v\"] / (1 - beta2 ** global_step)\n",
        "\n",
        "                # Classic Adam step (note: step is *direction*, no lr yet)\n",
        "                step_dir = m_hat / (v_hat.sqrt() + eps)\n",
        "\n",
        "                # Proposed parameter change  = -lr * step_dir\n",
        "                delta_theta = -lr * step_dir\n",
        "                proposed_steps[name] = delta_theta\n",
        "\n",
        "            # 3) Estimate Retain KL for this joint step using precomputed Fisher\n",
        "            kl_est = 0.0\n",
        "            if fisher_sub is not None:\n",
        "                for name, p in named_params:\n",
        "                    if name not in fisher_sub:\n",
        "                        continue\n",
        "                    delta = proposed_steps[name]\n",
        "                    if delta is None:\n",
        "                        continue\n",
        "                    F = fisher_sub[name].to(delta.device)\n",
        "                    # 0.5 * sum_i F_i * (_i)^2\n",
        "                    kl_est += 0.5 * (F * (delta ** 2)).sum().item()\n",
        "\n",
        "            # 4) Compute scaling factor to enforce KL  delta_kl\n",
        "            if fisher_sub is None or kl_est <= 0.0:\n",
        "                scale = 1.0\n",
        "            elif kl_est <= delta_kl:\n",
        "                scale = 1.0\n",
        "            else:\n",
        "                scale = (delta_kl / kl_est) ** 0.5\n",
        "\n",
        "            # 5) Apply scaled step\n",
        "            for name, p in named_params:\n",
        "                delta = proposed_steps[name]\n",
        "                if delta is None:\n",
        "                    continue\n",
        "                p.data.add_(delta * scale)\n",
        "\n",
        "            if (step + 1) % 100 == 0:\n",
        "                print(\n",
        "                    f\"[Epoch {epoch} Step {step+1}] \"\n",
        "                    f\"loss_new = {loss.item():.4f}, KL_est = {kl_est:.3e}, scale = {scale:.3f}\"\n",
        "                )\n",
        "\n",
        "        # 6) Evaluation at epoch end\n",
        "        print(f\"Epoch {epoch} evaluation:\")\n",
        "        update_ppl = eval_ppl(model, update_test_ds, \"Update new (Adam+KL)\")\n",
        "        retain_ppl  = eval_ppl(model, retain_test_ds,  \"Retain protected (Adam+KL)\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Precompute model Fisher on Retain (once)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "base_model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "fisher_retain = estimate_fisher_retain(base_model, num_batches=200)\n",
        "\n",
        "# Now run Update finetuning with TRPO-style KL trust region on Retain\n",
        "adam_trpo_model = run_adam_with_fisher_trust_region(\n",
        "    num_epochs=3,\n",
        "    lr=3e-4,\n",
        "    fisher_sub=fisher_retain,\n",
        "    delta_kl=1e-10,   # tune this up/down\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZW0AGZBHOA7O",
        "outputId": "128551b1-f88f-4f75-e6ef-94955a281746"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Final comparison ===\n",
            "Before training:\n",
            "Update new (before training) perplexity: 4.731\n",
            "Retain protected (before training) perplexity: 4.496\n",
            "Baseline Adam:\n",
            "Update new (baseline) perplexity: 2.890\n",
            "Retain protected (baseline) perplexity: 8.626\n",
            "\n",
            "EWC:\n",
            "Update new (EWC) perplexity: 2.659\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retain protected (EWC) perplexity: 6.835\n",
            "\n",
            "ProtectedAdam-:\n",
            "Update new (ProtectedAdam-) perplexity: 2.669\n",
            "Retain protected (ProtectedAdam-) perplexity: 5.102\n",
            "\n",
            "ProtectedAdam2-:\n",
            "Update new (ProtectedAdam-) perplexity: 2.670\n",
            "Retain protected (ProtectedAdam-) perplexity: 5.074\n",
            "\n",
            "Replay:\n",
            "Update new (ProtectedAdam-) perplexity: 1.609\n",
            "Retain protected (ProtectedAdam-) perplexity: 6.951\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "6.951274370651979"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"=== Final comparison ===\")\n",
        "\n",
        "print(\"Before training:\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "base_model.resize_token_embeddings(len(tokenizer))\n",
        "model = copy.deepcopy(base_model).to(device)\n",
        "\n",
        "eval_ppl(model, update_test_ds, \"Update new (before training)\")\n",
        "eval_ppl(model, retain_test_ds,  \"Retain protected (before training)\")\n",
        "\n",
        "print(\"Baseline Adam:\")\n",
        "eval_ppl(baseline_model, update_test_ds, \"Update new (baseline)\")\n",
        "eval_ppl(baseline_model, retain_test_ds,  \"Retain protected (baseline)\")\n",
        "\n",
        "print(\"\\nEWC:\")\n",
        "eval_ppl(ewc_model, update_test_ds, \"Update new (EWC)\")\n",
        "eval_ppl(ewc_model, retain_test_ds,  \"Retain protected (EWC)\")\n",
        "\n",
        "print(\"\\nProtectedAdam-:\")\n",
        "eval_ppl(protected_model, update_test_ds, \"Update new (ProtectedAdam-)\")\n",
        "eval_ppl(protected_model, retain_test_ds,  \"Retain protected (ProtectedAdam-)\")\n",
        "\n",
        "\n",
        "print(\"\\nProtectedAdam2-:\")\n",
        "eval_ppl(protected_model2, update_test_ds, \"Update new (ProtectedAdam-)\")\n",
        "eval_ppl(protected_model2, retain_test_ds,  \"Retain protected (ProtectedAdam-)\")\n",
        "\n",
        "print(\"\\nReplay:\")\n",
        "eval_ppl(replay_model, update_test_ds, \"Update new (ProtectedAdam-)\")\n",
        "eval_ppl(replay_model, retain_test_ds,  \"Retain protected (ProtectedAdam-)\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00a45ce331f049659ddc5a8d3a733689": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "019d2a1857fe46099f9fc01c6ba6b8d7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a9cd86371154105b73bcf2ede9feea1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4017aa81e3504205b797a47b4541d236",
              "IPY_MODEL_1afded3de67843e89d9ce05dacf97cf6",
              "IPY_MODEL_3d2e1581055140788c0638914354f8ed"
            ],
            "layout": "IPY_MODEL_b07617e6f8d34273a5929b6ed5998120"
          }
        },
        "11481ce4d7c54a20b328fe987088d02b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "149b331d4d87433398ee419064405baa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c48cfdbb01a6438087a163d31de89005",
            "placeholder": "",
            "style": "IPY_MODEL_a8851e5f5d6f46e8860bc772fa5b81ea",
            "value": "model.safetensors:100%"
          }
        },
        "17a88ca67a994b51b5f425842c0ab3ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7143d3bd72f4e1f9e119e1b29205cfb",
            "placeholder": "",
            "style": "IPY_MODEL_cf6bf77b10dc4e04920eb2c05b5a4347",
            "value": "generation_config.json:100%"
          }
        },
        "19a30732e19745b8a333de9eb6332a41": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_46a0746bf84440f38874b729d617f1df",
              "IPY_MODEL_9de635c4e355447e8e2a4d9f760fdfde",
              "IPY_MODEL_8998157212b8431ba8509dd0df98981b"
            ],
            "layout": "IPY_MODEL_9921b84c7f784e7ba9f32a97602546a7"
          }
        },
        "1afded3de67843e89d9ce05dacf97cf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a197dbff6da43a6b445dfe17b2c2522",
            "max": 26,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_412823a0f00e42059339c56fb4bd9c9f",
            "value": 26
          }
        },
        "1f6ddb9d63b442dcb6d2b772c5068212": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21357a168091432fbf46ec848df81842": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5f3adfc279949e58696997e532348ee",
            "placeholder": "",
            "style": "IPY_MODEL_4eb8fcc217cf4b9e98b63c09077a17ec",
            "value": "tokenizer.json:100%"
          }
        },
        "2846c79926f440708121d6b5efa2f901": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38706d9664be4357b79dd614fbac9b27": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3adc4be8a5e442cc8c4d230d8ec3ef85": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3c41a4d44ad349d2accb847f8ec65f41": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3d2e1581055140788c0638914354f8ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7bc5dd35c071495b82c20903d663b66c",
            "placeholder": "",
            "style": "IPY_MODEL_91a9aec8a55f4f418af506009da10f35",
            "value": "26.0/26.0[00:00&lt;00:00,3.32kB/s]"
          }
        },
        "3ee498aad5c54b4c956bb39619959b60": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6c8823a320447f7bae20c76e4070c0b",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8c9e710110a6423b84ade1d41b8a0529",
            "value": 1042301
          }
        },
        "4017aa81e3504205b797a47b4541d236": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c27c084743f643cc972967bcb0eb5753",
            "placeholder": "",
            "style": "IPY_MODEL_6e69c53fb73d44caaf004f4a1baf5330",
            "value": "tokenizer_config.json:100%"
          }
        },
        "412823a0f00e42059339c56fb4bd9c9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4644d3f2f33d4f089ee5d788d8a2a0ef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46a0746bf84440f38874b729d617f1df": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9182aedceb2421b9a77a245d20a9a1e",
            "placeholder": "",
            "style": "IPY_MODEL_95b18249d1884fa7895fb09393d2598b",
            "value": "config.json:100%"
          }
        },
        "46ab261f4cc549e181eeae76c646abaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_21357a168091432fbf46ec848df81842",
              "IPY_MODEL_53dbd36dc7e4462f8d94a403a0d82875",
              "IPY_MODEL_c1bac9dec2244a1e9eef085386d047f1"
            ],
            "layout": "IPY_MODEL_ec9d2bfc9b82450199239de8eada416d"
          }
        },
        "4a197dbff6da43a6b445dfe17b2c2522": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a2226e944a4410aa53f1717e07169b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e9ea7427ef0479c9b0216fdb9604c8a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4eb8fcc217cf4b9e98b63c09077a17ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f1ed9d96a5a4693ac360e9f90e42b8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "503d3af66d1c4b47ac0f9fd4caf475ae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53dbd36dc7e4462f8d94a403a0d82875": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4644d3f2f33d4f089ee5d788d8a2a0ef",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_38706d9664be4357b79dd614fbac9b27",
            "value": 1355256
          }
        },
        "5df1c3945e404094b3f18c099b502a84": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00a45ce331f049659ddc5a8d3a733689",
            "placeholder": "",
            "style": "IPY_MODEL_b7e41e3eac364027a5a1bf7342f7bd49",
            "value": "1.04M/1.04M[00:00&lt;00:00,1.57MB/s]"
          }
        },
        "6099f135e0814a47822b3749934696b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2f3c8562c204b1d9222851eed5f106f",
            "placeholder": "",
            "style": "IPY_MODEL_f521b4a4d7154de6ba16f3e6bdf59b51",
            "value": "vocab.json:100%"
          }
        },
        "65ac5143b332424ca9424506b20cd40a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "675449c0ef484dc3b3baae5bbe8b5fd5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e69c53fb73d44caaf004f4a1baf5330": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7681bec193b248ad8beca1bd01baf4c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2846c79926f440708121d6b5efa2f901",
            "placeholder": "",
            "style": "IPY_MODEL_3adc4be8a5e442cc8c4d230d8ec3ef85",
            "value": "124/124[00:00&lt;00:00,16.4kB/s]"
          }
        },
        "7805724c2c5448d4b29119483f055636": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7bc5dd35c071495b82c20903d663b66c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dddedc6cfcb46848461c6aa231650ec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8998157212b8431ba8509dd0df98981b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_675449c0ef484dc3b3baae5bbe8b5fd5",
            "placeholder": "",
            "style": "IPY_MODEL_65ac5143b332424ca9424506b20cd40a",
            "value": "665/665[00:00&lt;00:00,85.9kB/s]"
          }
        },
        "8bdf857cc0fb474f9bf8c73fa0ea2257": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_17a88ca67a994b51b5f425842c0ab3ff",
              "IPY_MODEL_ecdf1063c4cc48cab41115fbda94cc85",
              "IPY_MODEL_7681bec193b248ad8beca1bd01baf4c6"
            ],
            "layout": "IPY_MODEL_1f6ddb9d63b442dcb6d2b772c5068212"
          }
        },
        "8c9e710110a6423b84ade1d41b8a0529": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8d5615ddaac748d7a1ba5a26726aa6fb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ebe0ad1bb4e456a97821b31d8dfb9f6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ee340f1e44442aca9ad56eaf28e3ec6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91a9aec8a55f4f418af506009da10f35": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92bf0487d45e4c06869ae88ee0bb877c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95b18249d1884fa7895fb09393d2598b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9921b84c7f784e7ba9f32a97602546a7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9de635c4e355447e8e2a4d9f760fdfde": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11481ce4d7c54a20b328fe987088d02b",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e97ed5800cff4055900545ff376979be",
            "value": 665
          }
        },
        "a1fc267bd68947dfbafa9ed3e6fb56c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a2f3c8562c204b1d9222851eed5f106f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8851e5f5d6f46e8860bc772fa5b81ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b07617e6f8d34273a5929b6ed5998120": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0fa9161ed064445aece8d321b3d88a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ee340f1e44442aca9ad56eaf28e3ec6",
            "max": 548105171,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3c41a4d44ad349d2accb847f8ec65f41",
            "value": 548105171
          }
        },
        "b5f3adfc279949e58696997e532348ee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7e41e3eac364027a5a1bf7342f7bd49": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bbe725cd4bf4424490b1b6444fd128a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_149b331d4d87433398ee419064405baa",
              "IPY_MODEL_b0fa9161ed064445aece8d321b3d88a7",
              "IPY_MODEL_cf32259002024b7da355cc4eb0a752e8"
            ],
            "layout": "IPY_MODEL_503d3af66d1c4b47ac0f9fd4caf475ae"
          }
        },
        "c199e25df4d54001919f0481edc0b358": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e7c2de72aff4428db80fd3ddd8d82557",
              "IPY_MODEL_cb45de6c1d7d4dc7809c42da301e9fba",
              "IPY_MODEL_d8e0603c141f4674a1f57399cee4ef45"
            ],
            "layout": "IPY_MODEL_8d5615ddaac748d7a1ba5a26726aa6fb"
          }
        },
        "c1bac9dec2244a1e9eef085386d047f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ebe0ad1bb4e456a97821b31d8dfb9f6",
            "placeholder": "",
            "style": "IPY_MODEL_4a2226e944a4410aa53f1717e07169b5",
            "value": "1.36M/1.36M[00:00&lt;00:00,6.35MB/s]"
          }
        },
        "c27c084743f643cc972967bcb0eb5753": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c48cfdbb01a6438087a163d31de89005": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb45de6c1d7d4dc7809c42da301e9fba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d928ee45c23f436a9aa89ca5ef53df92",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ff49ae10aa48498dbb0cc35ab638c298",
            "value": 456318
          }
        },
        "ce768476c2144ff58e413e5195528b39": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf32259002024b7da355cc4eb0a752e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce768476c2144ff58e413e5195528b39",
            "placeholder": "",
            "style": "IPY_MODEL_4f1ed9d96a5a4693ac360e9f90e42b8a",
            "value": "548M/548M[00:01&lt;00:00,360MB/s]"
          }
        },
        "cf6bf77b10dc4e04920eb2c05b5a4347": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2e42c54ade243779ec71eb5fd4a51bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6099f135e0814a47822b3749934696b3",
              "IPY_MODEL_3ee498aad5c54b4c956bb39619959b60",
              "IPY_MODEL_5df1c3945e404094b3f18c099b502a84"
            ],
            "layout": "IPY_MODEL_7dddedc6cfcb46848461c6aa231650ec"
          }
        },
        "d8e0603c141f4674a1f57399cee4ef45": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_019d2a1857fe46099f9fc01c6ba6b8d7",
            "placeholder": "",
            "style": "IPY_MODEL_92bf0487d45e4c06869ae88ee0bb877c",
            "value": "456k/456k[00:00&lt;00:00,1.07MB/s]"
          }
        },
        "d928ee45c23f436a9aa89ca5ef53df92": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6c8823a320447f7bae20c76e4070c0b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7c2de72aff4428db80fd3ddd8d82557": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e9ea7427ef0479c9b0216fdb9604c8a",
            "placeholder": "",
            "style": "IPY_MODEL_7805724c2c5448d4b29119483f055636",
            "value": "merges.txt:100%"
          }
        },
        "e97ed5800cff4055900545ff376979be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ec9d2bfc9b82450199239de8eada416d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecdf1063c4cc48cab41115fbda94cc85": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed40a4f1820246b99ca964b03244c906",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a1fc267bd68947dfbafa9ed3e6fb56c9",
            "value": 124
          }
        },
        "ed40a4f1820246b99ca964b03244c906": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f521b4a4d7154de6ba16f3e6bdf59b51": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f7143d3bd72f4e1f9e119e1b29205cfb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9182aedceb2421b9a77a245d20a9a1e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff49ae10aa48498dbb0cc35ab638c298": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
