[
  {
    "module": "lighteval.tasks.tasks.triviaqa",
    "docstring": {
      "name": "Triviaqa",
      "dataset": [
        "mandarjoshi/trivia_qa"
      ],
      "abstract": "TriviaqQA is a reading comprehension dataset containing over 650K\nquestion-answer-evidence triples. TriviaqQA includes 95K question-answer pairs\nauthored by trivia enthusiasts and independently gathered evidence documents,\nsix per question on average, that provide high quality distant supervision for\nanswering the questions.",
      "languages": [
        "english"
      ],
      "tags": [
        "qa"
      ],
      "paper": "https://arxiv.org/abs/1705.03551"
    },
    "tasks": [
      {
        "name": "triviaqa",
        "config": {
          "name": "triviaqa",
          "prompt_function": "<function triviaqa_prompt at 0x7717f36d22a0>",
          "hf_repo": "mandarjoshi/trivia_qa",
          "hf_subset": "rc.nocontext",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0decbf0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "20",
          "generation_grammar": "None",
          "stop_sequence": "['\\n', '.', ',']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.xcopa",
    "docstring": {
      "name": "Xcopa",
      "dataset": [
        "cambridgeltl/xcopa"
      ],
      "abstract": "XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning The Cross-lingual\nChoice of Plausible Alternatives dataset is a benchmark to evaluate the ability\nof machine learning models to transfer commonsense reasoning across languages.",
      "languages": [
        "english"
      ],
      "tags": [
        "commonsense",
        "multilingual",
        "multiple-choice",
        "reasoning"
      ],
      "paper": "https://arxiv.org/abs/2005.00333"
    },
    "tasks": [
      {
        "name": "xcopa:en",
        "config": {
          "name": "xcopa:en",
          "prompt_function": "<function xcopa_en_prompt at 0x7717f36d23e0>",
          "hf_repo": "cambridgeltl/xcopa",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0decf20>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test', 'train', 'validation')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "xcopa:et",
        "config": {
          "name": "xcopa:et",
          "prompt_function": "<function xcopa_et_prompt at 0x7717f36d2480>",
          "hf_repo": "cambridgeltl/xcopa",
          "hf_subset": "et",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0ded2b0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test', 'train', 'validation')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "xcopa:ht",
        "config": {
          "name": "xcopa:ht",
          "prompt_function": "<function xcopa_ht_prompt at 0x7717f36d2520>",
          "hf_repo": "cambridgeltl/xcopa",
          "hf_subset": "ht",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0ded6a0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test', 'train', 'validation')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "xcopa:it",
        "config": {
          "name": "xcopa:it",
          "prompt_function": "<function xcopa_it_prompt at 0x7717f36d25c0>",
          "hf_repo": "cambridgeltl/xcopa",
          "hf_subset": "it",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0deda90>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test', 'train', 'validation')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "xcopa:id",
        "config": {
          "name": "xcopa:id",
          "prompt_function": "<function xcopa_id_prompt at 0x7717f36d2660>",
          "hf_repo": "cambridgeltl/xcopa",
          "hf_subset": "id",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0dede80>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test', 'train', 'validation')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "xcopa:qu",
        "config": {
          "name": "xcopa:qu",
          "prompt_function": "<function xcopa_qu_prompt at 0x7717f36d2700>",
          "hf_repo": "cambridgeltl/xcopa",
          "hf_subset": "qu",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0dee270>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test', 'train', 'validation')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "xcopa:sw",
        "config": {
          "name": "xcopa:sw",
          "prompt_function": "<function xcopa_sw_prompt at 0x7717f36d27a0>",
          "hf_repo": "cambridgeltl/xcopa",
          "hf_subset": "sw",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0dee630>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test', 'train', 'validation')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "xcopa:zh",
        "config": {
          "name": "xcopa:zh",
          "prompt_function": "<function xcopa_zh_prompt at 0x7717f36d2840>",
          "hf_repo": "cambridgeltl/xcopa",
          "hf_subset": "zh",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0deea20>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test', 'train', 'validation')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "xcopa:ta",
        "config": {
          "name": "xcopa:ta",
          "prompt_function": "<function xcopa_ta_prompt at 0x7717f36d28e0>",
          "hf_repo": "cambridgeltl/xcopa",
          "hf_subset": "ta",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0deede0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test', 'train', 'validation')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "xcopa:th",
        "config": {
          "name": "xcopa:th",
          "prompt_function": "<function xcopa_th_prompt at 0x7717f36d2980>",
          "hf_repo": "cambridgeltl/xcopa",
          "hf_subset": "th",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0def1d0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test', 'train', 'validation')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "xcopa:tr",
        "config": {
          "name": "xcopa:tr",
          "prompt_function": "<function xcopa_tr_prompt at 0x7717f36d2a20>",
          "hf_repo": "cambridgeltl/xcopa",
          "hf_subset": "tr",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0def5c0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test', 'train', 'validation')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "xcopa:vi",
        "config": {
          "name": "xcopa:vi",
          "prompt_function": "<function xcopa_vi_prompt at 0x7717f36d2ac0>",
          "hf_repo": "cambridgeltl/xcopa",
          "hf_subset": "vi",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0def980>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test', 'train', 'validation')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.the_pile",
    "docstring": {
      "name": "The Pile",
      "dataset": [
        "lighteval/pile_helm"
      ],
      "abstract": "The Pile corpus for measuring lanugage model performance across various domains.",
      "languages": [
        "english"
      ],
      "tags": [
        "language-modeling"
      ],
      "paper": "https://arxiv.org/abs/2101.00027"
    },
    "tasks": [
      {
        "name": "the_pile:arxiv",
        "config": {
          "name": "the_pile:arxiv",
          "prompt_function": "<function the_pile_prompt at 0x7717f36d2b60>",
          "hf_repo": "lighteval/pile_helm",
          "hf_subset": "arxiv",
          "metrics": "({'metric_name': 'word_perplexity', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0defe60>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0defe90>, 'batched_compute': False}, {'metric_name': 'byte_perplexity', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0defec0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0defef0>, 'batched_compute': False}, {'metric_name': 'bits_per_byte', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0deff50>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0deffb0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "the_pile:bibliotik",
        "config": {
          "name": "the_pile:bibliotik",
          "prompt_function": "<function the_pile_prompt at 0x7717f36d2b60>",
          "hf_repo": "lighteval/pile_helm",
          "hf_subset": "bibliotik",
          "metrics": "({'metric_name': 'word_perplexity', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c142f0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c14350>, 'batched_compute': False}, {'metric_name': 'byte_perplexity', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c143b0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c14410>, 'batched_compute': False}, {'metric_name': 'bits_per_byte', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c14470>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c144d0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "the_pile:commoncrawl",
        "config": {
          "name": "the_pile:commoncrawl",
          "prompt_function": "<function the_pile_prompt at 0x7717f36d2b60>",
          "hf_repo": "lighteval/pile_helm",
          "hf_subset": "commoncrawl",
          "metrics": "({'metric_name': 'word_perplexity', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c147d0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c14830>, 'batched_compute': False}, {'metric_name': 'byte_perplexity', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c14890>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c148f0>, 'batched_compute': False}, {'metric_name': 'bits_per_byte', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c14950>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c149b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "the_pile:dm-mathematics",
        "config": {
          "name": "the_pile:dm-mathematics",
          "prompt_function": "<function the_pile_prompt at 0x7717f36d2b60>",
          "hf_repo": "lighteval/pile_helm",
          "hf_subset": "dm-mathematics",
          "metrics": "({'metric_name': 'word_perplexity', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c14cb0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c14d10>, 'batched_compute': False}, {'metric_name': 'byte_perplexity', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c14d70>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c14dd0>, 'batched_compute': False}, {'metric_name': 'bits_per_byte', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c14e30>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c14e90>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "the_pile:enron",
        "config": {
          "name": "the_pile:enron",
          "prompt_function": "<function the_pile_prompt at 0x7717f36d2b60>",
          "hf_repo": "lighteval/pile_helm",
          "hf_subset": "enron",
          "metrics": "({'metric_name': 'word_perplexity', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c15190>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c151f0>, 'batched_compute': False}, {'metric_name': 'byte_perplexity', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c15250>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c152b0>, 'batched_compute': False}, {'metric_name': 'bits_per_byte', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c15310>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c15370>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "the_pile:europarl",
        "config": {
          "name": "the_pile:europarl",
          "prompt_function": "<function the_pile_prompt at 0x7717f36d2b60>",
          "hf_repo": "lighteval/pile_helm",
          "hf_subset": "europarl",
          "metrics": "({'metric_name': 'word_perplexity', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c15670>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c156d0>, 'batched_compute': False}, {'metric_name': 'byte_perplexity', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c15730>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c15790>, 'batched_compute': False}, {'metric_name': 'bits_per_byte', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c157f0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c15850>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "the_pile:freelaw",
        "config": {
          "name": "the_pile:freelaw",
          "prompt_function": "<function the_pile_prompt at 0x7717f36d2b60>",
          "hf_repo": "lighteval/pile_helm",
          "hf_subset": "freelaw",
          "metrics": "({'metric_name': 'word_perplexity', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c15b50>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c15bb0>, 'batched_compute': False}, {'metric_name': 'byte_perplexity', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c15c10>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c15c70>, 'batched_compute': False}, {'metric_name': 'bits_per_byte', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c15cd0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c15d30>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "the_pile:github",
        "config": {
          "name": "the_pile:github",
          "prompt_function": "<function the_pile_prompt at 0x7717f36d2b60>",
          "hf_repo": "lighteval/pile_helm",
          "hf_subset": "github",
          "metrics": "({'metric_name': 'word_perplexity', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c16030>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c16090>, 'batched_compute': False}, {'metric_name': 'byte_perplexity', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c160f0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c16150>, 'batched_compute': False}, {'metric_name': 'bits_per_byte', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c161b0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c16210>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "the_pile:gutenberg",
        "config": {
          "name": "the_pile:gutenberg",
          "prompt_function": "<function the_pile_prompt at 0x7717f36d2b60>",
          "hf_repo": "lighteval/pile_helm",
          "hf_subset": "gutenberg",
          "metrics": "({'metric_name': 'word_perplexity', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c16510>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c16570>, 'batched_compute': False}, {'metric_name': 'byte_perplexity', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c165d0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c16630>, 'batched_compute': False}, {'metric_name': 'bits_per_byte', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c16690>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c166f0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "the_pile:hackernews",
        "config": {
          "name": "the_pile:hackernews",
          "prompt_function": "<function the_pile_prompt at 0x7717f36d2b60>",
          "hf_repo": "lighteval/pile_helm",
          "hf_subset": "hackernews",
          "metrics": "({'metric_name': 'word_perplexity', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c169f0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c16a50>, 'batched_compute': False}, {'metric_name': 'byte_perplexity', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c16ab0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c16b10>, 'batched_compute': False}, {'metric_name': 'bits_per_byte', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c16b70>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c16bd0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "the_pile:nih-exporter",
        "config": {
          "name": "the_pile:nih-exporter",
          "prompt_function": "<function the_pile_prompt at 0x7717f36d2b60>",
          "hf_repo": "lighteval/pile_helm",
          "hf_subset": "nih-exporter",
          "metrics": "({'metric_name': 'word_perplexity', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c16ed0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c16f30>, 'batched_compute': False}, {'metric_name': 'byte_perplexity', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c16f90>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c16ff0>, 'batched_compute': False}, {'metric_name': 'bits_per_byte', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c17050>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c170b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "the_pile:opensubtitles",
        "config": {
          "name": "the_pile:opensubtitles",
          "prompt_function": "<function the_pile_prompt at 0x7717f36d2b60>",
          "hf_repo": "lighteval/pile_helm",
          "hf_subset": "opensubtitles",
          "metrics": "({'metric_name': 'word_perplexity', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c173b0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c17410>, 'batched_compute': False}, {'metric_name': 'byte_perplexity', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c17470>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c174d0>, 'batched_compute': False}, {'metric_name': 'bits_per_byte', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c17530>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c17590>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "the_pile:openwebtext2",
        "config": {
          "name": "the_pile:openwebtext2",
          "prompt_function": "<function the_pile_prompt at 0x7717f36d2b60>",
          "hf_repo": "lighteval/pile_helm",
          "hf_subset": "openwebtext2",
          "metrics": "({'metric_name': 'word_perplexity', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c17890>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c178f0>, 'batched_compute': False}, {'metric_name': 'byte_perplexity', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c17950>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c179b0>, 'batched_compute': False}, {'metric_name': 'bits_per_byte', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c17a10>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c17a70>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "the_pile:pubmed-abstracts",
        "config": {
          "name": "the_pile:pubmed-abstracts",
          "prompt_function": "<function the_pile_prompt at 0x7717f36d2b60>",
          "hf_repo": "lighteval/pile_helm",
          "hf_subset": "pubmed-abstracts",
          "metrics": "({'metric_name': 'word_perplexity', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c17d70>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c17dd0>, 'batched_compute': False}, {'metric_name': 'byte_perplexity', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c17e30>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c17e90>, 'batched_compute': False}, {'metric_name': 'bits_per_byte', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c17ef0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c17f50>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "the_pile:pubmed-central",
        "config": {
          "name": "the_pile:pubmed-central",
          "prompt_function": "<function the_pile_prompt at 0x7717f36d2b60>",
          "hf_repo": "lighteval/pile_helm",
          "hf_subset": "pubmed-central",
          "metrics": "({'metric_name': 'word_perplexity', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c18290>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c182f0>, 'batched_compute': False}, {'metric_name': 'byte_perplexity', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c18350>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c183b0>, 'batched_compute': False}, {'metric_name': 'bits_per_byte', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c18410>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c18470>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "the_pile:stackexchange",
        "config": {
          "name": "the_pile:stackexchange",
          "prompt_function": "<function the_pile_prompt at 0x7717f36d2b60>",
          "hf_repo": "lighteval/pile_helm",
          "hf_subset": "stackexchange",
          "metrics": "({'metric_name': 'word_perplexity', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c18770>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c187d0>, 'batched_compute': False}, {'metric_name': 'byte_perplexity', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c18830>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c18890>, 'batched_compute': False}, {'metric_name': 'bits_per_byte', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c188f0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c18950>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "the_pile:upsto",
        "config": {
          "name": "the_pile:upsto",
          "prompt_function": "<function the_pile_prompt at 0x7717f36d2b60>",
          "hf_repo": "lighteval/pile_helm",
          "hf_subset": "uspto",
          "metrics": "({'metric_name': 'word_perplexity', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c18c50>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c18cb0>, 'batched_compute': False}, {'metric_name': 'byte_perplexity', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c18d10>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c18d70>, 'batched_compute': False}, {'metric_name': 'bits_per_byte', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c18dd0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c18e30>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "the_pile:wikipedia",
        "config": {
          "name": "the_pile:wikipedia",
          "prompt_function": "<function the_pile_prompt at 0x7717f36d2b60>",
          "hf_repo": "lighteval/pile_helm",
          "hf_subset": "wikipedia",
          "metrics": "({'metric_name': 'word_perplexity', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c19130>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c19190>, 'batched_compute': False}, {'metric_name': 'byte_perplexity', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c191f0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c19250>, 'batched_compute': False}, {'metric_name': 'bits_per_byte', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c192b0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c19310>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "the_pile:youtubesubtitles",
        "config": {
          "name": "the_pile:youtubesubtitles",
          "prompt_function": "<function the_pile_prompt at 0x7717f36d2b60>",
          "hf_repo": "lighteval/pile_helm",
          "hf_subset": "youtubesubtitles",
          "metrics": "({'metric_name': 'word_perplexity', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c19610>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c19670>, 'batched_compute': False}, {'metric_name': 'byte_perplexity', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c196d0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c19730>, 'batched_compute': False}, {'metric_name': 'bits_per_byte', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0c19790>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0c197f0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.bigbench_hard",
    "docstring": {
      "name": "Bigbench Hard",
      "dataset": [
        "lighteval/bbh"
      ],
      "tags": [
        "reasoning"
      ]
    },
    "tasks": [
      {
        "name": "bigbench_hard:causal_judgment",
        "config": {
          "name": "bigbench_hard:causal_judgment",
          "prompt_function": "<function bbh_prompt at 0x7717f36d2d40>",
          "hf_repo": "lighteval/bbh",
          "hf_subset": "causal_judgement",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c19b80>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f36d2e80>]",
          "scorer": "<function choice.<locals>.score at 0x7717f36d2fc0>",
          "sample_fields": "<function record_to_sample at 0x7717f36d2de0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['</s>', 'Q=', '\\n\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench_hard:date_understanding",
        "config": {
          "name": "bigbench_hard:date_understanding",
          "prompt_function": "<function bbh_prompt at 0x7717f36d2d40>",
          "hf_repo": "lighteval/bbh",
          "hf_subset": "date_understanding",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c19ee0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f36d3100>]",
          "scorer": "<function choice.<locals>.score at 0x7717f3542020>",
          "sample_fields": "<function record_to_sample at 0x7717f36d2de0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['</s>', 'Q=', '\\n\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench_hard:disambiguation_qa",
        "config": {
          "name": "bigbench_hard:disambiguation_qa",
          "prompt_function": "<function bbh_prompt at 0x7717f36d2d40>",
          "hf_repo": "lighteval/bbh",
          "hf_subset": "disambiguation_qa",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c1a240>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f3542160>]",
          "scorer": "<function choice.<locals>.score at 0x7717f3542200>",
          "sample_fields": "<function record_to_sample at 0x7717f36d2de0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['</s>', 'Q=', '\\n\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench_hard:geometric_shapes",
        "config": {
          "name": "bigbench_hard:geometric_shapes",
          "prompt_function": "<function bbh_prompt at 0x7717f36d2d40>",
          "hf_repo": "lighteval/bbh",
          "hf_subset": "geometric_shapes",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c1a5a0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f3542340>]",
          "scorer": "<function choice.<locals>.score at 0x7717f35423e0>",
          "sample_fields": "<function record_to_sample at 0x7717f36d2de0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['</s>', 'Q=', '\\n\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench_hard:logical_deduction_five_objects",
        "config": {
          "name": "bigbench_hard:logical_deduction_five_objects",
          "prompt_function": "<function bbh_prompt at 0x7717f36d2d40>",
          "hf_repo": "lighteval/bbh",
          "hf_subset": "logical_deduction_five_objects",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c1a900>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f3542520>]",
          "scorer": "<function choice.<locals>.score at 0x7717f35425c0>",
          "sample_fields": "<function record_to_sample at 0x7717f36d2de0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['</s>', 'Q=', '\\n\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench_hard:logical_deduction_seven_objects",
        "config": {
          "name": "bigbench_hard:logical_deduction_seven_objects",
          "prompt_function": "<function bbh_prompt at 0x7717f36d2d40>",
          "hf_repo": "lighteval/bbh",
          "hf_subset": "logical_deduction_seven_objects",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c1ac60>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f3542700>]",
          "scorer": "<function choice.<locals>.score at 0x7717f35427a0>",
          "sample_fields": "<function record_to_sample at 0x7717f36d2de0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['</s>', 'Q=', '\\n\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench_hard:logical_deduction_three_objects",
        "config": {
          "name": "bigbench_hard:logical_deduction_three_objects",
          "prompt_function": "<function bbh_prompt at 0x7717f36d2d40>",
          "hf_repo": "lighteval/bbh",
          "hf_subset": "logical_deduction_three_objects",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c1af90>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f35428e0>]",
          "scorer": "<function choice.<locals>.score at 0x7717f3542980>",
          "sample_fields": "<function record_to_sample at 0x7717f36d2de0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['</s>', 'Q=', '\\n\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench_hard:movie_recommendation",
        "config": {
          "name": "bigbench_hard:movie_recommendation",
          "prompt_function": "<function bbh_prompt at 0x7717f36d2d40>",
          "hf_repo": "lighteval/bbh",
          "hf_subset": "movie_recommendation",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c1b2f0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f3542ac0>]",
          "scorer": "<function choice.<locals>.score at 0x7717f3542b60>",
          "sample_fields": "<function record_to_sample at 0x7717f36d2de0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['</s>', 'Q=', '\\n\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench_hard:navigate",
        "config": {
          "name": "bigbench_hard:navigate",
          "prompt_function": "<function bbh_prompt at 0x7717f36d2d40>",
          "hf_repo": "lighteval/bbh",
          "hf_subset": "navigate",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c1b650>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f3542ca0>]",
          "scorer": "<function choice.<locals>.score at 0x7717f3542d40>",
          "sample_fields": "<function record_to_sample at 0x7717f36d2de0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['</s>', 'Q=', '\\n\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench_hard:reasoning_about_colored_objects",
        "config": {
          "name": "bigbench_hard:reasoning_about_colored_objects",
          "prompt_function": "<function bbh_prompt at 0x7717f36d2d40>",
          "hf_repo": "lighteval/bbh",
          "hf_subset": "reasoning_about_colored_objects",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c1b9b0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f3542e80>]",
          "scorer": "<function choice.<locals>.score at 0x7717f3542f20>",
          "sample_fields": "<function record_to_sample at 0x7717f36d2de0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['</s>', 'Q=', '\\n\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench_hard:ruin_names",
        "config": {
          "name": "bigbench_hard:ruin_names",
          "prompt_function": "<function bbh_prompt at 0x7717f36d2d40>",
          "hf_repo": "lighteval/bbh",
          "hf_subset": "ruin_names",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c1bd10>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f3543060>]",
          "scorer": "<function choice.<locals>.score at 0x7717f3543100>",
          "sample_fields": "<function record_to_sample at 0x7717f36d2de0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['</s>', 'Q=', '\\n\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench_hard:salient_translation_error_detection",
        "config": {
          "name": "bigbench_hard:salient_translation_error_detection",
          "prompt_function": "<function bbh_prompt at 0x7717f36d2d40>",
          "hf_repo": "lighteval/bbh",
          "hf_subset": "salient_translation_error_detection",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c200b0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f3543240>]",
          "scorer": "<function choice.<locals>.score at 0x7717f35432e0>",
          "sample_fields": "<function record_to_sample at 0x7717f36d2de0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['</s>', 'Q=', '\\n\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench_hard:snarks",
        "config": {
          "name": "bigbench_hard:snarks",
          "prompt_function": "<function bbh_prompt at 0x7717f36d2d40>",
          "hf_repo": "lighteval/bbh",
          "hf_subset": "snarks",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c20410>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f3543380>]",
          "scorer": "<function choice.<locals>.score at 0x7717f35434c0>",
          "sample_fields": "<function record_to_sample at 0x7717f36d2de0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['</s>', 'Q=', '\\n\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench_hard:sports_understanding",
        "config": {
          "name": "bigbench_hard:sports_understanding",
          "prompt_function": "<function bbh_prompt at 0x7717f36d2d40>",
          "hf_repo": "lighteval/bbh",
          "hf_subset": "sports_understanding",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c20770>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f3543560>]",
          "scorer": "<function choice.<locals>.score at 0x7717f35436a0>",
          "sample_fields": "<function record_to_sample at 0x7717f36d2de0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['</s>', 'Q=', '\\n\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench_hard:temporal_sequences",
        "config": {
          "name": "bigbench_hard:temporal_sequences",
          "prompt_function": "<function bbh_prompt at 0x7717f36d2d40>",
          "hf_repo": "lighteval/bbh",
          "hf_subset": "temporal_sequences",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c20ad0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f35437e0>]",
          "scorer": "<function choice.<locals>.score at 0x7717f3543880>",
          "sample_fields": "<function record_to_sample at 0x7717f36d2de0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['</s>', 'Q=', '\\n\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench_hard:tracking_shuffled_objects_five_objects",
        "config": {
          "name": "bigbench_hard:tracking_shuffled_objects_five_objects",
          "prompt_function": "<function bbh_prompt at 0x7717f36d2d40>",
          "hf_repo": "lighteval/bbh",
          "hf_subset": "tracking_shuffled_objects_five_objects",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c20e30>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f35439c0>]",
          "scorer": "<function choice.<locals>.score at 0x7717f3543a60>",
          "sample_fields": "<function record_to_sample at 0x7717f36d2de0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['</s>', 'Q=', '\\n\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench_hard:tracking_shuffled_objects_seven_objects",
        "config": {
          "name": "bigbench_hard:tracking_shuffled_objects_seven_objects",
          "prompt_function": "<function bbh_prompt at 0x7717f36d2d40>",
          "hf_repo": "lighteval/bbh",
          "hf_subset": "tracking_shuffled_objects_seven_objects",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c21190>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f3543ba0>]",
          "scorer": "<function choice.<locals>.score at 0x7717f3543c40>",
          "sample_fields": "<function record_to_sample at 0x7717f36d2de0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['</s>', 'Q=', '\\n\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench_hard:tracking_shuffled_objects_three_objects",
        "config": {
          "name": "bigbench_hard:tracking_shuffled_objects_three_objects",
          "prompt_function": "<function bbh_prompt at 0x7717f36d2d40>",
          "hf_repo": "lighteval/bbh",
          "hf_subset": "tracking_shuffled_objects_three_objects",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c214f0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f3543d80>]",
          "scorer": "<function choice.<locals>.score at 0x7717f3543e20>",
          "sample_fields": "<function record_to_sample at 0x7717f36d2de0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['</s>', 'Q=', '\\n\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.glue",
    "docstring": {
      "name": "GLUE",
      "dataset": [
        "nyu-mll/glue",
        "aps/super_glue"
      ],
      "abstract": "The General Language Understanding Evaluation (GLUE) benchmark is a collection\nof resources for training, evaluating, and analyzing natural language\nunderstanding systems.",
      "languages": [
        "english"
      ],
      "tags": [
        "classification",
        "language-understanding"
      ]
    },
    "tasks": [
      {
        "name": "glue:cola",
        "config": {
          "name": "glue:cola",
          "prompt_function": "<function cola_prompt at 0x7717f3554360>",
          "hf_repo": "nyu-mll/glue",
          "hf_subset": "cola",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c21940>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'mcc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.LoglikelihoodPreparator object at 0x7717f1828800>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.MatthewsCorrCoef object at 0x7717f0c218e0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test', 'train', 'validation')",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "glue:mnli",
        "config": {
          "name": "glue:mnli",
          "prompt_function": "<function mnli_prompt at 0x7717f3554400>",
          "hf_repo": "nyu-mll/glue",
          "hf_subset": "mnli_matched",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c21ca0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'validation')",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "glue:mnli_mismatched",
        "config": {
          "name": "glue:mnli_mismatched",
          "prompt_function": "<function mnli_prompt at 0x7717f3554400>",
          "hf_repo": "nyu-mll/glue",
          "hf_subset": "mnli_mismatched",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c22090>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'validation')",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "glue:mrpc",
        "config": {
          "name": "glue:mrpc",
          "prompt_function": "<function mrpc_prompt at 0x7717f35544a0>",
          "hf_repo": "nyu-mll/glue",
          "hf_subset": "mrpc",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c22480>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'loglikelihood_f1', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.LoglikelihoodPreparator object at 0x7717f0c224e0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelF1Score object at 0x7717f0c22510>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test', 'train', 'validation')",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "glue:qnli",
        "config": {
          "name": "glue:qnli",
          "prompt_function": "<function qnli_prompt at 0x7717f3554540>",
          "hf_repo": "nyu-mll/glue",
          "hf_subset": "qnli",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c22840>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test', 'train', 'validation')",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "glue:qqp",
        "config": {
          "name": "glue:qqp",
          "prompt_function": "<function qqp_prompt at 0x7717f35545e0>",
          "hf_repo": "nyu-mll/glue",
          "hf_subset": "qqp",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c22c30>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'loglikelihood_f1', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.LoglikelihoodPreparator object at 0x7717f0c22c90>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelF1Score object at 0x7717f0c22cc0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'validation', 'test')",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "glue:rte",
        "config": {
          "name": "glue:rte",
          "prompt_function": "<function rte_prompt at 0x7717f3554680>",
          "hf_repo": "nyu-mll/glue",
          "hf_subset": "rte",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c23020>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test', 'train', 'validation')",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "glue:sst2",
        "config": {
          "name": "glue:sst2",
          "prompt_function": "<function sst_prompt at 0x7717f3554720>",
          "hf_repo": "nyu-mll/glue",
          "hf_subset": "sst2",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c233e0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test', 'train', 'validation')",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "glue:stsb",
        "config": {
          "name": "glue:stsb",
          "prompt_function": "<function stsb_prompt at 0x7717f35547c0>",
          "hf_repo": "nyu-mll/glue",
          "hf_subset": "stsb",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c237a0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test', 'train', 'validation')",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "glue:wnli",
        "config": {
          "name": "glue:wnli",
          "prompt_function": "<function wnli_prompt at 0x7717f3554860>",
          "hf_repo": "nyu-mll/glue",
          "hf_subset": "wnli",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c23b90>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test', 'train', 'validation')",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "super_glue:boolq",
        "config": {
          "name": "super_glue:boolq",
          "prompt_function": "<function boolq_harness_prompt at 0x7717f3543ec0>",
          "hf_repo": "aps/super_glue",
          "hf_subset": "boolq",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c23f80>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test', 'train', 'validation')",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "super_glue:cb",
        "config": {
          "name": "super_glue:cb",
          "prompt_function": "<function cb_prompt at 0x7717f3554040>",
          "hf_repo": "aps/super_glue",
          "hf_subset": "cb",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c28380>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'mf1', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.LoglikelihoodPreparator object at 0x7717f0c283e0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelF1Score object at 0x7717f0c28440>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test', 'train', 'validation')",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "super_glue:copa",
        "config": {
          "name": "super_glue:copa",
          "prompt_function": "<function copa_prompt at 0x7717f35540e0>",
          "hf_repo": "aps/super_glue",
          "hf_subset": "copa",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c287a0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test', 'train', 'validation')",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "super_glue:rte",
        "config": {
          "name": "super_glue:rte",
          "prompt_function": "<function rte_prompt at 0x7717f3554680>",
          "hf_repo": "aps/super_glue",
          "hf_subset": "rte",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c28b90>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test', 'train', 'validation')",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "super_glue:multirc",
        "config": {
          "name": "super_glue:multirc",
          "prompt_function": "<function multirc_prompt at 0x7717f3554180>",
          "hf_repo": "aps/super_glue",
          "hf_subset": "multirc",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c28f80>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'validation')",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "super_glue:wic",
        "config": {
          "name": "super_glue:wic",
          "prompt_function": "<function wic_prompt at 0x7717f3554220>",
          "hf_repo": "aps/super_glue",
          "hf_subset": "wic",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c29370>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test', 'train', 'validation')",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "super_glue:wsc",
        "config": {
          "name": "super_glue:wsc",
          "prompt_function": "<function wsc_prompt at 0x7717f35542c0>",
          "hf_repo": "aps/super_glue",
          "hf_subset": "wsc",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c29760>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test', 'train', 'validation')",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.coqa",
    "docstring": {
      "name": "Coqa",
      "dataset": [
        "stanfordnlp/coqa"
      ],
      "abstract": "CoQA is a large-scale dataset for building Conversational Question Answering\nsystems. The goal of the CoQA challenge is to measure the ability of machines to\nunderstand a text passage and answer a series of interconnected questions that\nappear in a conversation.",
      "languages": [
        "english"
      ],
      "tags": [
        "dialog",
        "qa"
      ],
      "paper": "https://arxiv.org/abs/1808.07042"
    },
    "tasks": [
      {
        "name": "coqa",
        "config": {
          "name": "coqa",
          "prompt_function": "<function coqa_prompt at 0x7717f3554900>",
          "hf_repo": "stanfordnlp/coqa",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c29cd0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'validation')",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "100",
          "generation_grammar": "None",
          "stop_sequence": "['\\n', 'Question:', 'question:']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "1"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.bigbench",
    "docstring": {
      "name": "Bigbench",
      "dataset": [
        "tasksource/bigbench"
      ],
      "abstract": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models\n166 tasks from bigbench benchmark.",
      "languages": [
        "english"
      ],
      "tags": [
        "reasoning"
      ],
      "paper": "https://arxiv.org/abs/2206.04615"
    },
    "tasks": [
      {
        "name": "bigbench:abstract_narrative_understanding",
        "config": {
          "name": "bigbench:abstract_narrative_understanding",
          "prompt_function": "<function bigbench_prompt at 0x7717f3554b80>",
          "hf_repo": "tasksource/bigbench",
          "hf_subset": "abstract_narrative_understanding",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c2a150>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f3554cc0>]",
          "scorer": "<function choice.<locals>.score at 0x7717f3554e00>",
          "sample_fields": "<function record_to_sample at 0x7717f3554c20>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('default', 'train', 'validation')",
          "evaluation_splits": "('default',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench:anachronisms",
        "config": {
          "name": "bigbench:anachronisms",
          "prompt_function": "<function bigbench_prompt at 0x7717f3554b80>",
          "hf_repo": "tasksource/bigbench",
          "hf_subset": "anachronisms",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c2a480>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f3554f40>]",
          "scorer": "<function choice.<locals>.score at 0x7717f3554fe0>",
          "sample_fields": "<function record_to_sample at 0x7717f3554c20>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('default', 'train', 'validation')",
          "evaluation_splits": "('default',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench:analogical_similarity",
        "config": {
          "name": "bigbench:analogical_similarity",
          "prompt_function": "<function bigbench_prompt at 0x7717f3554b80>",
          "hf_repo": "tasksource/bigbench",
          "hf_subset": "analogical_similarity",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c2a7e0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f3555120>]",
          "scorer": "<function choice.<locals>.score at 0x7717f35551c0>",
          "sample_fields": "<function record_to_sample at 0x7717f3554c20>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('default', 'train', 'validation')",
          "evaluation_splits": "('default',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench:moral_permissibility",
        "config": {
          "name": "bigbench:moral_permissibility",
          "prompt_function": "<function bigbench_prompt at 0x7717f3554b80>",
          "hf_repo": "tasksource/bigbench",
          "hf_subset": "moral_permissibility",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c2ab40>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('default', 'train', 'validation')",
          "evaluation_splits": "('default',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench:movie_dialog_same_or_different",
        "config": {
          "name": "bigbench:movie_dialog_same_or_different",
          "prompt_function": "<function bigbench_prompt at 0x7717f3554b80>",
          "hf_repo": "tasksource/bigbench",
          "hf_subset": "movie_dialog_same_or_different",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c2af30>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('default', 'train', 'validation')",
          "evaluation_splits": "('default',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench:movie_recommendation",
        "config": {
          "name": "bigbench:movie_recommendation",
          "prompt_function": "<function bigbench_prompt at 0x7717f3554b80>",
          "hf_repo": "tasksource/bigbench",
          "hf_subset": "movie_recommendation",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c2b320>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('default', 'train', 'validation')",
          "evaluation_splits": "('default',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench:mult_data_wrangling",
        "config": {
          "name": "bigbench:mult_data_wrangling",
          "prompt_function": "<function bigbench_prompt at 0x7717f3554b80>",
          "hf_repo": "tasksource/bigbench",
          "hf_subset": "mult_data_wrangling",
          "metrics": "({'metric_name': 'em:strip_strings=False', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c2b710>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f3588f40>]",
          "scorer": "<function choice.<locals>.score at 0x7717f3588fe0>",
          "sample_fields": "<function record_to_sample at 0x7717f3554c20>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('default', 'train', 'validation')",
          "evaluation_splits": "('default',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench:simple_ethical_questions",
        "config": {
          "name": "bigbench:simple_ethical_questions",
          "prompt_function": "<function bigbench_prompt at 0x7717f3554b80>",
          "hf_repo": "tasksource/bigbench",
          "hf_subset": "simple_ethical_questions",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c2ba70>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('default', 'train', 'validation')",
          "evaluation_splits": "('default',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench:simple_text_editing",
        "config": {
          "name": "bigbench:simple_text_editing",
          "prompt_function": "<function bigbench_prompt at 0x7717f3554b80>",
          "hf_repo": "tasksource/bigbench",
          "hf_subset": "simple_text_editing",
          "metrics": "({'metric_name': 'em:strip_strings=False', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c2be60>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('default', 'train', 'validation')",
          "evaluation_splits": "('default',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench:snarks",
        "config": {
          "name": "bigbench:snarks",
          "prompt_function": "<function bigbench_prompt at 0x7717f3554b80>",
          "hf_repo": "tasksource/bigbench",
          "hf_subset": "snarks",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c38290>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('default', 'train', 'validation')",
          "evaluation_splits": "('default',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench:social_iqa",
        "config": {
          "name": "bigbench:social_iqa",
          "prompt_function": "<function bigbench_prompt at 0x7717f3554b80>",
          "hf_repo": "tasksource/bigbench",
          "hf_subset": "social_iqa",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c38680>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('default', 'train', 'validation')",
          "evaluation_splits": "('default',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench:social_support",
        "config": {
          "name": "bigbench:social_support",
          "prompt_function": "<function bigbench_prompt at 0x7717f3554b80>",
          "hf_repo": "tasksource/bigbench",
          "hf_subset": "social_support",
          "metrics": "({'metric_name': 'f1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0c38a40>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelF1Score object at 0x7717f0c38aa0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f35894e0>]",
          "scorer": "<function choice.<locals>.score at 0x7717f3589580>",
          "sample_fields": "<function record_to_sample at 0x7717f3554c20>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('default', 'train', 'validation')",
          "evaluation_splits": "('default',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench:sports_understanding",
        "config": {
          "name": "bigbench:sports_understanding",
          "prompt_function": "<function bigbench_prompt at 0x7717f3554b80>",
          "hf_repo": "tasksource/bigbench",
          "hf_subset": "sports_understanding",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c38dd0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('default', 'train', 'validation')",
          "evaluation_splits": "('default',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench:strange_stories",
        "config": {
          "name": "bigbench:strange_stories",
          "prompt_function": "<function bigbench_whitespace_after_query_prompt at 0x7717f3554ae0>",
          "hf_repo": "tasksource/bigbench",
          "hf_subset": "strange_stories",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c391c0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('default', 'train', 'validation')",
          "evaluation_splits": "('default',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench:strategyqa",
        "config": {
          "name": "bigbench:strategyqa",
          "prompt_function": "<function bigbench_linefeed_before_whitespace_after_query_prompt at 0x7717f3554a40>",
          "hf_repo": "tasksource/bigbench",
          "hf_subset": "strategyqa",
          "metrics": "({'metric_name': 'bleu', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0c39550>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f18287d0>, 'batched_compute': False}, {'metric_name': ['rouge1', 'rouge2', 'rougeL', 'rougeLsum'], 'higher_is_better': {'rouge1': True, 'rouge2': True, 'rougeL': True, 'rougeLsum': True}, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ROUGE object at 0x7717f0c394f0>, 'corpus_level_fn': {'rouge1': <function mean at 0x771aa6d5b3b0>, 'rouge2': <function mean at 0x771aa6d5b3b0>, 'rougeL': <function mean at 0x771aa6d5b3b0>, 'rougeLsum': <function mean at 0x771aa6d5b3b0>}, 'batched_compute': False}, {'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c395b0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('default', 'train', 'validation')",
          "evaluation_splits": "('default',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench:sufficient_information",
        "config": {
          "name": "bigbench:sufficient_information",
          "prompt_function": "<function bigbench_prompt at 0x7717f3554b80>",
          "hf_repo": "tasksource/bigbench",
          "hf_subset": "sufficient_information",
          "metrics": "({'metric_name': 'em:strip_strings=False', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c398e0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('default', 'train', 'validation')",
          "evaluation_splits": "('default',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench:suicide_risk",
        "config": {
          "name": "bigbench:suicide_risk",
          "prompt_function": "<function bigbench_prompt at 0x7717f3554b80>",
          "hf_repo": "tasksource/bigbench",
          "hf_subset": "suicide_risk",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c39cd0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('default', 'train', 'validation')",
          "evaluation_splits": "('default',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench:swahili_english_proverbs",
        "config": {
          "name": "bigbench:swahili_english_proverbs",
          "prompt_function": "<function bigbench_prompt at 0x7717f3554b80>",
          "hf_repo": "tasksource/bigbench",
          "hf_subset": "swahili_english_proverbs",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c3a0c0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('default', 'train', 'validation')",
          "evaluation_splits": "('default',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench:swedish_to_german_proverbs",
        "config": {
          "name": "bigbench:swedish_to_german_proverbs",
          "prompt_function": "<function bigbench_prompt at 0x7717f3554b80>",
          "hf_repo": "tasksource/bigbench",
          "hf_subset": "swedish_to_german_proverbs",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c3a4b0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('default', 'train', 'validation')",
          "evaluation_splits": "('default',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench:symbol_interpretation",
        "config": {
          "name": "bigbench:symbol_interpretation",
          "prompt_function": "<function bigbench_linefeed_before_whitespace_after_query_prompt at 0x7717f3554a40>",
          "hf_repo": "tasksource/bigbench",
          "hf_subset": "symbol_interpretation",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c3a8a0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('default', 'train', 'validation')",
          "evaluation_splits": "('default',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench:tellmewhy",
        "config": {
          "name": "bigbench:tellmewhy",
          "prompt_function": "<function bigbench_prompt at 0x7717f3554b80>",
          "hf_repo": "tasksource/bigbench",
          "hf_subset": "tellmewhy",
          "metrics": "({'metric_name': 'bleu', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0c3ac90>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0c3acf0>, 'batched_compute': False}, {'metric_name': ['rouge1', 'rouge2', 'rougeL', 'rougeLsum'], 'higher_is_better': {'rouge1': True, 'rouge2': True, 'rougeL': True, 'rougeLsum': True}, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ROUGE object at 0x7717f0c3ad20>, 'corpus_level_fn': {'rouge1': <function mean at 0x771aa6d5b3b0>, 'rouge2': <function mean at 0x771aa6d5b3b0>, 'rougeL': <function mean at 0x771aa6d5b3b0>, 'rougeLsum': <function mean at 0x771aa6d5b3b0>}, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('default', 'train', 'validation')",
          "evaluation_splits": "('default',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench:temporal_sequences",
        "config": {
          "name": "bigbench:temporal_sequences",
          "prompt_function": "<function bigbench_prompt at 0x7717f3554b80>",
          "hf_repo": "tasksource/bigbench",
          "hf_subset": "temporal_sequences",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c3b080>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('default', 'train', 'validation')",
          "evaluation_splits": "('default',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench:tense",
        "config": {
          "name": "bigbench:tense",
          "prompt_function": "<function bigbench_prompt at 0x7717f3554b80>",
          "hf_repo": "tasksource/bigbench",
          "hf_subset": "tense",
          "metrics": "({'metric_name': 'em:strip_strings=False', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c3b470>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('default', 'train', 'validation')",
          "evaluation_splits": "('default',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench:timedial",
        "config": {
          "name": "bigbench:timedial",
          "prompt_function": "<function bigbench_prompt at 0x7717f3554b80>",
          "hf_repo": "tasksource/bigbench",
          "hf_subset": "timedial",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c3b860>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('default', 'train', 'validation')",
          "evaluation_splits": "('default',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench:topical_chat",
        "config": {
          "name": "bigbench:topical_chat",
          "prompt_function": "<function bigbench_prompt at 0x7717f3554b80>",
          "hf_repo": "tasksource/bigbench",
          "hf_subset": "topical_chat",
          "metrics": "({'metric_name': 'bleu', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0c3bc50>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0c3bcb0>, 'batched_compute': False}, {'metric_name': ['rouge1', 'rouge2', 'rougeL', 'rougeLsum'], 'higher_is_better': {'rouge1': True, 'rouge2': True, 'rougeL': True, 'rougeLsum': True}, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ROUGE object at 0x7717f0c3bd10>, 'corpus_level_fn': {'rouge1': <function mean at 0x771aa6d5b3b0>, 'rouge2': <function mean at 0x771aa6d5b3b0>, 'rougeL': <function mean at 0x771aa6d5b3b0>, 'rougeLsum': <function mean at 0x771aa6d5b3b0>}, 'batched_compute': False}, {'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c3bd70>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'bleurt', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.BLEURT object at 0x7717f0c3bdd0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f35896c0>]",
          "scorer": "<function choice.<locals>.score at 0x7717f3589760>",
          "sample_fields": "<function record_to_sample at 0x7717f3554c20>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('default', 'train', 'validation')",
          "evaluation_splits": "('default',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench:tracking_shuffled_objects",
        "config": {
          "name": "bigbench:tracking_shuffled_objects",
          "prompt_function": "<function bigbench_prompt at 0x7717f3554b80>",
          "hf_repo": "tasksource/bigbench",
          "hf_subset": "tracking_shuffled_objects",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c3c080>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('default', 'train', 'validation')",
          "evaluation_splits": "('default',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench:understanding_fables",
        "config": {
          "name": "bigbench:understanding_fables",
          "prompt_function": "<function bigbench_prompt at 0x7717f3554b80>",
          "hf_repo": "tasksource/bigbench",
          "hf_subset": "understanding_fables",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c3c470>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('default', 'train', 'validation')",
          "evaluation_splits": "('default',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench:undo_permutation",
        "config": {
          "name": "bigbench:undo_permutation",
          "prompt_function": "<function bigbench_prompt at 0x7717f3554b80>",
          "hf_repo": "tasksource/bigbench",
          "hf_subset": "undo_permutation",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c3c860>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('default', 'train', 'validation')",
          "evaluation_splits": "('default',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench:unit_conversion",
        "config": {
          "name": "bigbench:unit_conversion",
          "prompt_function": "<function bigbench_prompt at 0x7717f3554b80>",
          "hf_repo": "tasksource/bigbench",
          "hf_subset": "unit_conversion",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c3cc50>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('default', 'train', 'validation')",
          "evaluation_splits": "('default',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench:unit_interpretation",
        "config": {
          "name": "bigbench:unit_interpretation",
          "prompt_function": "<function bigbench_prompt at 0x7717f3554b80>",
          "hf_repo": "tasksource/bigbench",
          "hf_subset": "unit_interpretation",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c3d040>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('default', 'train', 'validation')",
          "evaluation_splits": "('default',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench:unnatural_in_context_learning",
        "config": {
          "name": "bigbench:unnatural_in_context_learning",
          "prompt_function": "<function bigbench_prompt at 0x7717f3554b80>",
          "hf_repo": "tasksource/bigbench",
          "hf_subset": "unnatural_in_context_learning",
          "metrics": "({'metric_name': 'em:strip_strings=False', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c3d430>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('default', 'train', 'validation')",
          "evaluation_splits": "('default',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench:vitaminc_fact_verification",
        "config": {
          "name": "bigbench:vitaminc_fact_verification",
          "prompt_function": "<function bigbench_whitespace_after_query_prompt at 0x7717f3554ae0>",
          "hf_repo": "tasksource/bigbench",
          "hf_subset": "vitaminc_fact_verification",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c3d7f0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('default', 'train', 'validation')",
          "evaluation_splits": "('default',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench:what_is_the_tao",
        "config": {
          "name": "bigbench:what_is_the_tao",
          "prompt_function": "<function bigbench_prompt at 0x7717f3554b80>",
          "hf_repo": "tasksource/bigbench",
          "hf_subset": "what_is_the_tao",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c3dbe0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('default', 'train', 'validation')",
          "evaluation_splits": "('default',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench:which_wiki_edit",
        "config": {
          "name": "bigbench:which_wiki_edit",
          "prompt_function": "<function bigbench_prompt at 0x7717f3554b80>",
          "hf_repo": "tasksource/bigbench",
          "hf_subset": "which_wiki_edit",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c3dfd0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('default', 'train', 'validation')",
          "evaluation_splits": "('default',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench:winowhy",
        "config": {
          "name": "bigbench:winowhy",
          "prompt_function": "<function bigbench_whitespace_after_query_prompt at 0x7717f3554ae0>",
          "hf_repo": "tasksource/bigbench",
          "hf_subset": "winowhy",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c3e3c0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('default', 'train', 'validation')",
          "evaluation_splits": "('default',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench:word_sorting",
        "config": {
          "name": "bigbench:word_sorting",
          "prompt_function": "<function bigbench_prompt at 0x7717f3554b80>",
          "hf_repo": "tasksource/bigbench",
          "hf_subset": "word_sorting",
          "metrics": "({'metric_name': 'em:strip_strings=False', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c3e7b0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('default', 'train', 'validation')",
          "evaluation_splits": "('default',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench:word_unscrambling",
        "config": {
          "name": "bigbench:word_unscrambling",
          "prompt_function": "<function bigbench_prompt at 0x7717f3554b80>",
          "hf_repo": "tasksource/bigbench",
          "hf_subset": "word_unscrambling",
          "metrics": "({'metric_name': 'em:strip_strings=False', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c3eb70>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('default', 'train', 'validation')",
          "evaluation_splits": "('default',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.entitymatching",
    "docstring": {
      "name": "Entitymatching",
      "dataset": [
        "lighteval/EntityMatching"
      ],
      "abstract": "Simple entity matching benchmark.",
      "languages": [
        "english"
      ],
      "tags": [
        "classification",
        "reasoning"
      ],
      "paper": "https://dl.acm.org/doi/10.14778/3007263.3007314"
    },
    "tasks": [
      {
        "name": "entity_matching:Abt_Buy",
        "config": {
          "name": "entity_matching:Abt_Buy",
          "prompt_function": "<function entity_matching_prompt at 0x7717f3589800>",
          "hf_repo": "lighteval/EntityMatching",
          "hf_subset": "Abt_Buy",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c3f020>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "entity_matching:Amazon_Google",
        "config": {
          "name": "entity_matching:Amazon_Google",
          "prompt_function": "<function entity_matching_prompt at 0x7717f3589800>",
          "hf_repo": "lighteval/EntityMatching",
          "hf_subset": "Amazon_Google",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c3f3e0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "entity_matching:Beer",
        "config": {
          "name": "entity_matching:Beer",
          "prompt_function": "<function entity_matching_prompt at 0x7717f3589800>",
          "hf_repo": "lighteval/EntityMatching",
          "hf_subset": "Beer",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c3f7a0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "entity_matching:Company",
        "config": {
          "name": "entity_matching:Company",
          "prompt_function": "<function entity_matching_prompt at 0x7717f3589800>",
          "hf_repo": "lighteval/EntityMatching",
          "hf_subset": "Company",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c3fb60>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "entity_matching:DBLP_ACM",
        "config": {
          "name": "entity_matching:DBLP_ACM",
          "prompt_function": "<function entity_matching_prompt at 0x7717f3589800>",
          "hf_repo": "lighteval/EntityMatching",
          "hf_subset": "DBLP_ACM",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c3ff20>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "entity_matching:DBLP_GoogleScholar",
        "config": {
          "name": "entity_matching:DBLP_GoogleScholar",
          "prompt_function": "<function entity_matching_prompt at 0x7717f3589800>",
          "hf_repo": "lighteval/EntityMatching",
          "hf_subset": "DBLP_GoogleScholar",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c482f0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "entity_matching:Dirty_DBLP_ACM",
        "config": {
          "name": "entity_matching:Dirty_DBLP_ACM",
          "prompt_function": "<function entity_matching_prompt at 0x7717f3589800>",
          "hf_repo": "lighteval/EntityMatching",
          "hf_subset": "Dirty_DBLP_ACM",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c486b0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "entity_matching:Dirty_DBLP_GoogleScholar",
        "config": {
          "name": "entity_matching:Dirty_DBLP_GoogleScholar",
          "prompt_function": "<function entity_matching_prompt at 0x7717f3589800>",
          "hf_repo": "lighteval/EntityMatching",
          "hf_subset": "Dirty_DBLP_GoogleScholar",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c48a70>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "entity_matching:Dirty_Walmart_Amazon",
        "config": {
          "name": "entity_matching:Dirty_Walmart_Amazon",
          "prompt_function": "<function entity_matching_prompt at 0x7717f3589800>",
          "hf_repo": "lighteval/EntityMatching",
          "hf_subset": "Dirty_Walmart_Amazon",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c48e30>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "entity_matching:Dirty_iTunes_Amazon",
        "config": {
          "name": "entity_matching:Dirty_iTunes_Amazon",
          "prompt_function": "<function entity_matching_prompt at 0x7717f3589800>",
          "hf_repo": "lighteval/EntityMatching",
          "hf_subset": "Dirty_iTunes_Amazon",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c491f0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "entity_matching=Fodors_Zagats",
        "config": {
          "name": "entity_matching=Fodors_Zagats",
          "prompt_function": "<function entity_matching_prompt at 0x7717f3589800>",
          "hf_repo": "lighteval/EntityMatching",
          "hf_subset": "Fodors_Zagats",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c495b0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "entity_matching:Walmart_Amazon",
        "config": {
          "name": "entity_matching:Walmart_Amazon",
          "prompt_function": "<function entity_matching_prompt at 0x7717f3589800>",
          "hf_repo": "lighteval/EntityMatching",
          "hf_subset": "Walmart_Amazon",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c49970>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "entity_matching:iTunes_Amazon",
        "config": {
          "name": "entity_matching:iTunes_Amazon",
          "prompt_function": "<function entity_matching_prompt at 0x7717f3589800>",
          "hf_repo": "lighteval/EntityMatching",
          "hf_subset": "iTunes_Amazon",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c49d30>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.natural_questions",
    "docstring": {
      "name": "Natural Questions",
      "dataset": [
        "lighteval/small_natural_questions"
      ],
      "abstract": "This dataset is a collection of question-answer pairs from the Natural Questions\ndataset. See Natural Questions for additional information. This dataset can be\nused directly with Sentence Transformers to train embedding models.",
      "languages": [
        "english"
      ],
      "tags": [
        "general-knowledge",
        "qa"
      ],
      "paper": "https://ai.google.com/research/NaturalQuestions"
    },
    "tasks": [
      {
        "name": "natural_questions",
        "config": {
          "name": "natural_questions",
          "prompt_function": "<function get_mcq_prompt_function.<locals>.prompt_fn at 0x7717f3589c60>",
          "hf_repo": "lighteval/small_natural_questions",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c4a1e0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'validation', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "few_shot",
          "few_shots_select": "None",
          "generation_size": "250",
          "generation_grammar": "None",
          "stop_sequence": "['\\n', 'Question:', 'question:']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "1"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.qa4mre",
    "docstring": {
      "name": "Qa4Mre",
      "dataset": [
        "qa4mre"
      ],
      "abstract": "QA4MRE is a machine reading comprehension benchmark from the CLEF 2011-2013\nchallenges. It evaluates systems' ability to answer questions requiring deep\nunderstanding of short texts, supported by external background knowledge.\nCovering tasks like modality, negation, biomedical reading, and entrance exams,\nQA4MRE tests reasoning beyond surface-level text matching.",
      "languages": [
        "english"
      ],
      "tags": [
        "biomedical",
        "health",
        "qa"
      ],
      "paper": "https://link.springer.com/chapter/10.1007/978-3-642-40802-1_29"
    },
    "tasks": [
      {
        "name": "qa4mre:2011",
        "config": {
          "name": "qa4mre:2011",
          "prompt_function": "<function qa4mre_prompt at 0x7717f3589f80>",
          "hf_repo": "qa4mre",
          "hf_subset": "2011.main.EN",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c4a720>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "qa4mre:2012",
        "config": {
          "name": "qa4mre:2012",
          "prompt_function": "<function qa4mre_prompt at 0x7717f3589f80>",
          "hf_repo": "qa4mre",
          "hf_subset": "2012.main.EN",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c4ab40>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "qa4mre:2013",
        "config": {
          "name": "qa4mre:2013",
          "prompt_function": "<function qa4mre_prompt at 0x7717f3589f80>",
          "hf_repo": "qa4mre",
          "hf_subset": "2013.main.EN",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c4af60>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.prost",
    "docstring": {
      "name": "Prost",
      "dataset": [
        "lighteval/prost"
      ],
      "abstract": "PROST is a benchmark for testing physical reasoning about objects through space\nand time. It includes 18,736 multiple-choice questions covering 10 core physics\nconcepts, designed to probe models in zero-shot settings. Results show that even\nlarge pretrained models struggle with physical reasoning and are sensitive to\nquestion phrasing, underscoring their limited real-world understanding.",
      "languages": [
        "english"
      ],
      "tags": [
        "reasoning",
        "qa",
        "physical-commonsense"
      ],
      "paper": "https://arxiv.org/abs/2106.03634"
    },
    "tasks": [
      {
        "name": "prost",
        "config": {
          "name": "prost",
          "prompt_function": "<function prost_prompt at 0x7717f358a3e0>",
          "hf_repo": "lighteval/prost",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c4b4d0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.aimo",
    "docstring": {
      "name": "AIMO Progress Prize 1",
      "dataset": [
        "lighteval/aimo_progress_prize_1"
      ],
      "languages": [
        "english"
      ],
      "tags": [
        "math",
        "reasoning"
      ]
    },
    "tasks": [
      {
        "name": "aimo_progress_prize_1",
        "config": {
          "name": "aimo_progress_prize_1",
          "prompt_function": "<function aimo_prompt at 0x7717f358a480>",
          "hf_repo": "lighteval/aimo_progress_prize_1",
          "hf_subset": "",
          "metrics": "({'metric_name': 'em:normalize_gold=<function math_normalizer at 0x7717f3b9cea0>&normalize_pred=<function math_normalizer at 0x7717f3b9cea0>', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c4b9e0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function generate.<locals>.solve at 0x7717f358a5c0>]",
          "scorer": "<function math_scorer.<locals>.score at 0x7717f358a8e0>",
          "sample_fields": "<function record_to_sample at 0x7717f358a520>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "train",
          "few_shots_select": "sequential",
          "generation_size": "2048",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.mathvista",
    "docstring": {
      "name": "MathVista",
      "dataset": [
        "AI4Math/MathVista"
      ],
      "abstract": "Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit impressive problem-solving skills in many tasks and domains, but their ability in mathematical reasoning in visual contexts has not been systematically studied. To bridge this gap, we present MathVista, a benchmark designed to combine challenges from diverse mathematical and visual tasks. It consists of 6,141 examples, derived from 28 existing multimodal datasets involving mathematics and 3 newly created datasets (i.e., IQTest, FunctionQA, and PaperQA). Completing these tasks requires fine-grained, deep visual understanding and compositional reasoning, which all state-of-the-art foundation models find challenging.",
      "languages": [
        "english"
      ],
      "tags": [
        "math",
        "qa",
        "reasoning"
      ],
      "paper": "https://arxiv.org/pdf/2310.02255",
      "starred": "true"
    },
    "tasks": [
      {
        "name": "mathvista",
        "config": {
          "name": "mathvista",
          "prompt_function": "<function <lambda> at 0x7717f358b240>",
          "hf_repo": "AI4Math/MathVista",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'extractive_match', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.dynamic_metrics.MultilingualExtractiveMatchMetric object at 0x7717f0c4be60>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function mathvista_solver.<locals>.solve at 0x7717f358b420>]",
          "scorer": "<function mathvista_scorer.<locals>.score at 0x7717f358b560>",
          "sample_fields": "<function record_to_sample at 0x7717f358ae80>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "<function <lambda> at 0x7717f358b380>",
          "hf_avail_splits": "('testmini, test',)",
          "evaluation_splits": "('testmini',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "512",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.med",
    "docstring": {
      "name": "Med",
      "dataset": [
        "lighteval/med_mcqa",
        "lighteval/med_paragraph_simplification",
        "bigbio/med_qa"
      ],
      "abstract": "A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering",
      "languages": [
        "english"
      ],
      "tags": [
        "health",
        "medical"
      ],
      "paper": "https://medmcqa.github.io/"
    },
    "tasks": [
      {
        "name": "med_mcqa",
        "config": {
          "name": "med_mcqa",
          "prompt_function": "<function med_mcqa_prompt at 0x7717f358b6a0>",
          "hf_repo": "lighteval/med_mcqa",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c503e0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "med_paragraph_simplification",
        "config": {
          "name": "med_paragraph_simplification",
          "prompt_function": "<function med_paragraph_simplification_prompt at 0x7717f358b600>",
          "hf_repo": "lighteval/med_paragraph_simplification",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c507d0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "512",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "med_qa",
        "config": {
          "name": "med_qa",
          "prompt_function": "<function med_qa_prompt at 0x7717f358b740>",
          "hf_repo": "bigbio/med_qa",
          "hf_subset": "med_qa_en_source",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c50b60>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.dyck_language",
    "docstring": {
      "name": "Dyck Language",
      "dataset": [
        "lighteval/DyckLanguage"
      ],
      "abstract": "Scenario testing hierarchical reasoning through the Dyck formal languages.",
      "languages": [
        "english"
      ],
      "tags": [
        "reasoning"
      ],
      "paper": "https://aclanthology.org/W19-3905/"
    },
    "tasks": [
      {
        "name": "dyck_language:2",
        "config": {
          "name": "dyck_language:2",
          "prompt_function": "<function dyck_language_prompt at 0x7717f358b9c0>",
          "hf_repo": "lighteval/DyckLanguage",
          "hf_subset": "2",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c51010>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function generate.<locals>.solve at 0x7717f358ba60>]",
          "scorer": "<function exact.<locals>.score at 0x7717f358bba0>",
          "sample_fields": "<function record_to_sample at 0x7717f358b920>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "dyck_language:3",
        "config": {
          "name": "dyck_language:3",
          "prompt_function": "<function dyck_language_prompt at 0x7717f358b9c0>",
          "hf_repo": "lighteval/DyckLanguage",
          "hf_subset": "3",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c51370>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function generate.<locals>.solve at 0x7717f358bce0>]",
          "scorer": "<function exact.<locals>.score at 0x7717f358bd80>",
          "sample_fields": "<function record_to_sample at 0x7717f358b920>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "dyck_language:4",
        "config": {
          "name": "dyck_language:4",
          "prompt_function": "<function dyck_language_prompt at 0x7717f358b9c0>",
          "hf_repo": "lighteval/DyckLanguage",
          "hf_subset": "4",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c516d0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function generate.<locals>.solve at 0x7717f358bec0>]",
          "scorer": "<function exact.<locals>.score at 0x7717f358bf60>",
          "sample_fields": "<function record_to_sample at 0x7717f358b920>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.asdiv",
    "docstring": {
      "name": "Asdiv",
      "dataset": [
        "EleutherAI/asdiv"
      ],
      "abstract": "ASDiv is a dataset for arithmetic reasoning that contains 2,000+ questions\ncovering addition, subtraction, multiplication, and division.",
      "languages": [
        "english"
      ],
      "tags": [
        "math",
        "reasoning"
      ],
      "paper": "https://arxiv.org/abs/2410.12853"
    },
    "tasks": [
      {
        "name": "asdiv",
        "config": {
          "name": "asdiv",
          "prompt_function": "<function asdiv_prompt at 0x7717f35d8040>",
          "hf_repo": "EleutherAI/asdiv",
          "hf_subset": "asdiv",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c51b80>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function generate.<locals>.solve at 0x7717f35d8180>]",
          "scorer": "<function math_scorer.<locals>.score at 0x7717f35d8360>",
          "sample_fields": "<function record_to_sample at 0x7717f35d80e0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('validation',)",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.math_500",
    "docstring": {
      "name": "Math 500",
      "dataset": [
        "HuggingFaceH4/MATH-500"
      ],
      "abstract": "This dataset contains a subset of 500 problems from the MATH benchmark that\nOpenAI created in their Let's Verify Step by Step paper.",
      "languages": [
        "english"
      ],
      "tags": [
        "math",
        "reasoning"
      ],
      "paper": "https://arxiv.org/abs/2305.20050",
      "starred": "true"
    },
    "tasks": [
      {
        "name": "math_500",
        "config": {
          "name": "math_500",
          "prompt_function": "<function math_500_prompt at 0x7717f35d84a0>",
          "hf_repo": "HuggingFaceH4/MATH-500",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'pass@k:k=1&n=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f1904ce0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function prompt_template.<locals>.solve at 0x7717f35d8540>, <function generate.<locals>.solve at 0x7717f35d8680>]",
          "scorer": "<function _model_graded_qa_single.<locals>.score at 0x7717f35d87c0>",
          "sample_fields": "<function record_to_sample at 0x7717f35d8400>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "32768",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "2"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.mmmu_pro",
    "docstring": {
      "name": "Mmmu Pro",
      "dataset": [
        "MMMU/MMMU_pro"
      ],
      "languages": [
        "english"
      ],
      "tags": [
        "general-knowledge",
        "knowledge",
        "multimodal",
        "multiple-choice"
      ],
      "paper": "https://arxiv.org/abs/2409.02813"
    },
    "tasks": [
      {
        "name": "mmmu_pro:standard-4",
        "config": {
          "name": "mmmu_pro:standard-4",
          "prompt_function": "<function mmmu_pro_prompt at 0x7717f35d8900>",
          "hf_repo": "MMMU/MMMU_pro",
          "hf_subset": "standard (4 options)",
          "metrics": "({'metric_name': 'extractive_match', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.dynamic_metrics.MultilingualExtractiveMatchMetric object at 0x7717f0c52600>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "30",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmmu_pro:standard-10",
        "config": {
          "name": "mmmu_pro:standard-10",
          "prompt_function": "<function mmmu_pro_prompt at 0x7717f35d8900>",
          "hf_repo": "MMMU/MMMU_pro",
          "hf_subset": "standard (10 options)",
          "metrics": "({'metric_name': 'extractive_match', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.dynamic_metrics.MultilingualExtractiveMatchMetric object at 0x7717f0c52a80>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "30",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmmu_pro:vision",
        "config": {
          "name": "mmmu_pro:vision",
          "prompt_function": "<function mmmu_pro_vision_prompt at 0x7717f35d8860>",
          "hf_repo": "MMMU/MMMU_pro",
          "hf_subset": "vision",
          "metrics": "({'metric_name': 'extractive_match', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.dynamic_metrics.MultilingualExtractiveMatchMetric object at 0x7717f0c52ed0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "30",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.math",
    "docstring": {
      "name": "Math",
      "dataset": [
        "DigitalLearningGmbH/MATH-lighteval"
      ],
      "languages": [
        "english"
      ],
      "tags": [
        "math",
        "reasoning"
      ],
      "paper": "https://arxiv.org/abs/2305.20050"
    },
    "tasks": [
      {
        "name": "math:algebra",
        "config": {
          "name": "math:algebra",
          "prompt_function": "<function math_prompt at 0x7717f35d89a0>",
          "hf_repo": "DigitalLearningGmbH/MATH-lighteval",
          "hf_subset": "algebra",
          "metrics": "({'metric_name': 'maj@n:n=4&strip_strings=True&normalize_pred=<function math_normalizer at 0x7717f3b9cea0>&normalize_gold=<function math_normalizer at 0x7717f3b9cea0>', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.MajAtN object at 0x7717f0c53440>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "2048",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "1"
        }
      },
      {
        "name": "math:counting_and_probability",
        "config": {
          "name": "math:counting_and_probability",
          "prompt_function": "<function math_prompt at 0x7717f35d89a0>",
          "hf_repo": "DigitalLearningGmbH/MATH-lighteval",
          "hf_subset": "counting_and_probability",
          "metrics": "({'metric_name': 'maj@n:n=4&strip_strings=True&normalize_pred=<function math_normalizer at 0x7717f3b9cea0>&normalize_gold=<function math_normalizer at 0x7717f3b9cea0>', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.MajAtN object at 0x7717f0c53860>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "2048",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "1"
        }
      },
      {
        "name": "math:geometry",
        "config": {
          "name": "math:geometry",
          "prompt_function": "<function math_prompt at 0x7717f35d89a0>",
          "hf_repo": "DigitalLearningGmbH/MATH-lighteval",
          "hf_subset": "geometry",
          "metrics": "({'metric_name': 'maj@n:n=4&strip_strings=True&normalize_pred=<function math_normalizer at 0x7717f3b9cea0>&normalize_gold=<function math_normalizer at 0x7717f3b9cea0>', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.MajAtN object at 0x7717f0c53c80>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "2048",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "1"
        }
      },
      {
        "name": "math:intermediate_algebra",
        "config": {
          "name": "math:intermediate_algebra",
          "prompt_function": "<function math_prompt at 0x7717f35d89a0>",
          "hf_repo": "DigitalLearningGmbH/MATH-lighteval",
          "hf_subset": "intermediate_algebra",
          "metrics": "({'metric_name': 'maj@n:n=4&strip_strings=True&normalize_pred=<function math_normalizer at 0x7717f3b9cea0>&normalize_gold=<function math_normalizer at 0x7717f3b9cea0>', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.MajAtN object at 0x7717f0c600e0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "2048",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "1"
        }
      },
      {
        "name": "math:number_theory",
        "config": {
          "name": "math:number_theory",
          "prompt_function": "<function math_prompt at 0x7717f35d89a0>",
          "hf_repo": "DigitalLearningGmbH/MATH-lighteval",
          "hf_subset": "number_theory",
          "metrics": "({'metric_name': 'maj@n:n=4&strip_strings=True&normalize_pred=<function math_normalizer at 0x7717f3b9cea0>&normalize_gold=<function math_normalizer at 0x7717f3b9cea0>', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.MajAtN object at 0x7717f0c60500>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "2048",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "1"
        }
      },
      {
        "name": "math:prealgebra",
        "config": {
          "name": "math:prealgebra",
          "prompt_function": "<function math_prompt at 0x7717f35d89a0>",
          "hf_repo": "DigitalLearningGmbH/MATH-lighteval",
          "hf_subset": "prealgebra",
          "metrics": "({'metric_name': 'maj@n:n=4&strip_strings=True&normalize_pred=<function math_normalizer at 0x7717f3b9cea0>&normalize_gold=<function math_normalizer at 0x7717f3b9cea0>', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.MajAtN object at 0x7717f0c60920>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "2048",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "1"
        }
      },
      {
        "name": "math:precalculus",
        "config": {
          "name": "math:precalculus",
          "prompt_function": "<function math_prompt at 0x7717f35d89a0>",
          "hf_repo": "DigitalLearningGmbH/MATH-lighteval",
          "hf_subset": "precalculus",
          "metrics": "({'metric_name': 'maj@n:n=4&strip_strings=True&normalize_pred=<function math_normalizer at 0x7717f3b9cea0>&normalize_gold=<function math_normalizer at 0x7717f3b9cea0>', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.MajAtN object at 0x7717f0c60d40>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "2048",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "1"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.lexglue",
    "docstring": {
      "name": "Lexglue",
      "dataset": [
        "lighteval/lexglue"
      ],
      "abstract": "LexGLUE: A Benchmark Dataset for Legal Language Understanding in English",
      "languages": [
        "english"
      ],
      "tags": [
        "classification",
        "legal"
      ],
      "paper": "https://arxiv.org/abs/2110.00976"
    },
    "tasks": [
      {
        "name": "lexglue:case_hold",
        "config": {
          "name": "lexglue:case_hold",
          "prompt_function": "<function lex_glue_case_hold_prompt at 0x7717f35d8f40>",
          "hf_repo": "lighteval/lexglue",
          "hf_subset": "case_hold",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c61280>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lexglue:ecthr_a",
        "config": {
          "name": "lexglue:ecthr_a",
          "prompt_function": "<function lex_glue_ecthr_a_prompt at 0x7717f35d8b80>",
          "hf_repo": "lighteval/lexglue",
          "hf_subset": "ecthr_a",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c61640>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "20",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lexglue:ecthr_b",
        "config": {
          "name": "lexglue:ecthr_b",
          "prompt_function": "<function lex_glue_ecthr_b_prompt at 0x7717f35d8c20>",
          "hf_repo": "lighteval/lexglue",
          "hf_subset": "ecthr_b",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c61a00>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "20",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lexglue:eurlex",
        "config": {
          "name": "lexglue:eurlex",
          "prompt_function": "<function lex_glue_eurlex_prompt at 0x7717f35d8d60>",
          "hf_repo": "lighteval/lexglue",
          "hf_subset": "eurlex",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c61dc0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "20",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lexglue:ledgar",
        "config": {
          "name": "lexglue:ledgar",
          "prompt_function": "<function lex_glue_ledgar_prompt at 0x7717f35d8e00>",
          "hf_repo": "lighteval/lexglue",
          "hf_subset": "ledgar",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c62180>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "20",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lexglue:scotus",
        "config": {
          "name": "lexglue:scotus",
          "prompt_function": "<function lex_glue_scotus_prompt at 0x7717f35d8cc0>",
          "hf_repo": "lighteval/lexglue",
          "hf_subset": "scotus",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c62540>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lexglue:unfair_tos",
        "config": {
          "name": "lexglue:unfair_tos",
          "prompt_function": "<function lex_glue_unfair_tos_prompt at 0x7717f35d8ea0>",
          "hf_repo": "lighteval/lexglue",
          "hf_subset": "unfair_tos",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c62900>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "20",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.babi_qa",
    "docstring": {
      "name": "Babi Qa",
      "dataset": [
        "facebook/babi_qa"
      ],
      "abstract": "The bAbI benchmark for measuring understanding and reasoning, evaluates reading\ncomprehension via question answering.",
      "languages": [
        "english"
      ],
      "tags": [
        "qa",
        "reasoning"
      ],
      "paper": "https://arxiv.org/abs/1502.05698"
    },
    "tasks": [
      {
        "name": "babi_qa",
        "config": {
          "name": "babi_qa",
          "prompt_function": "<function babi_qa_prompt at 0x7717f35d8fe0>",
          "hf_repo": "facebook/babi_qa",
          "hf_subset": "en-valid-qa1",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c62de0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.arc",
    "docstring": {
      "name": "Arc",
      "dataset": [
        "allenai/ai2_arc"
      ],
      "abstract": "7,787 genuine grade-school level, multiple-choice science questions, assembled\nto encourage research in advanced question-answering. The dataset is partitioned\ninto a Challenge Set and an Easy Set, where the former contains only questions\nanswered incorrectly by both a retrieval-based algorithm and a word\nco-occurrence algorithm",
      "languages": [
        "english"
      ],
      "tags": [
        "multiple-choice"
      ],
      "paper": "https://arxiv.org/abs/1803.05457"
    },
    "tasks": [
      {
        "name": "arc:challenge",
        "config": {
          "name": "arc:challenge",
          "prompt_function": "<function arc_prompt at 0x7717f35d9080>",
          "hf_repo": "allenai/ai2_arc",
          "hf_subset": "ARC-Challenge",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c632c0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f35d91c0>]",
          "scorer": "<function choice.<locals>.score at 0x7717f35d9300>",
          "sample_fields": "<function record_to_sample at 0x7717f35d9120>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "random_sampling_from_train",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "arc:easy",
        "config": {
          "name": "arc:easy",
          "prompt_function": "<function arc_prompt at 0x7717f35d9080>",
          "hf_repo": "allenai/ai2_arc",
          "hf_subset": "ARC-Easy",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c635f0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f35d9440>]",
          "scorer": "<function choice.<locals>.score at 0x7717f35d94e0>",
          "sample_fields": "<function record_to_sample at 0x7717f35d9120>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'validation', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "random_sampling_from_train",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.anli",
    "docstring": {
      "name": "Anli",
      "dataset": [
        "facebook/anli"
      ],
      "abstract": "The Adversarial Natural Language Inference (ANLI) is a new large-scale NLI\nbenchmark dataset, The dataset is collected via an iterative, adversarial\nhuman-and-model-in-the-loop procedure. ANLI is much more difficult than its\npredecessors including SNLI and MNLI. It contains three rounds. Each round has\ntrain/dev/test splits.",
      "languages": [
        "english"
      ],
      "tags": [
        "nli",
        "reasoning"
      ],
      "paper": "https://arxiv.org/abs/1910.14599"
    },
    "tasks": [
      {
        "name": "anli:r1",
        "config": {
          "name": "anli:r1",
          "prompt_function": "<function anli_prompt at 0x7717f35d9620>",
          "hf_repo": "facebook/anli",
          "hf_subset": "plain_text",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c63920>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f35d96c0>]",
          "scorer": "<function choice.<locals>.score at 0x7717f35d9800>",
          "sample_fields": "<function record_to_sample at 0x7717f35d9580>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train_r1', 'dev_r1', 'test_r1')",
          "evaluation_splits": "('test_r1',)",
          "few_shots_split": "train_r1",
          "few_shots_select": "random_sampling_from_train",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "anli:r2",
        "config": {
          "name": "anli:r2",
          "prompt_function": "<function anli_prompt at 0x7717f35d9620>",
          "hf_repo": "facebook/anli",
          "hf_subset": "plain_text",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c63c20>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f35d9940>]",
          "scorer": "<function choice.<locals>.score at 0x7717f35d99e0>",
          "sample_fields": "<function record_to_sample at 0x7717f35d9580>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train_r2', 'dev_r2', 'test_r2')",
          "evaluation_splits": "('test_r2',)",
          "few_shots_split": "train_r2",
          "few_shots_select": "random_sampling_from_train",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "anli:r3",
        "config": {
          "name": "anli:r3",
          "prompt_function": "<function anli_prompt at 0x7717f35d9620>",
          "hf_repo": "facebook/anli",
          "hf_subset": "plain_text",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c63f20>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f35d9b20>]",
          "scorer": "<function choice.<locals>.score at 0x7717f35d9bc0>",
          "sample_fields": "<function record_to_sample at 0x7717f35d9580>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train_r3', 'dev_r3', 'test_r3')",
          "evaluation_splits": "('test_r3',)",
          "few_shots_split": "train_r3",
          "few_shots_select": "random_sampling_from_train",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.entity_data_imputation",
    "docstring": {
      "name": "Entity Data Imputation",
      "dataset": [
        "lighteval/Buy",
        "lighteval/Restaurant"
      ],
      "abstract": "Scenario that tests the ability to impute missing entities in a data table.",
      "languages": [
        "english"
      ],
      "tags": [
        "reasoning"
      ],
      "paper": "https://ieeexplore.ieee.org/document/9458712"
    },
    "tasks": [
      {
        "name": "entity_data_imputation:Buy",
        "config": {
          "name": "entity_data_imputation:Buy",
          "prompt_function": "<function entity_data_imputation_prompt at 0x7717f35d9c60>",
          "hf_repo": "lighteval/Buy",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c64350>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'valid')",
          "evaluation_splits": "('valid', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "entity_data_imputation:Restaurant",
        "config": {
          "name": "entity_data_imputation:Restaurant",
          "prompt_function": "<function entity_data_imputation_prompt at 0x7717f35d9c60>",
          "hf_repo": "lighteval/Restaurant",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c64710>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.wikifact",
    "docstring": {
      "name": "Wikifact",
      "dataset": [
        "lighteval/wikifact"
      ],
      "abstract": "Extensively test factual knowledge.",
      "languages": [
        "english"
      ],
      "tags": [
        "factuality",
        "knowledge"
      ],
      "paper": "https://aclanthology.org/D19-1250/"
    },
    "tasks": [
      {
        "name": "wikifact:applies_to_jurisdiction",
        "config": {
          "name": "wikifact:applies_to_jurisdiction",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "applies_to_jurisdiction",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c64c20>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:atomic_number",
        "config": {
          "name": "wikifact:atomic_number",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "atomic_number",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c65010>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:author",
        "config": {
          "name": "wikifact:author",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "author",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c65400>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:employer",
        "config": {
          "name": "wikifact:employer",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "employer",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c657f0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:field_of_work",
        "config": {
          "name": "wikifact:field_of_work",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "field_of_work",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c65be0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:file_extension",
        "config": {
          "name": "wikifact:file_extension",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "file_extension",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c65fd0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:genetic_association",
        "config": {
          "name": "wikifact:genetic_association",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "genetic_association",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c663c0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:instrument",
        "config": {
          "name": "wikifact:instrument",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "instrument",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c667b0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:language_of_work_or_name",
        "config": {
          "name": "wikifact:language_of_work_or_name",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "language_of_work_or_name",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c66ba0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:languages_spoken_written_or_signed",
        "config": {
          "name": "wikifact:languages_spoken_written_or_signed",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "languages_spoken_written_or_signed",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c66f90>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:laws_applied",
        "config": {
          "name": "wikifact:laws_applied",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "laws_applied",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c67380>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:located_in_the_administrative_territorial_entity",
        "config": {
          "name": "wikifact:located_in_the_administrative_territorial_entity",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "located_in_the_administrative_territorial_entity",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c67770>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:location",
        "config": {
          "name": "wikifact:location",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "location",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c67b60>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:location_of_discovery",
        "config": {
          "name": "wikifact:location_of_discovery",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "location_of_discovery",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c67f50>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:location_of_formation",
        "config": {
          "name": "wikifact:location_of_formation",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "location_of_formation",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c70380>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:member_of",
        "config": {
          "name": "wikifact:member_of",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "member_of",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c70740>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:member_of_political_party",
        "config": {
          "name": "wikifact:member_of_political_party",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "member_of_political_party",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c70b30>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:member_of_sports_team",
        "config": {
          "name": "wikifact:member_of_sports_team",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "member_of_sports_team",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c70f20>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:movement",
        "config": {
          "name": "wikifact:movement",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "movement",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c71310>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:headquarters_location",
        "config": {
          "name": "wikifact:headquarters_location",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "headquarters_location",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c71700>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:industry",
        "config": {
          "name": "wikifact:industry",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "industry",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c71af0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:named_after",
        "config": {
          "name": "wikifact:named_after",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "named_after",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c71ee0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:native_language",
        "config": {
          "name": "wikifact:native_language",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "native_language",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c722d0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:number_of_processor_cores",
        "config": {
          "name": "wikifact:number_of_processor_cores",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "number_of_processor_cores",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c726c0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:occupation",
        "config": {
          "name": "wikifact:occupation",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "occupation",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c72ab0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:original_language_of_film_or_TV_show",
        "config": {
          "name": "wikifact:original_language_of_film_or_TV_show",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "original_language_of_film_or_TV_show",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c72ea0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:original_network",
        "config": {
          "name": "wikifact:original_network",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "original_network",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c73290>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:overrules",
        "config": {
          "name": "wikifact:overrules",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "overrules",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c73680>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:owned_by",
        "config": {
          "name": "wikifact:owned_by",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "owned_by",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c73a70>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:part_of",
        "config": {
          "name": "wikifact:part_of",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "part_of",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c73e30>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:participating_team",
        "config": {
          "name": "wikifact:participating_team",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "participating_team",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c78260>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:place_of_birth",
        "config": {
          "name": "wikifact:place_of_birth",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "place_of_birth",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c78650>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:place_of_death",
        "config": {
          "name": "wikifact:place_of_death",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "place_of_death",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c78a40>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:position_played_on_team",
        "config": {
          "name": "wikifact:position_played_on_team",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "position_played_on_team",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c78e30>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:programming_language",
        "config": {
          "name": "wikifact:programming_language",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "programming_language",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c79220>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:recommended_unit_of_measurement",
        "config": {
          "name": "wikifact:recommended_unit_of_measurement",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "recommended_unit_of_measurement",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c795e0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:record_label",
        "config": {
          "name": "wikifact:record_label",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "record_label",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c799d0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:religion",
        "config": {
          "name": "wikifact:religion",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "religion",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c79dc0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:repealed_by",
        "config": {
          "name": "wikifact:repealed_by",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "repealed_by",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c7a1b0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:shares_border_with",
        "config": {
          "name": "wikifact:shares_border_with",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "shares_border_with",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c7a5a0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:solved_by",
        "config": {
          "name": "wikifact:solved_by",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "solved_by",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c7a990>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:statement_describes",
        "config": {
          "name": "wikifact:statement_describes",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "statement_describes",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c7ad80>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:stock_exchange",
        "config": {
          "name": "wikifact:stock_exchange",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "stock_exchange",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c7b170>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:subclass_of",
        "config": {
          "name": "wikifact:subclass_of",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "subclass_of",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c7b560>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:subsidiary",
        "config": {
          "name": "wikifact:subsidiary",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "subsidiary",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c7b950>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:symptoms_and_signs",
        "config": {
          "name": "wikifact:symptoms_and_signs",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "symptoms_and_signs",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c7bd40>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:therapeutic_area",
        "config": {
          "name": "wikifact:therapeutic_area",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "therapeutic_area",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c80170>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:time_of_discovery_or_invention",
        "config": {
          "name": "wikifact:time_of_discovery_or_invention",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "time_of_discovery_or_invention",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c80560>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:twinned_administrative_body",
        "config": {
          "name": "wikifact:twinned_administrative_body",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "twinned_administrative_body",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c80950>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wikifact:work_location",
        "config": {
          "name": "wikifact:work_location",
          "prompt_function": "<function wikifact_prompt at 0x7717f35d9da0>",
          "hf_repo": "lighteval/wikifact",
          "hf_subset": "work_location",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c80d40>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.lextreme",
    "docstring": {
      "name": "Lextreme",
      "dataset": [
        "lighteval/lextreme"
      ],
      "abstract": "LEXTREME: A Multi-Lingual and Multi-Task Benchmark for the Legal Domain",
      "languages": [
        "bulgarian",
        "czech",
        "danish",
        "german",
        "greek",
        "english",
        "spanish",
        "estonian",
        "finnish",
        "french",
        "ga",
        "croatian",
        "hungarian",
        "italian",
        "lithuanian",
        "latvian",
        "mt",
        "dutch",
        "polish",
        "portuguese",
        "romanian",
        "slovak",
        "slovenian",
        "swedish"
      ],
      "tags": [
        "classification",
        "legal"
      ],
      "paper": "https://arxiv.org/abs/2301.13126"
    },
    "tasks": [
      {
        "name": "lextreme:brazilian_court_decisions_judgment",
        "config": {
          "name": "lextreme:brazilian_court_decisions_judgment",
          "prompt_function": "<function lextreme_brazilian_court_decisions_judgment_prompt at 0x7717f35da0c0>",
          "hf_repo": "lighteval/lextreme",
          "hf_subset": "brazilian_court_decisions_judgment",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c811f0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lextreme:brazilian_court_decisions_unanimity",
        "config": {
          "name": "lextreme:brazilian_court_decisions_unanimity",
          "prompt_function": "<function lextreme_brazilian_court_decisions_unanimity_prompt at 0x7717f35da160>",
          "hf_repo": "lighteval/lextreme",
          "hf_subset": "brazilian_court_decisions_unanimity",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c818e0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lextreme:covid19_emergency_event",
        "config": {
          "name": "lextreme:covid19_emergency_event",
          "prompt_function": "<function lextreme_covid19_emergency_event_prompt at 0x7717f35da660>",
          "hf_repo": "lighteval/lextreme",
          "hf_subset": "covid19_emergency_event",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c81ca0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "10",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lextreme:german_argument_mining",
        "config": {
          "name": "lextreme:german_argument_mining",
          "prompt_function": "<function lextreme_german_argument_mining_prompt at 0x7717f35da200>",
          "hf_repo": "lighteval/lextreme",
          "hf_subset": "german_argument_mining",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c82060>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lextreme:greek_legal_code_chapter",
        "config": {
          "name": "lextreme:greek_legal_code_chapter",
          "prompt_function": "<function lextreme_greek_legal_code_chapter_prompt at 0x7717f35da2a0>",
          "hf_repo": "lighteval/lextreme",
          "hf_subset": "greek_legal_code_chapter",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c82420>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "20",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lextreme:greek_legal_code_subject",
        "config": {
          "name": "lextreme:greek_legal_code_subject",
          "prompt_function": "<function lextreme_greek_legal_code_subject_prompt at 0x7717f35da340>",
          "hf_repo": "lighteval/lextreme",
          "hf_subset": "greek_legal_code_subject",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c827e0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "20",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lextreme:greek_legal_code_volume",
        "config": {
          "name": "lextreme:greek_legal_code_volume",
          "prompt_function": "<function lextreme_greek_legal_code_volume_prompt at 0x7717f35da3e0>",
          "hf_repo": "lighteval/lextreme",
          "hf_subset": "greek_legal_code_volume",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c82ba0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "20",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lextreme:greek_legal_ner",
        "config": {
          "name": "lextreme:greek_legal_ner",
          "prompt_function": "<function lextreme_greek_legal_ner_prompt at 0x7717f35da8e0>",
          "hf_repo": "lighteval/lextreme",
          "hf_subset": "greek_legal_ner",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c82f60>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "430",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lextreme:legalnero",
        "config": {
          "name": "lextreme:legalnero",
          "prompt_function": "<function lextreme_legalnero_prompt at 0x7717f35da980>",
          "hf_repo": "lighteval/lextreme",
          "hf_subset": "legalnero",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c83320>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "788",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lextreme:lener_br",
        "config": {
          "name": "lextreme:lener_br",
          "prompt_function": "<function lextreme_lener_br_prompt at 0x7717f35daa20>",
          "hf_repo": "lighteval/lextreme",
          "hf_subset": "lener_br",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c836e0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "338",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lextreme:mapa_coarse",
        "config": {
          "name": "lextreme:mapa_coarse",
          "prompt_function": "<function lextreme_mapa_coarse_prompt at 0x7717f35daac0>",
          "hf_repo": "lighteval/lextreme",
          "hf_subset": "mapa_coarse",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c83aa0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "274",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lextreme:mapa_fine",
        "config": {
          "name": "lextreme:mapa_fine",
          "prompt_function": "<function lextreme_mapa_fine_prompt at 0x7717f35dab60>",
          "hf_repo": "lighteval/lextreme",
          "hf_subset": "mapa_fine",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c83e60>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "274",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lextreme:multi_eurlex_level_1",
        "config": {
          "name": "lextreme:multi_eurlex_level_1",
          "prompt_function": "<function lextreme_multi_eurlex_level_1_prompt at 0x7717f35da700>",
          "hf_repo": "lighteval/lextreme",
          "hf_subset": "multi_eurlex_level_1",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c8c260>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "10",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lextreme:multi_eurlex_level_2",
        "config": {
          "name": "lextreme:multi_eurlex_level_2",
          "prompt_function": "<function lextreme_multi_eurlex_level_2_prompt at 0x7717f35da7a0>",
          "hf_repo": "lighteval/lextreme",
          "hf_subset": "multi_eurlex_level_2",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c8c620>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "10",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lextreme:multi_eurlex_level_3",
        "config": {
          "name": "lextreme:multi_eurlex_level_3",
          "prompt_function": "<function lextreme_multi_eurlex_level_3_prompt at 0x7717f35da840>",
          "hf_repo": "lighteval/lextreme",
          "hf_subset": "multi_eurlex_level_3",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c8c9e0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "10",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lextreme:online_terms_of_service_clause_topics",
        "config": {
          "name": "lextreme:online_terms_of_service_clause_topics",
          "prompt_function": "<function lextreme_online_terms_of_service_clause_topics_prompt at 0x7717f35da5c0>",
          "hf_repo": "lighteval/lextreme",
          "hf_subset": "online_terms_of_service_clause_topics",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c8cda0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "10",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lextreme:online_terms_of_service_unfairness_levels",
        "config": {
          "name": "lextreme:online_terms_of_service_unfairness_levels",
          "prompt_function": "<function lextreme_online_terms_of_service_unfairness_levels_prompt at 0x7717f35da520>",
          "hf_repo": "lighteval/lextreme",
          "hf_subset": "online_terms_of_service_unfairness_levels",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c8d160>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "10",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lextreme:swiss_judgment_prediction",
        "config": {
          "name": "lextreme:swiss_judgment_prediction",
          "prompt_function": "<function lextreme_swiss_judgment_prediction_prompt at 0x7717f35da480>",
          "hf_repo": "lighteval/lextreme",
          "hf_subset": "swiss_judgment_prediction",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c8d520>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.aa_omniscience",
    "docstring": {
      "name": "AA Omniscience",
      "dataset": [
        "ArtificialAnalysis/AA-Omniscience-Public"
      ],
      "abstract": "benchmark dataset designed to measure a model's ability to both recall factual\ninformation accurately across domains, and correctly abstain when its knowledge\nis insufficient. AA-Omniscience is characterized by its penalty for incorrect\nguesses, distinct from both accuracy (number of questions answered correctly)\nand hallucination rate (proportion of incorrect guesses when model does not know\nthe answer), making it extremely relevant for users to choose a model for their\nnext domain specific task.",
      "paper": "https://arxiv.org/abs/2511.13029",
      "languages": [
        "english"
      ],
      "starred": "true"
    },
    "tasks": [
      {
        "name": "aa_omniscience",
        "config": {
          "name": "aa_omniscience",
          "prompt_function": "<function aa_omniscience_prompt at 0x7717f35db560>",
          "hf_repo": "ArtificialAnalysis/AA-Omniscience-Public",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c8da00>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function generate.<locals>.solve at 0x7717f35db600>]",
          "scorer": "<function aa_omniscience_scorer.<locals>.score at 0x7717f35db740>",
          "sample_fields": "<function record_to_sample at 0x7717f35db420>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "100",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.summarization",
    "docstring": {
      "name": "Summarization",
      "dataset": [
        "lighteval/summarization"
      ],
      "abstract": "Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural\nNetworks for Extreme Summarization and: Abstractive Text Summarization using\nSequence-to-sequence RNNs and Beyond",
      "languages": [
        "english"
      ],
      "tags": [
        "summarization"
      ],
      "paper": "https://aclanthology.org/D18-1206/\nhttps://aclanthology.org/K16-1028/"
    },
    "tasks": [
      {
        "name": "summarization:cnn-dm",
        "config": {
          "name": "summarization:cnn-dm",
          "prompt_function": "<function cnn_dm_prompt at 0x7717f35db7e0>",
          "hf_repo": "lighteval/summarization",
          "hf_subset": "cnn-dm",
          "metrics": "({'metric_name': 'rouge1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ROUGE object at 0x7717f0c8de80>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'rouge2', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ROUGE object at 0x7717f0c8dee0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'rougeL', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ROUGE object at 0x7717f0c8df40>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'summac', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.Faithfulness object at 0x7717f0c8dfa0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': ['summarization_coverage', 'summarization_density', 'summarization_compression'], 'higher_is_better': {'summarization_coverage': True, 'summarization_density': True, 'summarization_compression': True}, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.Extractiveness object at 0x7717f0c8de20>, 'corpus_level_fn': {'summarization_coverage': <function mean at 0x771aa6d5b3b0>, 'summarization_density': <function mean at 0x771aa6d5b3b0>, 'summarization_compression': <function mean at 0x771aa6d5b3b0>}, 'batched_compute': False}, {'metric_name': ['BERTScore-P', 'BERTScore-R', 'BERTScore-F'], 'higher_is_better': {'BERTScore-P': True, 'BERTScore-R': True, 'BERTScore-F': True}, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.BertScore object at 0x7717f0c8e000>, 'corpus_level_fn': {'BERTScore-P': <function mean at 0x771aa6d5b3b0>, 'BERTScore-R': <function mean at 0x771aa6d5b3b0>, 'BERTScore-F': <function mean at 0x771aa6d5b3b0>}, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "128",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "summarization:xsum",
        "config": {
          "name": "summarization:xsum",
          "prompt_function": "<function xsum_prompt at 0x7717f35db920>",
          "hf_repo": "lighteval/summarization",
          "hf_subset": "xsum",
          "metrics": "({'metric_name': 'rouge1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ROUGE object at 0x7717f0c8e2a0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'rouge2', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ROUGE object at 0x7717f0c8e300>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'rougeL', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ROUGE object at 0x7717f0c8e360>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'summac', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.Faithfulness object at 0x7717f0c8e3c0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': ['summarization_coverage', 'summarization_density', 'summarization_compression'], 'higher_is_better': {'summarization_coverage': True, 'summarization_density': True, 'summarization_compression': True}, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.Extractiveness object at 0x7717f0c8e420>, 'corpus_level_fn': {'summarization_coverage': <function mean at 0x771aa6d5b3b0>, 'summarization_density': <function mean at 0x771aa6d5b3b0>, 'summarization_compression': <function mean at 0x771aa6d5b3b0>}, 'batched_compute': False}, {'metric_name': ['BERTScore-P', 'BERTScore-R', 'BERTScore-F'], 'higher_is_better': {'BERTScore-P': True, 'BERTScore-R': True, 'BERTScore-F': True}, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.BertScore object at 0x7717f0c8e480>, 'corpus_level_fn': {'BERTScore-P': <function mean at 0x771aa6d5b3b0>, 'BERTScore-R': <function mean at 0x771aa6d5b3b0>, 'BERTScore-F': <function mean at 0x771aa6d5b3b0>}, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "64",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "summarization:xsum-sampled",
        "config": {
          "name": "summarization:xsum-sampled",
          "prompt_function": "<function xsum_prompt at 0x7717f35db920>",
          "hf_repo": "lighteval/summarization",
          "hf_subset": "xsum-sampled",
          "metrics": "({'metric_name': 'rouge1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ROUGE object at 0x7717f0c8e720>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'rouge2', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ROUGE object at 0x7717f0c8e780>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'rougeL', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ROUGE object at 0x7717f0c8e7e0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'summac', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.Faithfulness object at 0x7717f0c8e840>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': ['summarization_coverage', 'summarization_density', 'summarization_compression'], 'higher_is_better': {'summarization_coverage': True, 'summarization_density': True, 'summarization_compression': True}, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.Extractiveness object at 0x7717f0c8e8a0>, 'corpus_level_fn': {'summarization_coverage': <function mean at 0x771aa6d5b3b0>, 'summarization_density': <function mean at 0x771aa6d5b3b0>, 'summarization_compression': <function mean at 0x771aa6d5b3b0>}, 'batched_compute': False}, {'metric_name': ['BERTScore-P', 'BERTScore-R', 'BERTScore-F'], 'higher_is_better': {'BERTScore-P': True, 'BERTScore-R': True, 'BERTScore-F': True}, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.BertScore object at 0x7717f0c8e900>, 'corpus_level_fn': {'BERTScore-P': <function mean at 0x771aa6d5b3b0>, 'BERTScore-R': <function mean at 0x771aa6d5b3b0>, 'BERTScore-F': <function mean at 0x771aa6d5b3b0>}, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "64",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.arithmetic",
    "docstring": {
      "name": "Arithmetic",
      "dataset": [
        "EleutherAI/arithmetic"
      ],
      "abstract": "A small battery of 10 tests that involve asking language models a simple\narithmetic problem in natural language.",
      "languages": [
        "english"
      ],
      "tags": [
        "math",
        "reasoning"
      ],
      "paper": "https://arxiv.org/abs/2005.14165"
    },
    "tasks": [
      {
        "name": "arithmetic:1dc",
        "config": {
          "name": "arithmetic:1dc",
          "prompt_function": "<function arithmetic_prompt at 0x7717f35dba60>",
          "hf_repo": "EleutherAI/arithmetic",
          "hf_subset": "arithmetic_1dc",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c8ecc0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function generate.<locals>.solve at 0x7717f35dbba0>]",
          "scorer": "<function math_scorer.<locals>.score at 0x7717f35dbd80>",
          "sample_fields": "<function record_to_sample at 0x7717f35dbb00>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('validation',)",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "256",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "arithmetic:2da",
        "config": {
          "name": "arithmetic:2da",
          "prompt_function": "<function arithmetic_prompt at 0x7717f35dba60>",
          "hf_repo": "EleutherAI/arithmetic",
          "hf_subset": "arithmetic_2da",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c8f050>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function generate.<locals>.solve at 0x7717f35dbec0>]",
          "scorer": "<function math_scorer.<locals>.score at 0x7717f35dbf60>",
          "sample_fields": "<function record_to_sample at 0x7717f35dbb00>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('validation',)",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "256",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "arithmetic:2dm",
        "config": {
          "name": "arithmetic:2dm",
          "prompt_function": "<function arithmetic_prompt at 0x7717f35dba60>",
          "hf_repo": "EleutherAI/arithmetic",
          "hf_subset": "arithmetic_2dm",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c8f3e0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function generate.<locals>.solve at 0x7717f34100e0>]",
          "scorer": "<function math_scorer.<locals>.score at 0x7717f3410180>",
          "sample_fields": "<function record_to_sample at 0x7717f35dbb00>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('validation',)",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "256",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "arithmetic:2ds",
        "config": {
          "name": "arithmetic:2ds",
          "prompt_function": "<function arithmetic_prompt at 0x7717f35dba60>",
          "hf_repo": "EleutherAI/arithmetic",
          "hf_subset": "arithmetic_2ds",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c8f770>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function generate.<locals>.solve at 0x7717f34102c0>]",
          "scorer": "<function math_scorer.<locals>.score at 0x7717f3410360>",
          "sample_fields": "<function record_to_sample at 0x7717f35dbb00>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('validation',)",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "256",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "arithmetic:3da",
        "config": {
          "name": "arithmetic:3da",
          "prompt_function": "<function arithmetic_prompt at 0x7717f35dba60>",
          "hf_repo": "EleutherAI/arithmetic",
          "hf_subset": "arithmetic_3da",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c8fb00>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function generate.<locals>.solve at 0x7717f34104a0>]",
          "scorer": "<function math_scorer.<locals>.score at 0x7717f3410540>",
          "sample_fields": "<function record_to_sample at 0x7717f35dbb00>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('validation',)",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "256",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "arithmetic:3ds",
        "config": {
          "name": "arithmetic:3ds",
          "prompt_function": "<function arithmetic_prompt at 0x7717f35dba60>",
          "hf_repo": "EleutherAI/arithmetic",
          "hf_subset": "arithmetic_3ds",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c8fe90>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function generate.<locals>.solve at 0x7717f3410680>]",
          "scorer": "<function math_scorer.<locals>.score at 0x7717f3410720>",
          "sample_fields": "<function record_to_sample at 0x7717f35dbb00>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('validation',)",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "256",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "arithmetic:4da",
        "config": {
          "name": "arithmetic:4da",
          "prompt_function": "<function arithmetic_prompt at 0x7717f35dba60>",
          "hf_repo": "EleutherAI/arithmetic",
          "hf_subset": "arithmetic_4da",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c90260>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function generate.<locals>.solve at 0x7717f3410860>]",
          "scorer": "<function math_scorer.<locals>.score at 0x7717f3410900>",
          "sample_fields": "<function record_to_sample at 0x7717f35dbb00>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('validation',)",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "256",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "arithmetic:4ds",
        "config": {
          "name": "arithmetic:4ds",
          "prompt_function": "<function arithmetic_prompt at 0x7717f35dba60>",
          "hf_repo": "EleutherAI/arithmetic",
          "hf_subset": "arithmetic_4ds",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c905f0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function generate.<locals>.solve at 0x7717f3410a40>]",
          "scorer": "<function math_scorer.<locals>.score at 0x7717f3410ae0>",
          "sample_fields": "<function record_to_sample at 0x7717f35dbb00>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('validation',)",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "256",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "arithmetic:5da",
        "config": {
          "name": "arithmetic:5da",
          "prompt_function": "<function arithmetic_prompt at 0x7717f35dba60>",
          "hf_repo": "EleutherAI/arithmetic",
          "hf_subset": "arithmetic_5da",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c90980>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function generate.<locals>.solve at 0x7717f3410c20>]",
          "scorer": "<function math_scorer.<locals>.score at 0x7717f3410cc0>",
          "sample_fields": "<function record_to_sample at 0x7717f35dbb00>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('validation',)",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "256",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "arithmetic:5ds",
        "config": {
          "name": "arithmetic:5ds",
          "prompt_function": "<function arithmetic_prompt at 0x7717f35dba60>",
          "hf_repo": "EleutherAI/arithmetic",
          "hf_subset": "arithmetic_5ds",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c90d10>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function generate.<locals>.solve at 0x7717f3410e00>]",
          "scorer": "<function math_scorer.<locals>.score at 0x7717f3410ea0>",
          "sample_fields": "<function record_to_sample at 0x7717f35dbb00>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('validation',)",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "256",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.toxigen",
    "docstring": {
      "name": "Toxigen",
      "dataset": [
        "skg/toxigen-data"
      ],
      "abstract": "This dataset is for implicit hate speech detection. All instances were generated\nusing GPT-3 and the methods described in our paper.",
      "languages": [
        "english"
      ],
      "tags": [
        "generation",
        "safety"
      ],
      "paper": "https://arxiv.org/abs/2203.09509"
    },
    "tasks": [
      {
        "name": "toxigen",
        "config": {
          "name": "toxigen",
          "prompt_function": "<function toxigen_prompt at 0x7717f3410fe0>",
          "hf_repo": "skg/toxigen-data",
          "hf_subset": "annotated",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c911f0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.gpqa",
    "docstring": {
      "name": "Gpqa",
      "dataset": [
        "Idavidrein/gpqa"
      ],
      "abstract": "GPQA is a dataset of 448 expert-written multiple-choice questions in biology,\nphysics, and chemistry, designed to test graduate-level reasoning. The questions\nare extremely difficult\u2014PhD-level experts score about 65%, skilled non-experts\n34% (even with web access), and GPT-4 around 39%. GPQA aims to support research\non scalable oversight, helping humans evaluate and trust AI systems that may\nexceed human expertise.",
      "languages": [
        "english"
      ],
      "tags": [
        "biology",
        "chemistry",
        "graduate-level",
        "multiple-choice",
        "physics",
        "qa",
        "reasoning",
        "science"
      ],
      "paper": "https://arxiv.org/abs/2311.12022",
      "starred": "true"
    },
    "tasks": [
      {
        "name": "gpqa:mc",
        "config": {
          "name": "gpqa:mc",
          "prompt_function": "<function gpqa_prompt at 0x7717f3411120>",
          "hf_repo": "Idavidrein/gpqa",
          "hf_subset": "gpqa_main",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0c91820>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f3411260>]",
          "scorer": "<function choice.<locals>.score at 0x7717f34113a0>",
          "sample_fields": "<function record_to_sample at 0x7717f3410f40>",
          "sample_to_fewshot": "<function sample_to_fewshot at 0x7717f3411080>",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "random_sampling",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "gpqa:diamond",
        "config": {
          "name": "gpqa:diamond",
          "prompt_function": "<function gpqa_instruct_prompt at 0x7717f34111c0>",
          "hf_repo": "Idavidrein/gpqa",
          "hf_subset": "gpqa_diamond",
          "metrics": "({'metric_name': 'gpqa_pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0c91b50>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f34114e0>]",
          "scorer": "<function choice.<locals>.score at 0x7717f3411580>",
          "sample_fields": "<function record_to_sample at 0x7717f3410f40>",
          "sample_to_fewshot": "<function sample_to_fewshot at 0x7717f3411080>",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "32768",
          "generation_grammar": "None",
          "stop_sequence": "[]",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "1"
        }
      },
      {
        "name": "gpqa:extended",
        "config": {
          "name": "gpqa:extended",
          "prompt_function": "<function gpqa_instruct_prompt at 0x7717f34111c0>",
          "hf_repo": "Idavidrein/gpqa",
          "hf_subset": "gpqa_extended",
          "metrics": "({'metric_name': 'extractive_match', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.dynamic_metrics.MultilingualExtractiveMatchMetric object at 0x7717f0c91f40>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f34116c0>]",
          "scorer": "<function choice.<locals>.score at 0x7717f3411760>",
          "sample_fields": "<function record_to_sample at 0x7717f3410f40>",
          "sample_to_fewshot": "<function sample_to_fewshot at 0x7717f3411080>",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "32768",
          "generation_grammar": "None",
          "stop_sequence": "[]",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "gpqa:main",
        "config": {
          "name": "gpqa:main",
          "prompt_function": "<function gpqa_instruct_prompt at 0x7717f34111c0>",
          "hf_repo": "Idavidrein/gpqa",
          "hf_subset": "gpqa_main",
          "metrics": "({'metric_name': 'extractive_match', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.dynamic_metrics.MultilingualExtractiveMatchMetric object at 0x7717f0c92300>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f34118a0>]",
          "scorer": "<function choice.<locals>.score at 0x7717f3411940>",
          "sample_fields": "<function record_to_sample at 0x7717f3410f40>",
          "sample_to_fewshot": "<function sample_to_fewshot at 0x7717f3411080>",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "32768",
          "generation_grammar": "None",
          "stop_sequence": "[]",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.slr_bench",
    "docstring": {
      "name": "SLR-Bench",
      "dataset": [
        "AIML-TUDA/SLR-Bench"
      ],
      "abstract": "SLR-Bench is a large-scale benchmark for scalable logical reasoning with\nlanguage models, comprising 19,000 prompts organized into 20 curriculum levels.",
      "languages": [
        "english"
      ],
      "tags": [
        "reasoning",
        "symbolic"
      ],
      "paper": "https://arxiv.org/abs/2506.15787"
    },
    "tasks": [
      {
        "name": "slr_bench_all",
        "config": {
          "name": "slr_bench_all",
          "prompt_function": "<function prompt_fn at 0x7717f3411bc0>",
          "hf_repo": "AIML-TUDA/SLR-Bench",
          "hf_subset": "v1-All",
          "metrics": "({'metric_name': 'verifiable_reward', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.tasks.tasks.slr_bench.VerifiableRewardMetric object at 0x7717f0c927b0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'validation', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "train",
          "few_shots_select": "random_sampling_from_train",
          "generation_size": "4096",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "1"
        }
      },
      {
        "name": "slr_bench_basic",
        "config": {
          "name": "slr_bench_basic",
          "prompt_function": "<function prompt_fn at 0x7717f3411bc0>",
          "hf_repo": "AIML-TUDA/SLR-Bench",
          "hf_subset": "v1-Basic",
          "metrics": "({'metric_name': 'verifiable_reward', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.tasks.tasks.slr_bench.VerifiableRewardMetric object at 0x7717f0c92ae0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'validation', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "train",
          "few_shots_select": "random_sampling_from_train",
          "generation_size": "4096",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "1"
        }
      },
      {
        "name": "slr_bench_easy",
        "config": {
          "name": "slr_bench_easy",
          "prompt_function": "<function prompt_fn at 0x7717f3411bc0>",
          "hf_repo": "AIML-TUDA/SLR-Bench",
          "hf_subset": "v1-Easy",
          "metrics": "({'metric_name': 'verifiable_reward', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.tasks.tasks.slr_bench.VerifiableRewardMetric object at 0x7717f0c92e70>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'validation', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "train",
          "few_shots_select": "random_sampling_from_train",
          "generation_size": "4096",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "1"
        }
      },
      {
        "name": "slr_bench_medium",
        "config": {
          "name": "slr_bench_medium",
          "prompt_function": "<function prompt_fn at 0x7717f3411bc0>",
          "hf_repo": "AIML-TUDA/SLR-Bench",
          "hf_subset": "v1-Medium",
          "metrics": "({'metric_name': 'verifiable_reward', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.tasks.tasks.slr_bench.VerifiableRewardMetric object at 0x7717f0c93200>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'validation', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "train",
          "few_shots_select": "random_sampling_from_train",
          "generation_size": "4096",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "1"
        }
      },
      {
        "name": "slr_bench_hard",
        "config": {
          "name": "slr_bench_hard",
          "prompt_function": "<function prompt_fn at 0x7717f3411bc0>",
          "hf_repo": "AIML-TUDA/SLR-Bench",
          "hf_subset": "v1-Hard",
          "metrics": "({'metric_name': 'verifiable_reward', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.tasks.tasks.slr_bench.VerifiableRewardMetric object at 0x7717f0c93590>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'validation', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "train",
          "few_shots_select": "random_sampling_from_train",
          "generation_size": "4096",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "1"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.mmlu",
    "docstring": {
      "name": "Mmlu",
      "dataset": [
        "lighteval/mmlu"
      ],
      "abstract": "MMMLU is a benchmark of general-knowledge and English language understanding.",
      "languages": [
        "english"
      ],
      "tags": [
        "general-knowledge",
        "knowledge",
        "multiple-choice"
      ],
      "paper": "https://arxiv.org/abs/2009.03300"
    },
    "tasks": [
      {
        "name": "mmlu:abstract_algebra",
        "config": {
          "name": "mmlu:abstract_algebra",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "abstract_algebra",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c93a40>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:anatomy",
        "config": {
          "name": "mmlu:anatomy",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "anatomy",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0c93e00>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:astronomy",
        "config": {
          "name": "mmlu:astronomy",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "astronomy",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0ca0200>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:business_ethics",
        "config": {
          "name": "mmlu:business_ethics",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "business_ethics",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0ca05c0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:clinical_knowledge",
        "config": {
          "name": "mmlu:clinical_knowledge",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "clinical_knowledge",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0ca0980>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:college_biology",
        "config": {
          "name": "mmlu:college_biology",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "college_biology",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0ca0d40>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:college_chemistry",
        "config": {
          "name": "mmlu:college_chemistry",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "college_chemistry",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0ca1100>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:college_computer_science",
        "config": {
          "name": "mmlu:college_computer_science",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "college_computer_science",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0ca14c0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:college_mathematics",
        "config": {
          "name": "mmlu:college_mathematics",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "college_mathematics",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0ca1850>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:college_medicine",
        "config": {
          "name": "mmlu:college_medicine",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "college_medicine",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0ca1c10>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:college_physics",
        "config": {
          "name": "mmlu:college_physics",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "college_physics",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0ca1fd0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:computer_security",
        "config": {
          "name": "mmlu:computer_security",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "computer_security",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0ca2390>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:conceptual_physics",
        "config": {
          "name": "mmlu:conceptual_physics",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "conceptual_physics",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0ca2750>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:econometrics",
        "config": {
          "name": "mmlu:econometrics",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "econometrics",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0ca2b10>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:electrical_engineering",
        "config": {
          "name": "mmlu:electrical_engineering",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "electrical_engineering",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0ca2ed0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:elementary_mathematics",
        "config": {
          "name": "mmlu:elementary_mathematics",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "elementary_mathematics",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0ca3290>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:formal_logic",
        "config": {
          "name": "mmlu:formal_logic",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "formal_logic",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0ca3650>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:global_facts",
        "config": {
          "name": "mmlu:global_facts",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "global_facts",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0ca3a10>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:high_school_biology",
        "config": {
          "name": "mmlu:high_school_biology",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "high_school_biology",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0ca3dd0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:high_school_chemistry",
        "config": {
          "name": "mmlu:high_school_chemistry",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "high_school_chemistry",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0ca81d0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:high_school_computer_science",
        "config": {
          "name": "mmlu:high_school_computer_science",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "high_school_computer_science",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0ca8590>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:high_school_european_history",
        "config": {
          "name": "mmlu:high_school_european_history",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "high_school_european_history",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0ca8950>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:high_school_geography",
        "config": {
          "name": "mmlu:high_school_geography",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "high_school_geography",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0ca8d10>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:high_school_government_and_politics",
        "config": {
          "name": "mmlu:high_school_government_and_politics",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "high_school_government_and_politics",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0ca90d0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:high_school_macroeconomics",
        "config": {
          "name": "mmlu:high_school_macroeconomics",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "high_school_macroeconomics",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0ca9490>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:high_school_mathematics",
        "config": {
          "name": "mmlu:high_school_mathematics",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "high_school_mathematics",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0ca9850>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:high_school_microeconomics",
        "config": {
          "name": "mmlu:high_school_microeconomics",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "high_school_microeconomics",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0ca9c10>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:high_school_physics",
        "config": {
          "name": "mmlu:high_school_physics",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "high_school_physics",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0ca9fd0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:high_school_psychology",
        "config": {
          "name": "mmlu:high_school_psychology",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "high_school_psychology",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0caa390>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:high_school_statistics",
        "config": {
          "name": "mmlu:high_school_statistics",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "high_school_statistics",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0caa750>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:high_school_us_history",
        "config": {
          "name": "mmlu:high_school_us_history",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "high_school_us_history",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0caab10>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:high_school_world_history",
        "config": {
          "name": "mmlu:high_school_world_history",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "high_school_world_history",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0caaed0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:human_aging",
        "config": {
          "name": "mmlu:human_aging",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "human_aging",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cab290>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:human_sexuality",
        "config": {
          "name": "mmlu:human_sexuality",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "human_sexuality",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cab650>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:international_law",
        "config": {
          "name": "mmlu:international_law",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "international_law",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0caba10>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:jurisprudence",
        "config": {
          "name": "mmlu:jurisprudence",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "jurisprudence",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cabdd0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:logical_fallacies",
        "config": {
          "name": "mmlu:logical_fallacies",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "logical_fallacies",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cac1d0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:machine_learning",
        "config": {
          "name": "mmlu:machine_learning",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "machine_learning",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cac590>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:management",
        "config": {
          "name": "mmlu:management",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "management",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cac950>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:marketing",
        "config": {
          "name": "mmlu:marketing",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "marketing",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cacd10>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:medical_genetics",
        "config": {
          "name": "mmlu:medical_genetics",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "medical_genetics",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cad0d0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:miscellaneous",
        "config": {
          "name": "mmlu:miscellaneous",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "miscellaneous",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cad490>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:moral_disputes",
        "config": {
          "name": "mmlu:moral_disputes",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "moral_disputes",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cad850>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:moral_scenarios",
        "config": {
          "name": "mmlu:moral_scenarios",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "moral_scenarios",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cadc10>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:nutrition",
        "config": {
          "name": "mmlu:nutrition",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "nutrition",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cadfd0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:philosophy",
        "config": {
          "name": "mmlu:philosophy",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "philosophy",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cae390>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:prehistory",
        "config": {
          "name": "mmlu:prehistory",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "prehistory",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cae750>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:professional_accounting",
        "config": {
          "name": "mmlu:professional_accounting",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "professional_accounting",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0caeb10>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:professional_law",
        "config": {
          "name": "mmlu:professional_law",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "professional_law",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0caeed0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:professional_medicine",
        "config": {
          "name": "mmlu:professional_medicine",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "professional_medicine",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0caf290>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:professional_psychology",
        "config": {
          "name": "mmlu:professional_psychology",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "professional_psychology",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0caf650>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:public_relations",
        "config": {
          "name": "mmlu:public_relations",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "public_relations",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cafa10>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:security_studies",
        "config": {
          "name": "mmlu:security_studies",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "security_studies",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cafdd0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:sociology",
        "config": {
          "name": "mmlu:sociology",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "sociology",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cb81d0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:us_foreign_policy",
        "config": {
          "name": "mmlu:us_foreign_policy",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "us_foreign_policy",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cb8590>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:virology",
        "config": {
          "name": "mmlu:virology",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "virology",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cb8950>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu:world_religions",
        "config": {
          "name": "mmlu:world_religions",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "lighteval/mmlu",
          "hf_subset": "world_religions",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cb8d10>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('auxiliary_train', 'test', 'validation', 'dev')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "dev",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.arc_agi_2",
    "docstring": {
      "name": "ArcAgi 2",
      "dataset": [
        "arc-agi-community/arc-agi-2"
      ],
      "abstract": "ARC-AGI tasks are a series of three to five input and output tasks followed by a\nfinal task with only the input listed. Each task tests the utilization of a\nspecific learned skill based on a minimal number of cognitive priors.\nIn their native form, tasks are a JSON lists of integers. These JSON can also be\nrepresented visually as a grid of colors using an ARC-AGI task viewer. You can\nview an example of a task here.\nA successful submission is a pixel-perfect description (color and position) of\nthe final task's output.\n100% of tasks in the ARC-AGI-2 dataset were solved by a minimim of 2 people in\nless than or equal to 2 attempts (many were solved more). ARC-AGI-2 is more\ndifficult for AI.",
      "languages": [
        "english"
      ],
      "tags": [
        "multiple-choice"
      ],
      "paper": "https://arcprize.org/guide"
    },
    "tasks": [
      {
        "name": "arc_agi_2",
        "config": {
          "name": "arc_agi_2",
          "prompt_function": "<function arc_agi_2_prompt at 0x7717f34120c0>",
          "hf_repo": "arc-agi-community/arc-agi-2",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cb91c0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function generate.<locals>.solve at 0x7717f3412200>]",
          "scorer": "<function exact.<locals>.score at 0x7717f3412340>",
          "sample_fields": "<function record_to_sample at 0x7717f3412160>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.race_high",
    "docstring": {
      "name": "Race High",
      "dataset": [
        "EleutherAI/race"
      ],
      "abstract": "RACE is a large-scale reading comprehension dataset with more than 28,000\npassages and nearly 100,000 questions. The dataset is collected from English\nexaminations in China, which are designed for middle school and high school\nstudents. The dataset can be served as the training and test sets for machine\ncomprehension.",
      "languages": [
        "english"
      ],
      "tags": [
        "multiple-choice",
        "reading-comprehension"
      ],
      "paper": "https://aclanthology.org/D17-1082/"
    },
    "tasks": [
      {
        "name": "race:high",
        "config": {
          "name": "race:high",
          "prompt_function": "<function race_prompt at 0x7717f34123e0>",
          "hf_repo": "EleutherAI/race",
          "hf_subset": "high",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0cb9610>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.storycloze",
    "docstring": {
      "name": "Storycloze",
      "dataset": [
        "MoE-UNC/story_cloze"
      ],
      "abstract": "A Corpus and Cloze Evaluation for Deeper Understanding of\nCommonsense Stories",
      "languages": [
        "english"
      ],
      "tags": [
        "narrative",
        "reasoning"
      ],
      "paper": "https://arxiv.org/abs/1604.01696"
    },
    "tasks": [
      {
        "name": "storycloze:2016",
        "config": {
          "name": "storycloze:2016",
          "prompt_function": "<function storycloze_prompt at 0x7717f3412660>",
          "hf_repo": "MoE-UNC/story_cloze",
          "hf_subset": "2016",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cb9b20>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('validation',)",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "storycloze:2018",
        "config": {
          "name": "storycloze:2018",
          "prompt_function": "<function storycloze_prompt at 0x7717f3412660>",
          "hf_repo": "MoE-UNC/story_cloze",
          "hf_subset": "2018",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cb9f40>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('validation',)",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.squad_v2",
    "docstring": {
      "name": "Squad V2",
      "dataset": [
        "rajpurkar/squad_v2"
      ],
      "abstract": "Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset,\nconsisting of questions posed by crowdworkers on a set of Wikipedia articles,\nwhere the answer to every question is a segment of text, or span, from the\ncorresponding reading passage, or the question might be unanswerable.\nSQuAD 2.0 combines the 100,000 questions in SQuAD1.1 with over 50,000\nunanswerable questions written adversarially by crowdworkers to look similar to\nanswerable ones. To do well on SQuAD2.0, systems must not only answer questions\nwhen possible, but also determine when no answer is supported by the paragraph\nand abstain from answering.",
      "languages": [
        "english"
      ],
      "tags": [
        "qa"
      ],
      "paper": "https://arxiv.org/abs/1806.03822"
    },
    "tasks": [
      {
        "name": "squad_v2",
        "config": {
          "name": "squad_v2",
          "prompt_function": "<function get_mcq_prompt_function.<locals>.prompt_fn at 0x7717f3412840>",
          "hf_repo": "rajpurkar/squad_v2",
          "hf_subset": "squad_v2",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cba480>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "<function <lambda> at 0x7717f34128e0>",
          "hf_avail_splits": "('train', 'validation', 'test')",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "train",
          "few_shots_select": "None",
          "generation_size": "200",
          "generation_grammar": "None",
          "stop_sequence": "['\\n', 'Question:', 'question:']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "1"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.simpleqa",
    "docstring": {
      "name": "Simpleqa",
      "dataset": [
        "lighteval/SimpleQA"
      ],
      "abstract": "A factuality benchmark called SimpleQA that measures the ability for language\nmodels to answer short, fact-seeking questions.",
      "languages": [
        "english"
      ],
      "tags": [
        "factuality",
        "general-knowledge",
        "qa"
      ],
      "paper": "https://openai.com/index/introducing-simpleqa/",
      "starred": "true"
    },
    "tasks": [
      {
        "name": "simpleqa",
        "config": {
          "name": "simpleqa",
          "prompt_function": "<function simpleqa_prompt at 0x7717f3412980>",
          "hf_repo": "lighteval/SimpleQA",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cba960>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function generate.<locals>.solve at 0x7717f3412ac0>]",
          "scorer": "<function _model_graded_qa_single.<locals>.score at 0x7717f3412c00>",
          "sample_fields": "<function record_to_sample at 0x7717f3412a20>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "few_shot",
          "few_shots_select": "None",
          "generation_size": "2048",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.openbookqa",
    "docstring": {
      "name": "Openbookqa",
      "dataset": [
        "allenai/openbookqa"
      ],
      "abstract": "OpenBookQA is a question-answering dataset modeled after open-book exams for\nassessing human understanding of a subject. It contains multiple-choice\nquestions that require combining facts from a given open book with broad common\nknowledge. The task tests language models' ability to leverage provided\ninformation and apply common sense reasoning.",
      "languages": [
        "english"
      ],
      "tags": [
        "multiple-choice",
        "qa"
      ],
      "paper": "https://arxiv.org/abs/1809.02789"
    },
    "tasks": [
      {
        "name": "openbookqa",
        "config": {
          "name": "openbookqa",
          "prompt_function": "<function openbookqa_prompt at 0x7717f3412de0>",
          "hf_repo": "allenai/openbookqa",
          "hf_subset": "main",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cbade0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.ethics",
    "docstring": {
      "name": "Ethics",
      "dataset": [
        "lighteval/hendrycks_ethics"
      ],
      "abstract": "The Ethics benchmark for evaluating the ability of language models to reason about\nethical issues.",
      "languages": [
        "english"
      ],
      "tags": [
        "classification",
        "ethics",
        "justice",
        "morality",
        "utilitarianism",
        "virtue"
      ],
      "paper": "https://arxiv.org/abs/2008.02275"
    },
    "tasks": [
      {
        "name": "ethics:commonsense",
        "config": {
          "name": "ethics:commonsense",
          "prompt_function": "<function ethics_commonsense_prompt at 0x7717f3412d40>",
          "hf_repo": "lighteval/hendrycks_ethics",
          "hf_subset": "commonsense",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0cbb350>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'validation', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "ethics:deontology",
        "config": {
          "name": "ethics:deontology",
          "prompt_function": "<function ethics_deontology_prompt at 0x7717f3412e80>",
          "hf_repo": "lighteval/hendrycks_ethics",
          "hf_subset": "deontology",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0cbb740>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'validation', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "ethics:justice",
        "config": {
          "name": "ethics:justice",
          "prompt_function": "<function ethics_justice_prompt at 0x7717f3412f20>",
          "hf_repo": "lighteval/hendrycks_ethics",
          "hf_subset": "justice",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0cbbb30>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'validation', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "ethics:utilitarianism",
        "config": {
          "name": "ethics:utilitarianism",
          "prompt_function": "<function ethics_utilitarianism_prompt at 0x7717f3412fc0>",
          "hf_repo": "lighteval/hendrycks_ethics",
          "hf_subset": "utilitarianism",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0cbbf20>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'validation', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "ethics:virtue",
        "config": {
          "name": "ethics:virtue",
          "prompt_function": "<function ethics_virtue_prompt at 0x7717f3413060>",
          "hf_repo": "lighteval/hendrycks_ethics",
          "hf_subset": "virtue",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0cc0320>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'validation', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.logiqa",
    "docstring": {
      "name": "Logiqa",
      "dataset": [
        "lighteval/logiqa_harness"
      ],
      "abstract": "LogiQA is a machine reading comprehension dataset focused on testing logical\nreasoning abilities. It contains 8,678 expert-written multiple-choice questions\ncovering various types of deductive reasoning. While humans perform strongly,\nstate-of-the-art models lag far behind, making LogiQA a benchmark for advancing\nlogical reasoning in NLP systems.",
      "languages": [
        "english"
      ],
      "tags": [
        "qa"
      ],
      "paper": "https://arxiv.org/abs/2007.08124"
    },
    "tasks": [
      {
        "name": "logiqa",
        "config": {
          "name": "logiqa",
          "prompt_function": "<function logiqa_prompt at 0x7717f3413100>",
          "hf_repo": "lighteval/logiqa_harness",
          "hf_subset": "logiqa",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0cc0860>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'validation', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.sacrebleu",
    "docstring": {
      "name": "Sacrebleu",
      "dataset": [
        "lighteval/sacrebleu_manual",
        "wmt14",
        "wmt16"
      ],
      "abstract": "tasks from sacrebleu",
      "languages": [
        "english",
        "german",
        "french",
        "japanese",
        "korean",
        "chinese",
        "arabic"
      ],
      "tags": [
        "translation"
      ],
      "paper": "https://github.com/mjpost/sacrebleu"
    },
    "tasks": [
      {
        "name": "wmt14:de-en",
        "config": {
          "name": "wmt14:de-en",
          "prompt_function": "<function wmt_alphabetical_prompt at 0x7717f34132e0>",
          "hf_repo": "lighteval/sacrebleu_manual",
          "hf_subset": "wmt14_de-en",
          "metrics": "({'metric_name': 'bleu', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cc0e90>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cc0ef0>, 'batched_compute': False}, {'metric_name': 'chrf', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cc0f50>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cc0fb0>, 'batched_compute': False}, {'metric_name': 'ter', 'higher_is_better': False, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cc1010>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cc1070>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wmt16:en-cs",
        "config": {
          "name": "wmt16:en-cs",
          "prompt_function": "<function wmt_reverse_alphabetical_prompt at 0x7717f3413b00>",
          "hf_repo": "lighteval/sacrebleu_manual",
          "hf_subset": "wmt16_en-cs",
          "metrics": "({'metric_name': 'bleu', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cc1370>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cc13d0>, 'batched_compute': False}, {'metric_name': 'chrf', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cc1430>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cc1490>, 'batched_compute': False}, {'metric_name': 'ter', 'higher_is_better': False, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cc14f0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cc1550>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wmt19:en-cs",
        "config": {
          "name": "wmt19:en-cs",
          "prompt_function": "<function wmt_reverse_alphabetical_prompt at 0x7717f3413b00>",
          "hf_repo": "lighteval/sacrebleu_manual",
          "hf_subset": "wmt19_en-cs",
          "metrics": "({'metric_name': 'bleu', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cc1850>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cc18b0>, 'batched_compute': False}, {'metric_name': 'chrf', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cc1910>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cc1970>, 'batched_compute': False}, {'metric_name': 'ter', 'higher_is_better': False, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cc19d0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cc1a30>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wmt19:en-de",
        "config": {
          "name": "wmt19:en-de",
          "prompt_function": "<function wmt_reverse_alphabetical_prompt at 0x7717f3413b00>",
          "hf_repo": "lighteval/sacrebleu_manual",
          "hf_subset": "wmt19_en-de",
          "metrics": "({'metric_name': 'bleu', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cc1d30>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cc1d90>, 'batched_compute': False}, {'metric_name': 'chrf', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cc1df0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cc1e50>, 'batched_compute': False}, {'metric_name': 'ter', 'higher_is_better': False, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cc1eb0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cc1f10>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wmt19:en-fi",
        "config": {
          "name": "wmt19:en-fi",
          "prompt_function": "<function wmt_alphabetical_prompt at 0x7717f34132e0>",
          "hf_repo": "lighteval/sacrebleu_manual",
          "hf_subset": "wmt19_en-fi",
          "metrics": "({'metric_name': 'bleu', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cc2210>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cc2270>, 'batched_compute': False}, {'metric_name': 'chrf', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cc22d0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cc2330>, 'batched_compute': False}, {'metric_name': 'ter', 'higher_is_better': False, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cc2390>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cc2420>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wmt19:en-gu",
        "config": {
          "name": "wmt19:en-gu",
          "prompt_function": "<function wmt_alphabetical_prompt at 0x7717f34132e0>",
          "hf_repo": "lighteval/sacrebleu_manual",
          "hf_subset": "wmt19_en-gu",
          "metrics": "({'metric_name': 'bleu', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cc26f0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cc2780>, 'batched_compute': False}, {'metric_name': 'chrf', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cc27e0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cc2840>, 'batched_compute': False}, {'metric_name': 'ter', 'higher_is_better': False, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cc28a0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cc2900>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wmt19:en-kk",
        "config": {
          "name": "wmt19:en-kk",
          "prompt_function": "<function wmt_alphabetical_prompt at 0x7717f34132e0>",
          "hf_repo": "lighteval/sacrebleu_manual",
          "hf_subset": "wmt19_en-kk",
          "metrics": "({'metric_name': 'bleu', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cc2bd0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cc2c30>, 'batched_compute': False}, {'metric_name': 'chrf', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cc2c90>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cc2cf0>, 'batched_compute': False}, {'metric_name': 'ter', 'higher_is_better': False, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cc2d50>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cc2db0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wmt19:en-lt",
        "config": {
          "name": "wmt19:en-lt",
          "prompt_function": "<function wmt_alphabetical_prompt at 0x7717f34132e0>",
          "hf_repo": "lighteval/sacrebleu_manual",
          "hf_subset": "wmt19_en-lt",
          "metrics": "({'metric_name': 'bleu', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cc30b0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cc3110>, 'batched_compute': False}, {'metric_name': 'chrf', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cc3170>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cc31d0>, 'batched_compute': False}, {'metric_name': 'ter', 'higher_is_better': False, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cc3230>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cc3290>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wmt19:en-ru",
        "config": {
          "name": "wmt19:en-ru",
          "prompt_function": "<function wmt_alphabetical_prompt at 0x7717f34132e0>",
          "hf_repo": "lighteval/sacrebleu_manual",
          "hf_subset": "wmt19_en-ru",
          "metrics": "({'metric_name': 'bleu', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cc3590>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cc35f0>, 'batched_compute': False}, {'metric_name': 'chrf', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cc3650>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cc36b0>, 'batched_compute': False}, {'metric_name': 'ter', 'higher_is_better': False, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cc3710>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cc3770>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wmt19:en-zh",
        "config": {
          "name": "wmt19:en-zh",
          "prompt_function": "<function wmt_alphabetical_prompt at 0x7717f34132e0>",
          "hf_repo": "lighteval/sacrebleu_manual",
          "hf_subset": "wmt19_en-zh",
          "metrics": "({'metric_name': 'bleu', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cc3a70>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cc3ad0>, 'batched_compute': False}, {'metric_name': 'chrf', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cc3b30>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cc3b90>, 'batched_compute': False}, {'metric_name': 'ter', 'higher_is_better': False, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cc3bf0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cc3c50>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wmt19:fi-en",
        "config": {
          "name": "wmt19:fi-en",
          "prompt_function": "<function wmt_reverse_alphabetical_prompt at 0x7717f3413b00>",
          "hf_repo": "lighteval/sacrebleu_manual",
          "hf_subset": "wmt19_fi-en",
          "metrics": "({'metric_name': 'bleu', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cc3f50>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cc3fb0>, 'batched_compute': False}, {'metric_name': 'chrf', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0ccc050>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0ccc0b0>, 'batched_compute': False}, {'metric_name': 'ter', 'higher_is_better': False, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0ccc110>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0ccc170>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wmt19:fr-de",
        "config": {
          "name": "wmt19:fr-de",
          "prompt_function": "<function wmt_reverse_alphabetical_prompt at 0x7717f3413b00>",
          "hf_repo": "lighteval/sacrebleu_manual",
          "hf_subset": "wmt19_fr-de",
          "metrics": "({'metric_name': 'bleu', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0ccc470>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0ccc4d0>, 'batched_compute': False}, {'metric_name': 'chrf', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0ccc530>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0ccc590>, 'batched_compute': False}, {'metric_name': 'ter', 'higher_is_better': False, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0ccc5f0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0ccc650>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wmt19:gu-en",
        "config": {
          "name": "wmt19:gu-en",
          "prompt_function": "<function wmt_reverse_alphabetical_prompt at 0x7717f3413b00>",
          "hf_repo": "lighteval/sacrebleu_manual",
          "hf_subset": "wmt19_gu-en",
          "metrics": "({'metric_name': 'bleu', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0ccc950>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0ccc9b0>, 'batched_compute': False}, {'metric_name': 'chrf', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0ccca10>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0ccca70>, 'batched_compute': False}, {'metric_name': 'ter', 'higher_is_better': False, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cccad0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cccb30>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wmt19:kk-en",
        "config": {
          "name": "wmt19:kk-en",
          "prompt_function": "<function wmt_reverse_alphabetical_prompt at 0x7717f3413b00>",
          "hf_repo": "lighteval/sacrebleu_manual",
          "hf_subset": "wmt19_kk-en",
          "metrics": "({'metric_name': 'bleu', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0ccce30>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0ccce90>, 'batched_compute': False}, {'metric_name': 'chrf', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cccef0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cccf50>, 'batched_compute': False}, {'metric_name': 'ter', 'higher_is_better': False, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cccfb0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0ccd010>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wmt19:lt-en",
        "config": {
          "name": "wmt19:lt-en",
          "prompt_function": "<function wmt_reverse_alphabetical_prompt at 0x7717f3413b00>",
          "hf_repo": "lighteval/sacrebleu_manual",
          "hf_subset": "wmt19_lt-en",
          "metrics": "({'metric_name': 'bleu', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0ccd310>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0ccd370>, 'batched_compute': False}, {'metric_name': 'chrf', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0ccd3d0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0ccd430>, 'batched_compute': False}, {'metric_name': 'ter', 'higher_is_better': False, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0ccd490>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0ccd4f0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wmt19:ru-en",
        "config": {
          "name": "wmt19:ru-en",
          "prompt_function": "<function wmt_reverse_alphabetical_prompt at 0x7717f3413b00>",
          "hf_repo": "lighteval/sacrebleu_manual",
          "hf_subset": "wmt19_ru-en",
          "metrics": "({'metric_name': 'bleu', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0ccd7f0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0ccd850>, 'batched_compute': False}, {'metric_name': 'chrf', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0ccd8b0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0ccd910>, 'batched_compute': False}, {'metric_name': 'ter', 'higher_is_better': False, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0ccd970>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0ccd9d0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wmt19:zh-en",
        "config": {
          "name": "wmt19:zh-en",
          "prompt_function": "<function wmt_reverse_alphabetical_prompt at 0x7717f3413b00>",
          "hf_repo": "lighteval/sacrebleu_manual",
          "hf_subset": "wmt19_zh-en",
          "metrics": "({'metric_name': 'bleu', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0ccdcd0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0ccdd30>, 'batched_compute': False}, {'metric_name': 'chrf', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0ccdd90>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0ccddf0>, 'batched_compute': False}, {'metric_name': 'ter', 'higher_is_better': False, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0ccde50>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0ccdeb0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wmt20:cs-en",
        "config": {
          "name": "wmt20:cs-en",
          "prompt_function": "<function wmt_alphabetical_prompt at 0x7717f34132e0>",
          "hf_repo": "lighteval/sacrebleu_manual",
          "hf_subset": "wmt20_cs-en",
          "metrics": "({'metric_name': 'bleu', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cce1b0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cce210>, 'batched_compute': False}, {'metric_name': 'chrf', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cce270>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cce2d0>, 'batched_compute': False}, {'metric_name': 'ter', 'higher_is_better': False, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cce330>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cce390>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wmt20:de-en",
        "config": {
          "name": "wmt20:de-en",
          "prompt_function": "<function wmt_alphabetical_prompt at 0x7717f34132e0>",
          "hf_repo": "lighteval/sacrebleu_manual",
          "hf_subset": "wmt20_de-en",
          "metrics": "({'metric_name': 'bleu', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cce690>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cce6f0>, 'batched_compute': False}, {'metric_name': 'chrf', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cce750>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cce7b0>, 'batched_compute': False}, {'metric_name': 'ter', 'higher_is_better': False, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cce810>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cce870>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wmt20:en-de",
        "config": {
          "name": "wmt20:en-de",
          "prompt_function": "<function wmt_reverse_alphabetical_prompt at 0x7717f3413b00>",
          "hf_repo": "lighteval/sacrebleu_manual",
          "hf_subset": "wmt20_en-de",
          "metrics": "({'metric_name': 'bleu', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cceb70>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0ccebd0>, 'batched_compute': False}, {'metric_name': 'chrf', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0ccec30>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0ccec90>, 'batched_compute': False}, {'metric_name': 'ter', 'higher_is_better': False, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0ccecf0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cced50>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wmt20:en-iu",
        "config": {
          "name": "wmt20:en-iu",
          "prompt_function": "<function wmt_alphabetical_prompt at 0x7717f34132e0>",
          "hf_repo": "lighteval/sacrebleu_manual",
          "hf_subset": "wmt20_en-iu",
          "metrics": "({'metric_name': 'bleu', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0ccf050>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0ccf0b0>, 'batched_compute': False}, {'metric_name': 'chrf', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0ccf110>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0ccf170>, 'batched_compute': False}, {'metric_name': 'ter', 'higher_is_better': False, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0ccf1d0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0ccf230>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wmt20:en-ja",
        "config": {
          "name": "wmt20:en-ja",
          "prompt_function": "<function wmt_alphabetical_prompt at 0x7717f34132e0>",
          "hf_repo": "lighteval/sacrebleu_manual",
          "hf_subset": "wmt20_en-ja",
          "metrics": "({'metric_name': 'bleu', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0ccf530>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0ccf590>, 'batched_compute': False}, {'metric_name': 'chrf', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0ccf5f0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0ccf650>, 'batched_compute': False}, {'metric_name': 'ter', 'higher_is_better': False, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0ccf6b0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0ccf710>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wmt20:en-km",
        "config": {
          "name": "wmt20:en-km",
          "prompt_function": "<function wmt_alphabetical_prompt at 0x7717f34132e0>",
          "hf_repo": "lighteval/sacrebleu_manual",
          "hf_subset": "wmt20_en-km",
          "metrics": "({'metric_name': 'bleu', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0ccfa10>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0ccfa70>, 'batched_compute': False}, {'metric_name': 'chrf', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0ccfad0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0ccfb30>, 'batched_compute': False}, {'metric_name': 'ter', 'higher_is_better': False, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0ccfb90>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0ccfbf0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wmt20:en-pl",
        "config": {
          "name": "wmt20:en-pl",
          "prompt_function": "<function wmt_alphabetical_prompt at 0x7717f34132e0>",
          "hf_repo": "lighteval/sacrebleu_manual",
          "hf_subset": "wmt20_en-pl",
          "metrics": "({'metric_name': 'bleu', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0ccfef0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0ccff50>, 'batched_compute': False}, {'metric_name': 'chrf', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0ccffb0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cd0050>, 'batched_compute': False}, {'metric_name': 'ter', 'higher_is_better': False, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cd00b0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cd0110>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wmt20:en-ps",
        "config": {
          "name": "wmt20:en-ps",
          "prompt_function": "<function wmt_alphabetical_prompt at 0x7717f34132e0>",
          "hf_repo": "lighteval/sacrebleu_manual",
          "hf_subset": "wmt20_en-ps",
          "metrics": "({'metric_name': 'bleu', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cd0410>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cd0470>, 'batched_compute': False}, {'metric_name': 'chrf', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cd04d0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cd0530>, 'batched_compute': False}, {'metric_name': 'ter', 'higher_is_better': False, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cd0590>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cd05f0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wmt20:en-ru",
        "config": {
          "name": "wmt20:en-ru",
          "prompt_function": "<function wmt_alphabetical_prompt at 0x7717f34132e0>",
          "hf_repo": "lighteval/sacrebleu_manual",
          "hf_subset": "wmt20_en-ru",
          "metrics": "({'metric_name': 'bleu', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cd08f0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cd0950>, 'batched_compute': False}, {'metric_name': 'chrf', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cd09b0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cd0a10>, 'batched_compute': False}, {'metric_name': 'ter', 'higher_is_better': False, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cd0a70>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cd0ad0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wmt20:en-ta",
        "config": {
          "name": "wmt20:en-ta",
          "prompt_function": "<function wmt_alphabetical_prompt at 0x7717f34132e0>",
          "hf_repo": "lighteval/sacrebleu_manual",
          "hf_subset": "wmt20_en-ta",
          "metrics": "({'metric_name': 'bleu', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cd0dd0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cd0e30>, 'batched_compute': False}, {'metric_name': 'chrf', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cd0e90>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cd0ef0>, 'batched_compute': False}, {'metric_name': 'ter', 'higher_is_better': False, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cd0f50>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cd0fb0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wmt20:en-zh",
        "config": {
          "name": "wmt20:en-zh",
          "prompt_function": "<function wmt_alphabetical_prompt at 0x7717f34132e0>",
          "hf_repo": "lighteval/sacrebleu_manual",
          "hf_subset": "wmt20_en-zh",
          "metrics": "({'metric_name': 'bleu', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cd12b0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cd1310>, 'batched_compute': False}, {'metric_name': 'chrf', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cd1370>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cd13d0>, 'batched_compute': False}, {'metric_name': 'ter', 'higher_is_better': False, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cd1430>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cd1490>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wmt20:fr-de",
        "config": {
          "name": "wmt20:fr-de",
          "prompt_function": "<function wmt_reverse_alphabetical_prompt at 0x7717f3413b00>",
          "hf_repo": "lighteval/sacrebleu_manual",
          "hf_subset": "wmt20_fr-de",
          "metrics": "({'metric_name': 'bleu', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cd1790>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cd17f0>, 'batched_compute': False}, {'metric_name': 'chrf', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cd1850>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cd18b0>, 'batched_compute': False}, {'metric_name': 'ter', 'higher_is_better': False, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cd1910>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cd1970>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wmt20:iu-en",
        "config": {
          "name": "wmt20:iu-en",
          "prompt_function": "<function wmt_reverse_alphabetical_prompt at 0x7717f3413b00>",
          "hf_repo": "lighteval/sacrebleu_manual",
          "hf_subset": "wmt20_iu-en",
          "metrics": "({'metric_name': 'bleu', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cd1c70>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cd1cd0>, 'batched_compute': False}, {'metric_name': 'chrf', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cd1d30>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cd1d90>, 'batched_compute': False}, {'metric_name': 'ter', 'higher_is_better': False, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cd1df0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cd1e50>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wmt20:ja-en",
        "config": {
          "name": "wmt20:ja-en",
          "prompt_function": "<function wmt_reverse_alphabetical_prompt at 0x7717f3413b00>",
          "hf_repo": "lighteval/sacrebleu_manual",
          "hf_subset": "wmt20_ja-en",
          "metrics": "({'metric_name': 'bleu', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cd2150>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cd21b0>, 'batched_compute': False}, {'metric_name': 'chrf', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cd2210>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cd2270>, 'batched_compute': False}, {'metric_name': 'ter', 'higher_is_better': False, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cd22d0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cd2330>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wmt20:km-en",
        "config": {
          "name": "wmt20:km-en",
          "prompt_function": "<function wmt_reverse_alphabetical_prompt at 0x7717f3413b00>",
          "hf_repo": "lighteval/sacrebleu_manual",
          "hf_subset": "wmt20_km-en",
          "metrics": "({'metric_name': 'bleu', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cd2630>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cd2690>, 'batched_compute': False}, {'metric_name': 'chrf', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cd26f0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cd2750>, 'batched_compute': False}, {'metric_name': 'ter', 'higher_is_better': False, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cd27b0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cd2810>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wmt20:pl-en",
        "config": {
          "name": "wmt20:pl-en",
          "prompt_function": "<function wmt_reverse_alphabetical_prompt at 0x7717f3413b00>",
          "hf_repo": "lighteval/sacrebleu_manual",
          "hf_subset": "wmt20_pl-en",
          "metrics": "({'metric_name': 'bleu', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cd2b10>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cd2b70>, 'batched_compute': False}, {'metric_name': 'chrf', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cd2bd0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cd2c30>, 'batched_compute': False}, {'metric_name': 'ter', 'higher_is_better': False, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cd2c90>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cd2cf0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wmt20:ps-en",
        "config": {
          "name": "wmt20:ps-en",
          "prompt_function": "<function wmt_reverse_alphabetical_prompt at 0x7717f3413b00>",
          "hf_repo": "lighteval/sacrebleu_manual",
          "hf_subset": "wmt20_ps-en",
          "metrics": "({'metric_name': 'bleu', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cd2ff0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cd3050>, 'batched_compute': False}, {'metric_name': 'chrf', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cd30b0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cd3110>, 'batched_compute': False}, {'metric_name': 'ter', 'higher_is_better': False, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cd3170>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cd31d0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wmt20:ru-en",
        "config": {
          "name": "wmt20:ru-en",
          "prompt_function": "<function wmt_reverse_alphabetical_prompt at 0x7717f3413b00>",
          "hf_repo": "lighteval/sacrebleu_manual",
          "hf_subset": "wmt20_ru-en",
          "metrics": "({'metric_name': 'bleu', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cd34d0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cd3530>, 'batched_compute': False}, {'metric_name': 'chrf', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cd3590>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cd35f0>, 'batched_compute': False}, {'metric_name': 'ter', 'higher_is_better': False, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cd3650>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cd36b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wmt20:ta-en",
        "config": {
          "name": "wmt20:ta-en",
          "prompt_function": "<function wmt_reverse_alphabetical_prompt at 0x7717f3413b00>",
          "hf_repo": "lighteval/sacrebleu_manual",
          "hf_subset": "wmt20_ta-en",
          "metrics": "({'metric_name': 'bleu', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cd39b0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cd3a10>, 'batched_compute': False}, {'metric_name': 'chrf', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cd3a70>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cd3ad0>, 'batched_compute': False}, {'metric_name': 'ter', 'higher_is_better': False, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cd3b30>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cd3b90>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "wmt20:zh-en",
        "config": {
          "name": "wmt20:zh-en",
          "prompt_function": "<function wmt_reverse_alphabetical_prompt at 0x7717f3413b00>",
          "hf_repo": "lighteval/sacrebleu_manual",
          "hf_subset": "wmt20_zh-en",
          "metrics": "({'metric_name': 'bleu', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cd3e90>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cd3ef0>, 'batched_compute': False}, {'metric_name': 'chrf', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cd3f50>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cd3fb0>, 'batched_compute': False}, {'metric_name': 'ter', 'higher_is_better': False, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.GenerativePreparator object at 0x7717f0cd8050>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelTranslationMetric object at 0x7717f0cd80b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.synthetic_reasoning",
    "docstring": {
      "name": "Synthetic Reasoning",
      "dataset": [
        "lighteval/synthetic_reasoning",
        "lighteval/synthetic_reasoning_natural"
      ],
      "abstract": "LIME: Learning Inductive Bias for Primitives of Mathematical Reasoning",
      "languages": [
        "english"
      ],
      "tags": [
        "reasoning"
      ],
      "paper": "https://arxiv.org/abs/2206.03855"
    },
    "tasks": [
      {
        "name": "synthetic_reasoning:induction",
        "config": {
          "name": "synthetic_reasoning:induction",
          "prompt_function": "<function synthetic_reasoning_prompt at 0x7717f34549a0>",
          "hf_repo": "lighteval/synthetic_reasoning",
          "hf_subset": "induction",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cd84a0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "50",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "synthetic_reasoning:natural_easy",
        "config": {
          "name": "synthetic_reasoning:natural_easy",
          "prompt_function": "<function synthetic_reasoning_natural_prompt at 0x7717f3454fe0>",
          "hf_repo": "lighteval/synthetic_reasoning_natural",
          "hf_subset": "easy",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cd8860>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "20",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "synthetic_reasoning:natural_hard",
        "config": {
          "name": "synthetic_reasoning:natural_hard",
          "prompt_function": "<function synthetic_reasoning_natural_prompt at 0x7717f3454fe0>",
          "hf_repo": "lighteval/synthetic_reasoning_natural",
          "hf_subset": "hard",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cd8c20>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "20",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "synthetic_reasoning:pattern_match",
        "config": {
          "name": "synthetic_reasoning:pattern_match",
          "prompt_function": "<function synthetic_reasoning_prompt at 0x7717f34549a0>",
          "hf_repo": "lighteval/synthetic_reasoning",
          "hf_subset": "pattern_match",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cd8fe0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "50",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "synthetic_reasoning:variable_substitution",
        "config": {
          "name": "synthetic_reasoning:variable_substitution",
          "prompt_function": "<function synthetic_reasoning_prompt at 0x7717f34549a0>",
          "hf_repo": "lighteval/synthetic_reasoning",
          "hf_subset": "variable_substitution",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cd9370>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "50",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.real_toxicity_prompts",
    "docstring": {
      "name": "Real Toxicity Prompts",
      "dataset": [
        "allenai/real-toxicity-prompts"
      ],
      "abstract": "The RealToxicityPrompts dataset for measuring toxicity in prompted model generations",
      "languages": [
        "english"
      ],
      "tags": [
        "generation",
        "safety"
      ],
      "paper": "https://aclanthology.org/2020.findings-emnlp.301/"
    },
    "tasks": [
      {
        "name": "real_toxicity_prompts",
        "config": {
          "name": "real_toxicity_prompts",
          "prompt_function": "<function real_toxicity_prompts_prompt at 0x7717f3455080>",
          "hf_repo": "allenai/real-toxicity-prompts",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cd9850>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "20",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.unscramble",
    "docstring": {
      "name": "Unscramble",
      "dataset": [
        "lighteval/GPT3_unscramble"
      ],
      "abstract": "Benchmark where we ask the model to unscramble a word, either anagram or\nrandom insertion.",
      "languages": [
        "english"
      ],
      "tags": [
        "language-modeling",
        "reasoning"
      ],
      "paper": "https://huggingface.co/datasets/lighteval/GPT3_unscramble"
    },
    "tasks": [
      {
        "name": "unscramble:anagrams1",
        "config": {
          "name": "unscramble:anagrams1",
          "prompt_function": "<function unscramble_prompt at 0x7717f34551c0>",
          "hf_repo": "lighteval/GPT3_unscramble",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'em:strip_strings=False', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cd9d60>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('mid_word_1_anagrams',)",
          "evaluation_splits": "('mid_word_1_anagrams',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "unscramble:anagrams2",
        "config": {
          "name": "unscramble:anagrams2",
          "prompt_function": "<function unscramble_prompt at 0x7717f34551c0>",
          "hf_repo": "lighteval/GPT3_unscramble",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'em:strip_strings=False', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cda180>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('mid_word_2_anagrams',)",
          "evaluation_splits": "('mid_word_2_anagrams',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "unscramble:cycle_letters",
        "config": {
          "name": "unscramble:cycle_letters",
          "prompt_function": "<function unscramble_prompt at 0x7717f34551c0>",
          "hf_repo": "lighteval/GPT3_unscramble",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'em:strip_strings=False', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cda5a0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('cycle_letters_in_word',)",
          "evaluation_splits": "('cycle_letters_in_word',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "unscramble:random_insertion",
        "config": {
          "name": "unscramble:random_insertion",
          "prompt_function": "<function unscramble_prompt at 0x7717f34551c0>",
          "hf_repo": "lighteval/GPT3_unscramble",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'em:strip_strings=False', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cda9c0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('random_insertion_in_word',)",
          "evaluation_splits": "('random_insertion_in_word',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "unscramble:reversed_words",
        "config": {
          "name": "unscramble:reversed_words",
          "prompt_function": "<function unscramble_prompt at 0x7717f34551c0>",
          "hf_repo": "lighteval/GPT3_unscramble",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'em:strip_strings=False', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cdadb0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('reversed_words',)",
          "evaluation_splits": "('reversed_words',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.twitterAAE",
    "docstring": {
      "name": "Twitteraae",
      "dataset": [
        "lighteval/twitterAAE"
      ],
      "abstract": "Demographic Dialectal Variation in Social Media: A Case Study of African-American English",
      "languages": [
        "english"
      ],
      "tags": [
        "language-modeling"
      ],
      "paper": "https://aclanthology.org/D16-1120/"
    },
    "tasks": [
      {
        "name": "twitterAAE:aa",
        "config": {
          "name": "twitterAAE:aa",
          "prompt_function": "<function twitter_aae_prompt at 0x7717f3455300>",
          "hf_repo": "lighteval/twitterAAE",
          "hf_subset": "aa",
          "metrics": "({'metric_name': 'word_perplexity', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0cdb2c0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0cdb320>, 'batched_compute': False}, {'metric_name': 'byte_perplexity', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0cdb380>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0cdb3e0>, 'batched_compute': False}, {'metric_name': 'bits_per_byte', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0cdb440>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0cdb4a0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "twitterAAE:white",
        "config": {
          "name": "twitterAAE:white",
          "prompt_function": "<function twitter_aae_prompt at 0x7717f3455300>",
          "hf_repo": "lighteval/twitterAAE",
          "hf_subset": "white",
          "metrics": "({'metric_name': 'word_perplexity', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0cdb7a0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0cdb800>, 'batched_compute': False}, {'metric_name': 'byte_perplexity', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0cdb860>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0cdb8c0>, 'batched_compute': False}, {'metric_name': 'bits_per_byte', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0cdb920>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0cdb980>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.tiny_benchmarks",
    "docstring": {
      "name": "Tiny Benchmarks",
      "dataset": [
        "tinyBenchmarks/tinyWinogrande",
        "tinyBenchmarks/tinyAI2_arc",
        "tinyBenchmarks/tinyHellaswag",
        "tinyBenchmarks/tinyMMLU",
        "tinyBenchmarks/tinyTruthfulQA",
        "tinyBenchmarks/tinyGSM8k"
      ],
      "abstract": "TinyBenchmarks is a benchmark for evaluating the performance of language models\non tiny benchmarks.",
      "languages": [
        "english"
      ],
      "tags": [
        "general-knowledge",
        "reasoning",
        "qa"
      ],
      "paper": "https://arxiv.org/abs/2402.14992"
    },
    "tasks": [
      {
        "name": "tiny:winogrande",
        "config": {
          "name": "tiny:winogrande",
          "prompt_function": "<function winogrande_prompt at 0x7717f3455e40>",
          "hf_repo": "tinyBenchmarks/tinyWinogrande",
          "hf_subset": "winogrande_xl",
          "metrics": "({'metric_name': ['irt', 'pirt', 'gpirt'], 'higher_is_better': {'irt': True, 'pirt': True, 'gpirt': True}, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.tasks.tasks.tiny_benchmarks.TinyCorpusAggregator object at 0x7717f0cdbda0>, 'corpus_level_fn': <lighteval.tasks.tasks.tiny_benchmarks.TinyCorpusAggregator object at 0x7717f0cdbe00>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'validation', 'test')",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "random_sampling",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "tiny:arc",
        "config": {
          "name": "tiny:arc",
          "prompt_function": "<function arc_prompt at 0x7717f35d9080>",
          "hf_repo": "tinyBenchmarks/tinyAI2_arc",
          "hf_subset": "ARC-Challenge",
          "metrics": "({'metric_name': ['irt', 'pirt', 'gpirt'], 'higher_is_better': {'irt': True, 'pirt': True, 'gpirt': True}, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.tasks.tasks.tiny_benchmarks.TinyCorpusAggregator object at 0x7717f0cdc1d0>, 'corpus_level_fn': <lighteval.tasks.tasks.tiny_benchmarks.TinyCorpusAggregator object at 0x7717f0cdc230>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'validation', 'test')",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "random_sampling",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "tiny:hellaswag",
        "config": {
          "name": "tiny:hellaswag",
          "prompt_function": "<function hellaswag_prompt at 0x7717f3455bc0>",
          "hf_repo": "tinyBenchmarks/tinyHellaswag",
          "hf_subset": "default",
          "metrics": "({'metric_name': ['irt', 'pirt', 'gpirt'], 'higher_is_better': {'irt': True, 'pirt': True, 'gpirt': True}, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.tasks.tasks.tiny_benchmarks.TinyCorpusAggregator object at 0x7717f0cdc5c0>, 'corpus_level_fn': <lighteval.tasks.tasks.tiny_benchmarks.TinyCorpusAggregator object at 0x7717f0cdc620>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'validation', 'test')",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "random_sampling",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "tiny:mmlu",
        "config": {
          "name": "tiny:mmlu",
          "prompt_function": "<function mmlu_prompt at 0x7717f34119e0>",
          "hf_repo": "tinyBenchmarks/tinyMMLU",
          "hf_subset": "all",
          "metrics": "({'metric_name': ['irt', 'pirt', 'gpirt'], 'higher_is_better': {'irt': True, 'pirt': True, 'gpirt': True}, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.tasks.tasks.tiny_benchmarks.TinyCorpusAggregator object at 0x7717f0cdc9b0>, 'corpus_level_fn': <lighteval.tasks.tasks.tiny_benchmarks.TinyCorpusAggregator object at 0x7717f0cdca10>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('validation', 'dev', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "random_sampling",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "tiny:truthfulqa",
        "config": {
          "name": "tiny:truthfulqa",
          "prompt_function": "<function truthful_qa_multiple_choice_prompt at 0x7717f3455c60>",
          "hf_repo": "tinyBenchmarks/tinyTruthfulQA",
          "hf_subset": "multiple_choice",
          "metrics": "({'metric_name': ['irt', 'pirt', 'gpirt'], 'higher_is_better': {'irt': True, 'pirt': True, 'gpirt': True}, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.tasks.tasks.tiny_benchmarks.TinyCorpusAggregator object at 0x7717f0cdcda0>, 'corpus_level_fn': <lighteval.tasks.tasks.tiny_benchmarks.TinyCorpusAggregator object at 0x7717f0cdce00>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('validation',)",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "random_sampling",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "tiny:gsm8k",
        "config": {
          "name": "tiny:gsm8k",
          "prompt_function": "<function gsm8k_prompt at 0x7717f34556c0>",
          "hf_repo": "tinyBenchmarks/tinyGSM8k",
          "hf_subset": "main",
          "metrics": "({'metric_name': ['irt', 'pirt', 'gpirt'], 'higher_is_better': {'irt': True, 'pirt': True, 'gpirt': True}, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.tasks.tasks.tiny_benchmarks.TinyCorpusAggregator object at 0x7717f0cdd1c0>, 'corpus_level_fn': <lighteval.tasks.tasks.tiny_benchmarks.TinyCorpusAggregator object at 0x7717f0cdd220>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "random_sampling",
          "generation_size": "256",
          "generation_grammar": "None",
          "stop_sequence": "['Question:', 'Question']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.mmlu_pro",
    "docstring": {
      "name": "MMLU Pro",
      "dataset": [
        "TIGER-Lab/MMLU-Pro"
      ],
      "abstract": "MMLU-Pro dataset is a more robust and challenging massive multi-task\nunderstanding dataset tailored to more rigorously benchmark large language\nmodels' capabilities. This dataset contains 12K complex questions across various\ndisciplines.",
      "languages": [
        "english"
      ],
      "tags": [
        "general-knowledge",
        "knowledge",
        "multiple-choice"
      ],
      "paper": "https://arxiv.org/abs/2406.01574",
      "starred": "true"
    },
    "tasks": [
      {
        "name": "mmlu_pro",
        "config": {
          "name": "mmlu_pro",
          "prompt_function": "<function mmlu_pro_prompt_function at 0x7717f3455f80>",
          "hf_repo": "TIGER-Lab/MMLU-Pro",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'extractive_match', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.dynamic_metrics.MultilingualExtractiveMatchMetric object at 0x7717f0cdd6d0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f3456340>]",
          "scorer": "<function choice.<locals>.score at 0x7717f3456480>",
          "sample_fields": "<function record_to_sample at 0x7717f34562a0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "3373e0b32277875b8db2aa555a333b78a08477ea",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'validation', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "validation",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.piqa",
    "docstring": {
      "name": "Piqa",
      "dataset": [
        "ybisk/piqa"
      ],
      "abstract": "PIQA is a benchmark for testing physical commonsense reasoning. It contains\nquestions requiring this kind of physical commonsense reasoning.",
      "languages": [
        "english"
      ],
      "tags": [
        "commonsense",
        "multiple-choice",
        "qa"
      ],
      "paper": "https://arxiv.org/abs/1911.11641"
    },
    "tasks": [
      {
        "name": "piqa",
        "config": {
          "name": "piqa",
          "prompt_function": "<function piqa_prompt at 0x7717f3456520>",
          "hf_repo": "ybisk/piqa",
          "hf_subset": "plain_text",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cddb80>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.hle",
    "docstring": {
      "name": "Humanity's Last Exam",
      "dataset": [
        "cais/hle"
      ],
      "abstract": "Humanity's Last Exam (HLE) is a global collaborative effort, with questions from\nnearly 1,000 subject expert contributors affiliated with over 500 institutions\nacross 50 countries - comprised mostly of professors, researchers, and graduate\ndegree holders.",
      "languages": [
        "english"
      ],
      "tags": [
        "qa",
        "reasoning",
        "general-knowledge"
      ],
      "paper": "https://arxiv.org/abs/2501.14249",
      "starred": "true"
    },
    "tasks": [
      {
        "name": "hle",
        "config": {
          "name": "hle",
          "prompt_function": "<function hle_text_only at 0x7717f3456a20>",
          "hf_repo": "cais/hle",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cde0c0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': ['accuracy', 'confidence_half_width', 'calibration_error'], 'higher_is_better': {'accuracy': True, 'confidence_half_width': True, 'calibration_error': True}, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.tasks.tasks.hle.JudgeLLMHLE object at 0x7717f0cde120>, 'corpus_level_fn': <lighteval.tasks.tasks.hle.JudgeLLMHLE object at 0x7717f0cde060>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "[<function system_message.<locals>.solve at 0x7717f3456b60>, <function generate.<locals>.solve at 0x7717f3456ca0>]",
          "scorer": "<function _model_graded_qa_single.<locals>.score at 0x7717f3456de0>",
          "sample_fields": "<function record_to_sample at 0x7717f3456ac0>",
          "sample_to_fewshot": "None",
          "filter": "<function <lambda> at 0x7717f3456f20>",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "8192",
          "generation_grammar": "None",
          "stop_sequence": "[]",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.med_dialog",
    "docstring": {
      "name": "Med Dialog",
      "dataset": [
        "lighteval/med_dialog"
      ],
      "abstract": "A collection of medical dialogue datasets.",
      "languages": [
        "english"
      ],
      "tags": [
        "dialog",
        "health",
        "medical"
      ]
    },
    "tasks": [
      {
        "name": "med_dialog:healthcaremagic",
        "config": {
          "name": "med_dialog:healthcaremagic",
          "prompt_function": "<function med_dialog_prompt at 0x7717f3456fc0>",
          "hf_repo": "lighteval/med_dialog",
          "hf_subset": "healthcaremagic",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cde5a0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "128",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "med_dialog:icliniq",
        "config": {
          "name": "med_dialog:icliniq",
          "prompt_function": "<function med_dialog_prompt at 0x7717f3456fc0>",
          "hf_repo": "lighteval/med_dialog",
          "hf_subset": "icliniq",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cde960>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "128",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.bbq",
    "docstring": {
      "name": "Bbq",
      "dataset": [
        "lighteval/bbq_helm"
      ],
      "abstract": "The Bias Benchmark for Question Answering (BBQ) for measuring social bias in\nquestion answering in ambiguous and unambigous context .",
      "languages": [
        "english"
      ],
      "tags": [
        "bias",
        "multiple-choice",
        "qa"
      ],
      "paper": "https://arxiv.org/abs/2110.08193"
    },
    "tasks": [
      {
        "name": "bbq",
        "config": {
          "name": "bbq",
          "prompt_function": "<function bbq_prompt at 0x7717f3457060>",
          "hf_repo": "lighteval/bbq_helm",
          "hf_subset": "all",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cdeea0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f34571a0>]",
          "scorer": "<function choice.<locals>.score at 0x7717f34572e0>",
          "sample_fields": "<function record_to_sample at 0x7717f3457100>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bbq:Age",
        "config": {
          "name": "bbq:Age",
          "prompt_function": "<function bbq_prompt at 0x7717f3457060>",
          "hf_repo": "lighteval/bbq_helm",
          "hf_subset": "Age",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cdf200>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f3457420>]",
          "scorer": "<function choice.<locals>.score at 0x7717f34574c0>",
          "sample_fields": "<function record_to_sample at 0x7717f3457100>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bbq:Disability_status",
        "config": {
          "name": "bbq:Disability_status",
          "prompt_function": "<function bbq_prompt at 0x7717f3457060>",
          "hf_repo": "lighteval/bbq_helm",
          "hf_subset": "Disability_status",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cdf560>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f3457600>]",
          "scorer": "<function choice.<locals>.score at 0x7717f34576a0>",
          "sample_fields": "<function record_to_sample at 0x7717f3457100>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bbq:Gender_identity",
        "config": {
          "name": "bbq:Gender_identity",
          "prompt_function": "<function bbq_prompt at 0x7717f3457060>",
          "hf_repo": "lighteval/bbq_helm",
          "hf_subset": "Gender_identity",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cdf8c0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f34577e0>]",
          "scorer": "<function choice.<locals>.score at 0x7717f3457880>",
          "sample_fields": "<function record_to_sample at 0x7717f3457100>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bbq:Nationality",
        "config": {
          "name": "bbq:Nationality",
          "prompt_function": "<function bbq_prompt at 0x7717f3457060>",
          "hf_repo": "lighteval/bbq_helm",
          "hf_subset": "Nationality",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cdfc20>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f34579c0>]",
          "scorer": "<function choice.<locals>.score at 0x7717f3457a60>",
          "sample_fields": "<function record_to_sample at 0x7717f3457100>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bbq:Physical_appearance",
        "config": {
          "name": "bbq:Physical_appearance",
          "prompt_function": "<function bbq_prompt at 0x7717f3457060>",
          "hf_repo": "lighteval/bbq_helm",
          "hf_subset": "Physical_appearance",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cdff80>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f3457ba0>]",
          "scorer": "<function choice.<locals>.score at 0x7717f3457c40>",
          "sample_fields": "<function record_to_sample at 0x7717f3457100>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bbq:Race_ethnicity",
        "config": {
          "name": "bbq:Race_ethnicity",
          "prompt_function": "<function bbq_prompt at 0x7717f3457060>",
          "hf_repo": "lighteval/bbq_helm",
          "hf_subset": "Race_ethnicity",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cf0320>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f3457d80>]",
          "scorer": "<function choice.<locals>.score at 0x7717f3457e20>",
          "sample_fields": "<function record_to_sample at 0x7717f3457100>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bbq:Race_x_SES",
        "config": {
          "name": "bbq:Race_x_SES",
          "prompt_function": "<function bbq_prompt at 0x7717f3457060>",
          "hf_repo": "lighteval/bbq_helm",
          "hf_subset": "Race_x_SES",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cf0680>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f3457f60>]",
          "scorer": "<function choice.<locals>.score at 0x7717f34b0040>",
          "sample_fields": "<function record_to_sample at 0x7717f3457100>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bbq:Race_x_gender",
        "config": {
          "name": "bbq:Race_x_gender",
          "prompt_function": "<function bbq_prompt at 0x7717f3457060>",
          "hf_repo": "lighteval/bbq_helm",
          "hf_subset": "Race_x_gender",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cf09e0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f34b0180>]",
          "scorer": "<function choice.<locals>.score at 0x7717f34b0220>",
          "sample_fields": "<function record_to_sample at 0x7717f3457100>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bbq:Religion",
        "config": {
          "name": "bbq:Religion",
          "prompt_function": "<function bbq_prompt at 0x7717f3457060>",
          "hf_repo": "lighteval/bbq_helm",
          "hf_subset": "Religion",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cf0d10>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f34b0360>]",
          "scorer": "<function choice.<locals>.score at 0x7717f34b0400>",
          "sample_fields": "<function record_to_sample at 0x7717f3457100>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bbq:SES",
        "config": {
          "name": "bbq:SES",
          "prompt_function": "<function bbq_prompt at 0x7717f3457060>",
          "hf_repo": "lighteval/bbq_helm",
          "hf_subset": "SES",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cf1070>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f34b0540>]",
          "scorer": "<function choice.<locals>.score at 0x7717f34b05e0>",
          "sample_fields": "<function record_to_sample at 0x7717f3457100>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bbq:Sexual_orientation",
        "config": {
          "name": "bbq:Sexual_orientation",
          "prompt_function": "<function bbq_prompt at 0x7717f3457060>",
          "hf_repo": "lighteval/bbq_helm",
          "hf_subset": "Sexual_orientation",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cf13d0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f34b0720>]",
          "scorer": "<function choice.<locals>.score at 0x7717f34b07c0>",
          "sample_fields": "<function record_to_sample at 0x7717f3457100>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.truthfulqa",
    "docstring": {
      "name": "Truthfulqa",
      "dataset": [
        "EleutherAI/truthful_qa_mc"
      ],
      "abstract": "TruthfulQA: Measuring How Models Mimic Human Falsehoods",
      "languages": [
        "english"
      ],
      "tags": [
        "factuality",
        "qa"
      ],
      "paper": "https://arxiv.org/abs/2109.07958"
    },
    "tasks": [
      {
        "name": "truthfulqa:gen",
        "config": {
          "name": "truthfulqa:gen",
          "prompt_function": "<function truthful_qa_generative_prompt at 0x7717f3455d00>",
          "hf_repo": "truthfulqa/truthful_qa",
          "hf_subset": "generation",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cf1850>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('validation',)",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "200",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "truthfulqa:mc",
        "config": {
          "name": "truthfulqa:mc",
          "prompt_function": "<function truthful_qa_multiple_choice_prompt at 0x7717f3455c60>",
          "hf_repo": "truthfulqa/truthful_qa",
          "hf_subset": "multiple_choice",
          "metrics": "({'metric_name': ['truthfulqa_mc1', 'truthfulqa_mc2'], 'higher_is_better': {'truthfulqa_mc1': True, 'truthfulqa_mc2': True}, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.harness_compatibility.truthful_qa.TruthfulqaMCMetrics object at 0x7717f0cf1c70>, 'corpus_level_fn': {'truthfulqa_mc1': <function mean at 0x771aa6d5b3b0>, 'truthfulqa_mc2': <function mean at 0x771aa6d5b3b0>}, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('validation',)",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.wikitext",
    "docstring": {
      "name": "Wikitext",
      "dataset": [
        "EleutherAI/wikitext_document_level"
      ],
      "abstract": "The WikiText language modeling dataset is a collection of over 100 million\ntokens extracted from the set of verified Good and Featured articles on\nWikipedia. The dataset is available under the Creative Commons\nAttribution-ShareAlike License.",
      "languages": [
        "english"
      ],
      "tags": [
        "language-modeling"
      ],
      "paper": "https://arxiv.org/abs/1609.07843"
    },
    "tasks": [
      {
        "name": "wikitext:103:document_level",
        "config": {
          "name": "wikitext:103:document_level",
          "prompt_function": "<function wikitext_prompt at 0x7717f34b0900>",
          "hf_repo": "EleutherAI/wikitext_document_level",
          "hf_subset": "wikitext-103-raw-v1",
          "metrics": "({'metric_name': 'word_perplexity', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0cf2180>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0cf21e0>, 'batched_compute': False}, {'metric_name': 'byte_perplexity', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0cf2240>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0cf22a0>, 'batched_compute': False}, {'metric_name': 'bits_per_byte', 'higher_is_better': False, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0cf2300>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0cf2360>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.bold",
    "docstring": {
      "name": "Bold",
      "dataset": [
        "lighteval/bold_helm"
      ],
      "abstract": "The Bias in Open-Ended Language Generation Dataset (BOLD) for measuring biases\nand toxicity in open-ended language generation.",
      "languages": [
        "english"
      ],
      "tags": [
        "bias",
        "generation"
      ],
      "paper": "https://dl.acm.org/doi/10.1145/3442188.3445924"
    },
    "tasks": [
      {
        "name": "bold",
        "config": {
          "name": "bold",
          "prompt_function": "<function bold_prompt at 0x7717f34b0860>",
          "hf_repo": "lighteval/bold_helm",
          "hf_subset": "all",
          "metrics": "({'metric_name': 'ppl', 'higher_is_better': True, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0cf2780>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0cf27e0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function generate.<locals>.solve at 0x7717f34b0a40>]",
          "scorer": "<function exact.<locals>.score at 0x7717f34b0b80>",
          "sample_fields": "<function record_to_sample at 0x7717f34b09a0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "100",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bold:gender",
        "config": {
          "name": "bold:gender",
          "prompt_function": "<function bold_prompt at 0x7717f34b0860>",
          "hf_repo": "lighteval/bold_helm",
          "hf_subset": "gender",
          "metrics": "({'metric_name': 'ppl', 'higher_is_better': True, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0cf2b10>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0cf2b70>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function generate.<locals>.solve at 0x7717f34b0cc0>]",
          "scorer": "<function exact.<locals>.score at 0x7717f34b0d60>",
          "sample_fields": "<function record_to_sample at 0x7717f34b09a0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "100",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bold:political_ideology",
        "config": {
          "name": "bold:political_ideology",
          "prompt_function": "<function bold_prompt at 0x7717f34b0860>",
          "hf_repo": "lighteval/bold_helm",
          "hf_subset": "political_ideology",
          "metrics": "({'metric_name': 'ppl', 'higher_is_better': True, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0cf2ea0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0cf2f00>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function generate.<locals>.solve at 0x7717f34b0ea0>]",
          "scorer": "<function exact.<locals>.score at 0x7717f34b0f40>",
          "sample_fields": "<function record_to_sample at 0x7717f34b09a0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "100",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bold:profession",
        "config": {
          "name": "bold:profession",
          "prompt_function": "<function bold_prompt at 0x7717f34b0860>",
          "hf_repo": "lighteval/bold_helm",
          "hf_subset": "profession",
          "metrics": "({'metric_name': 'ppl', 'higher_is_better': True, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0cf3230>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0cf3290>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function generate.<locals>.solve at 0x7717f34b1080>]",
          "scorer": "<function exact.<locals>.score at 0x7717f34b1120>",
          "sample_fields": "<function record_to_sample at 0x7717f34b09a0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "100",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bold:race",
        "config": {
          "name": "bold:race",
          "prompt_function": "<function bold_prompt at 0x7717f34b0860>",
          "hf_repo": "lighteval/bold_helm",
          "hf_subset": "race",
          "metrics": "({'metric_name': 'ppl', 'higher_is_better': True, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0cf35c0>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0cf3620>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function generate.<locals>.solve at 0x7717f34b1260>]",
          "scorer": "<function exact.<locals>.score at 0x7717f34b1300>",
          "sample_fields": "<function record_to_sample at 0x7717f34b09a0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "100",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bold:religious_ideology",
        "config": {
          "name": "bold:religious_ideology",
          "prompt_function": "<function bold_prompt at 0x7717f34b0860>",
          "hf_repo": "lighteval/bold_helm",
          "hf_subset": "religious_ideology",
          "metrics": "({'metric_name': 'ppl', 'higher_is_better': True, 'category': <SamplingMethod.PERPLEXITY: 'PERPLEXITY'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.PerplexityPreparator object at 0x7717f0cf3950>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0cf39b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function generate.<locals>.solve at 0x7717f34b1440>]",
          "scorer": "<function exact.<locals>.score at 0x7717f34b14e0>",
          "sample_fields": "<function record_to_sample at 0x7717f34b09a0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "100",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.lambada",
    "docstring": {
      "name": "Lambada",
      "dataset": [
        "cimec/lambada"
      ],
      "abstract": "LAMBADA is a benchmark for testing language models\u2019 ability to understand broad\nnarrative context. Each passage requires predicting its final word\u2014easy for\nhumans given the full passage but impossible from just the last sentence.\nSuccess demands long-range discourse comprehension.",
      "languages": [
        "english"
      ],
      "tags": [
        "language-modeling"
      ],
      "paper": "https://arxiv.org/abs/1606.06031"
    },
    "tasks": [
      {
        "name": "lambada:standard",
        "config": {
          "name": "lambada:standard",
          "prompt_function": "<function lambada_prompt at 0x7717f34b1580>",
          "hf_repo": "cimec/lambada",
          "hf_subset": "plain_text",
          "metrics": "({'metric_name': 'ppl', 'higher_is_better': False, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.TargetPerplexityPreparator object at 0x7717f0cf3e00>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0cf3e60>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "10",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lambada:standard_cloze",
        "config": {
          "name": "lambada:standard_cloze",
          "prompt_function": "<function lambada_cloze_prompt at 0x7717f34b1620>",
          "hf_repo": "cimec/lambada",
          "hf_subset": "plain_text",
          "metrics": "({'metric_name': 'ppl', 'higher_is_better': False, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.sample_preparator.TargetPerplexityPreparator object at 0x7717f0cf8260>, 'corpus_level_fn': <lighteval.metrics.metrics_corpus.CorpusLevelPerplexityMetric object at 0x7717f0cf82c0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "10",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.numeracy",
    "docstring": {
      "name": "Numeracy",
      "dataset": [
        "lighteval/numeracy"
      ],
      "abstract": "Numeracy is a benchmark for evaluating the ability of language models to reason about mathematics.",
      "languages": [
        "english"
      ],
      "tags": [
        "math",
        "reasoning"
      ]
    },
    "tasks": [
      {
        "name": "numeracy:linear_example",
        "config": {
          "name": "numeracy:linear_example",
          "prompt_function": "<function numeracy_prompt at 0x7717f34b16c0>",
          "hf_repo": "lighteval/numeracy",
          "hf_subset": "linear_example",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cf8770>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "20",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "numeracy:linear_standard",
        "config": {
          "name": "numeracy:linear_standard",
          "prompt_function": "<function numeracy_prompt at 0x7717f34b16c0>",
          "hf_repo": "lighteval/numeracy",
          "hf_subset": "linear_standard",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cf8b60>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "20",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "numeracy:parabola_example",
        "config": {
          "name": "numeracy:parabola_example",
          "prompt_function": "<function numeracy_prompt at 0x7717f34b16c0>",
          "hf_repo": "lighteval/numeracy",
          "hf_subset": "parabola_example",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cf8f50>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "20",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "numeracy:parabola_standard",
        "config": {
          "name": "numeracy:parabola_standard",
          "prompt_function": "<function numeracy_prompt at 0x7717f34b16c0>",
          "hf_repo": "lighteval/numeracy",
          "hf_subset": "parabola_standard",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cf9340>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "20",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "numeracy:paraboloid_example",
        "config": {
          "name": "numeracy:paraboloid_example",
          "prompt_function": "<function numeracy_prompt at 0x7717f34b16c0>",
          "hf_repo": "lighteval/numeracy",
          "hf_subset": "paraboloid_example",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cf9730>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "20",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "numeracy:paraboloid_standard",
        "config": {
          "name": "numeracy:paraboloid_standard",
          "prompt_function": "<function numeracy_prompt at 0x7717f34b16c0>",
          "hf_repo": "lighteval/numeracy",
          "hf_subset": "paraboloid_standard",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cf9af0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "20",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "numeracy:plane_example",
        "config": {
          "name": "numeracy:plane_example",
          "prompt_function": "<function numeracy_prompt at 0x7717f34b16c0>",
          "hf_repo": "lighteval/numeracy",
          "hf_subset": "plane_example",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cf9ee0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "20",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "numeracy:plane_standard",
        "config": {
          "name": "numeracy:plane_standard",
          "prompt_function": "<function numeracy_prompt at 0x7717f34b16c0>",
          "hf_repo": "lighteval/numeracy",
          "hf_subset": "plane_standard",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cfa2d0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "20",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.gsm8k",
    "docstring": {
      "name": "Gsm8K",
      "dataset": [
        "openai/gsm8k"
      ],
      "abstract": "GSM8K is a dataset of 8,000+ high-quality, single-step arithmetic word problems.",
      "languages": [
        "english"
      ],
      "tags": [
        "math",
        "reasoning"
      ],
      "paper": "https://arxiv.org/abs/2110.14168"
    },
    "tasks": [
      {
        "name": "gsm8k",
        "config": {
          "name": "gsm8k",
          "prompt_function": "<function gsm8k_prompt at 0x7717f34556c0>",
          "hf_repo": "openai/gsm8k",
          "hf_subset": "main",
          "metrics": "({'metric_name': 'extractive_match', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.dynamic_metrics.MultilingualExtractiveMatchMetric object at 0x7717f0cfa7e0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function prompt_template.<locals>.solve at 0x7717f3455760>, <function generate.<locals>.solve at 0x7717f34558a0>]",
          "scorer": "<function math_scorer.<locals>.score at 0x7717f34559e0>",
          "sample_fields": "<function record_to_sample at 0x7717f3455580>",
          "sample_to_fewshot": "<function sample_to_fewshot at 0x7717f3455620>",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "random_sampling_from_train",
          "generation_size": "256",
          "generation_grammar": "None",
          "stop_sequence": "['Question:']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.hellaswag",
    "docstring": {
      "name": "Hellaswag",
      "dataset": [
        "Rowan/hellaswag"
      ],
      "abstract": "HellaSwag is a commonsense inference benchmark designed to challenge language\nmodels with adversarially filtered multiple-choice questions.",
      "languages": [
        "english"
      ],
      "tags": [
        "multiple-choice",
        "narrative",
        "reasoning"
      ],
      "paper": "https://arxiv.org/abs/1905.07830"
    },
    "tasks": [
      {
        "name": "hellaswag",
        "config": {
          "name": "hellaswag",
          "prompt_function": "<function hellaswag_prompt at 0x7717f3455bc0>",
          "hf_repo": "Rowan/hellaswag",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0cfac30>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.gsm_plus",
    "docstring": {
      "name": "Gsm Plus",
      "dataset": [
        "qintongli/GSM-Plus"
      ],
      "abstract": "GSM-Plus is an adversarial extension of GSM8K that tests the robustness of LLMs'\nmathematical reasoning by introducing varied perturbations to grade-school math\nproblems.",
      "languages": [
        "english"
      ],
      "tags": [
        "math",
        "reasoning"
      ],
      "paper": "https://arxiv.org/abs/2402.19255",
      "starred": "true"
    },
    "tasks": [
      {
        "name": "gsm_plus",
        "config": {
          "name": "gsm_plus",
          "prompt_function": "<function gsm_plus_prompt at 0x7717f34b18a0>",
          "hf_repo": "qintongli/GSM-Plus",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'extractive_match', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.dynamic_metrics.MultilingualExtractiveMatchMetric object at 0x7717f0cfb1a0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function prompt_template.<locals>.solve at 0x7717f34b1940>, <function generate.<locals>.solve at 0x7717f34b1a80>]",
          "scorer": "<function math_scorer.<locals>.score at 0x7717f34b1bc0>",
          "sample_fields": "<function record_to_sample at 0x7717f34b1760>",
          "sample_to_fewshot": "<function sample_to_fewshot at 0x7717f34b1800>",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test', 'testmini')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.legal_summarization",
    "docstring": {
      "name": "Legal Summarization",
      "dataset": [
        "lighteval/legal_summarization"
      ],
      "abstract": "LegalSummarization is a dataset for legal summarization.",
      "languages": [
        "english"
      ],
      "tags": [
        "legal",
        "summarization"
      ],
      "paper": "https://arxiv.org/abs/2210.13448\nhttps://arxiv.org/abs/2210.13448"
    },
    "tasks": [
      {
        "name": "legal_summarization:billsum",
        "config": {
          "name": "legal_summarization:billsum",
          "prompt_function": "<function legal_summarization_prompt at 0x7717f34b1c60>",
          "hf_repo": "lighteval/legal_summarization",
          "hf_subset": "BillSum",
          "metrics": "({'metric_name': 'rouge1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ROUGE object at 0x7717f0cfb6b0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'rouge2', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ROUGE object at 0x7717f0cfb710>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'rougeL', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ROUGE object at 0x7717f0cfb770>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'summac', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.Faithfulness object at 0x7717f0cfb7d0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': ['summarization_coverage', 'summarization_density', 'summarization_compression'], 'higher_is_better': {'summarization_coverage': True, 'summarization_density': True, 'summarization_compression': True}, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.Extractiveness object at 0x7717f0cfb830>, 'corpus_level_fn': {'summarization_coverage': <function mean at 0x771aa6d5b3b0>, 'summarization_density': <function mean at 0x771aa6d5b3b0>, 'summarization_compression': <function mean at 0x771aa6d5b3b0>}, 'batched_compute': False}, {'metric_name': ['BERTScore-P', 'BERTScore-R', 'BERTScore-F'], 'higher_is_better': {'BERTScore-P': True, 'BERTScore-R': True, 'BERTScore-F': True}, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.BertScore object at 0x7717f0cfb890>, 'corpus_level_fn': {'BERTScore-P': <function mean at 0x771aa6d5b3b0>, 'BERTScore-R': <function mean at 0x771aa6d5b3b0>, 'BERTScore-F': <function mean at 0x771aa6d5b3b0>}, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1024",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "legal_summarization:eurlexsum",
        "config": {
          "name": "legal_summarization:eurlexsum",
          "prompt_function": "<function legal_summarization_prompt at 0x7717f34b1c60>",
          "hf_repo": "lighteval/legal_summarization",
          "hf_subset": "EurLexSum",
          "metrics": "({'metric_name': 'rouge1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ROUGE object at 0x7717f0cfbb60>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'rouge2', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ROUGE object at 0x7717f0cfbbc0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'rougeL', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ROUGE object at 0x7717f0cfbc20>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'summac', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.Faithfulness object at 0x7717f0cfbc80>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': ['summarization_coverage', 'summarization_density', 'summarization_compression'], 'higher_is_better': {'summarization_coverage': True, 'summarization_density': True, 'summarization_compression': True}, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.Extractiveness object at 0x7717f0cfbce0>, 'corpus_level_fn': {'summarization_coverage': <function mean at 0x771aa6d5b3b0>, 'summarization_density': <function mean at 0x771aa6d5b3b0>, 'summarization_compression': <function mean at 0x771aa6d5b3b0>}, 'batched_compute': False}, {'metric_name': ['BERTScore-P', 'BERTScore-R', 'BERTScore-F'], 'higher_is_better': {'BERTScore-P': True, 'BERTScore-R': True, 'BERTScore-F': True}, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.BertScore object at 0x7717f0cfbd40>, 'corpus_level_fn': {'BERTScore-P': <function mean at 0x771aa6d5b3b0>, 'BERTScore-R': <function mean at 0x771aa6d5b3b0>, 'BERTScore-F': <function mean at 0x771aa6d5b3b0>}, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "2048",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "legal_summarization:multilexsum",
        "config": {
          "name": "legal_summarization:multilexsum",
          "prompt_function": "<function multilexsum_prompt at 0x7717f34b1da0>",
          "hf_repo": "lighteval/legal_summarization",
          "hf_subset": "MultiLexSum",
          "metrics": "({'metric_name': 'rouge1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ROUGE object at 0x7717f0cfbfe0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'rouge2', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ROUGE object at 0x7717f0b00080>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'rougeL', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ROUGE object at 0x7717f0b000e0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'summac', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.Faithfulness object at 0x7717f0b00140>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': ['summarization_coverage', 'summarization_density', 'summarization_compression'], 'higher_is_better': {'summarization_coverage': True, 'summarization_density': True, 'summarization_compression': True}, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.Extractiveness object at 0x7717f0b001a0>, 'corpus_level_fn': {'summarization_coverage': <function mean at 0x771aa6d5b3b0>, 'summarization_density': <function mean at 0x771aa6d5b3b0>, 'summarization_compression': <function mean at 0x771aa6d5b3b0>}, 'batched_compute': False}, {'metric_name': ['BERTScore-P', 'BERTScore-R', 'BERTScore-F'], 'higher_is_better': {'BERTScore-P': True, 'BERTScore-R': True, 'BERTScore-F': True}, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.BertScore object at 0x7717f0b00200>, 'corpus_level_fn': {'BERTScore-P': <function mean at 0x771aa6d5b3b0>, 'BERTScore-R': <function mean at 0x771aa6d5b3b0>, 'BERTScore-F': <function mean at 0x771aa6d5b3b0>}, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "256",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.qasper",
    "docstring": {
      "name": "Qasper",
      "dataset": [
        "allenai/qasper"
      ],
      "abstract": "QASPER is a dataset for question answering on scientific research papers. It\nconsists of 5,049 questions over 1,585 Natural Language Processing papers. Each\nquestion is written by an NLP practitioner who read only the title and abstract\nof the corresponding paper, and the question seeks information present in the\nfull text. The questions are then answered by a separate set of NLP\npractitioners who also provide supporting evidence to answers.",
      "languages": [
        "english"
      ],
      "tags": [
        "qa",
        "scientific"
      ],
      "paper": "https://arxiv.org/abs/2105.03011"
    },
    "tasks": [
      {
        "name": "qasper",
        "config": {
          "name": "qasper",
          "prompt_function": "<function qasper_prompt at 0x7717f34b1e40>",
          "hf_repo": "allenai/qasper",
          "hf_subset": "qasper",
          "metrics": "({'metric_name': 'f1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.F1_score object at 0x7717f0b005f0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'validation')",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "20",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.quac",
    "docstring": {
      "name": "Quac",
      "dataset": [
        "lighteval/quac_helm"
      ],
      "abstract": "The QuAC benchmark for question answering in the context of dialogues.",
      "languages": [
        "english"
      ],
      "tags": [
        "dialog",
        "qa"
      ],
      "paper": "https://aclanthology.org/D18-1241/"
    },
    "tasks": [
      {
        "name": "quac",
        "config": {
          "name": "quac",
          "prompt_function": "<function quac_prompt at 0x7717f34b1ee0>",
          "hf_repo": "lighteval/quac_helm",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b00b60>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'validation')",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "100",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.raft",
    "docstring": {
      "name": "Raft",
      "dataset": [
        "ought/raft"
      ],
      "abstract": "The Real-world annotated few-shot (RAFT) meta-benchmark of 11 real-world text\nclassification tasks.",
      "languages": [
        "english"
      ],
      "tags": [
        "classification",
        "reasoning"
      ],
      "paper": "https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/ca46c1b9512a7a8315fa3c5a946e8265-Abstract-round2.html"
    },
    "tasks": [
      {
        "name": "raft:ade_corpus_v2",
        "config": {
          "name": "raft:ade_corpus_v2",
          "prompt_function": "<function raft_ade_corpus_v2_prompt at 0x7717f34b2020>",
          "hf_repo": "ought/raft",
          "hf_subset": "ade_corpus_v2",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b01070>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "30",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "raft:banking_77",
        "config": {
          "name": "raft:banking_77",
          "prompt_function": "<function raft_banking_77_prompt at 0x7717f34b20c0>",
          "hf_repo": "ought/raft",
          "hf_subset": "banking_77",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b01460>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "30",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "raft:neurips_impact_statement_risks",
        "config": {
          "name": "raft:neurips_impact_statement_risks",
          "prompt_function": "<function raft_neurips_impact_statement_risks_prompt at 0x7717f34b2160>",
          "hf_repo": "ought/raft",
          "hf_subset": "neurips_impact_statement_risks",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b01850>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "30",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "raft:one_stop_english",
        "config": {
          "name": "raft:one_stop_english",
          "prompt_function": "<function raft_one_stop_english_prompt at 0x7717f34b2200>",
          "hf_repo": "ought/raft",
          "hf_subset": "one_stop_english",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b01c40>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "30",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "raft:overruling",
        "config": {
          "name": "raft:overruling",
          "prompt_function": "<function raft_overruling_prompt at 0x7717f34b22a0>",
          "hf_repo": "ought/raft",
          "hf_subset": "overruling",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b02030>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "30",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "raft:semiconductor_org_types",
        "config": {
          "name": "raft:semiconductor_org_types",
          "prompt_function": "<function raft_semiconductor_org_types_prompt at 0x7717f34b2340>",
          "hf_repo": "ought/raft",
          "hf_subset": "semiconductor_org_types",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b02420>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "30",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "raft:systematic_review_inclusion",
        "config": {
          "name": "raft:systematic_review_inclusion",
          "prompt_function": "<function raft_systematic_review_inclusion_prompt at 0x7717f34b23e0>",
          "hf_repo": "ought/raft",
          "hf_subset": "systematic_review_inclusion",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b02810>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "30",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "raft:tai_safety_research",
        "config": {
          "name": "raft:tai_safety_research",
          "prompt_function": "<function raft_tai_safety_research_prompt at 0x7717f34b2480>",
          "hf_repo": "ought/raft",
          "hf_subset": "tai_safety_research",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b02c00>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "30",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "raft:terms_of_service",
        "config": {
          "name": "raft:terms_of_service",
          "prompt_function": "<function raft_terms_of_service_prompt at 0x7717f34b2520>",
          "hf_repo": "ought/raft",
          "hf_subset": "terms_of_service",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b02fc0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "30",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "raft:tweet_eval_hate",
        "config": {
          "name": "raft:tweet_eval_hate",
          "prompt_function": "<function raft_tweet_eval_hate_prompt at 0x7717f34b25c0>",
          "hf_repo": "ought/raft",
          "hf_subset": "tweet_eval_hate",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b033b0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "30",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "raft:twitter_complaints",
        "config": {
          "name": "raft:twitter_complaints",
          "prompt_function": "<function raft_twitter_complaints_prompt at 0x7717f34b2660>",
          "hf_repo": "ought/raft",
          "hf_subset": "twitter_complaints",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b037a0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "30",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.custom_task_classification_grammar_task",
    "docstring": {
      "name": "Emotion Classification",
      "dataset": [
        "dair-ai/emotion"
      ],
      "abstract": "This task performs emotion classification classifying text into one of six\nemotion categories: sadness, joy, love, anger, fear, surprise.",
      "languages": [
        "english"
      ],
      "tags": [
        "emotion",
        "classification",
        "multiple-choice"
      ]
    },
    "tasks": [
      {
        "name": "emotion_classification",
        "config": {
          "name": "emotion_classification",
          "prompt_function": "<function prompt_emotion_classification at 0x7717f34b2ac0>",
          "hf_repo": "emotion",
          "hf_subset": "None",
          "metrics": "({'metric_name': ['exact_match', 'unknown_prediction', 'total_samples'], 'higher_is_better': {'exact_match': True, 'unknown_prediction': False, 'total_samples': True}, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <function emotion_classification_metric at 0x7717f34b2980>, 'corpus_level_fn': {'exact_match': <function mean at 0x771aa6d5b3b0>, 'unknown_prediction': <function mean at 0x771aa6d5b3b0>, 'total_samples': <function sum at 0x771aa6d591f0>}, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'validation', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "64",
          "generation_grammar": "{'type': 'json', 'value': {'type': 'object', 'properties': {'classification': {'type': 'string', 'description': 'Emotion classification from the provided list', 'enum': ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']}}, 'required': ['classification'], 'additionalProperties': False}}",
          "stop_sequence": "['\\n\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.covid_dialogue",
    "docstring": {
      "name": "Covid Dialogue",
      "dataset": [
        "lighteval/covid_dialogue"
      ],
      "abstract": "The COVID-19 Dialogue dataset is a collection of 500+ dialogues between\ndoctors and patients during the COVID-19 pandemic.",
      "languages": [
        "english"
      ],
      "tags": [
        "dialog",
        "medical"
      ],
      "paper": "https://arxiv.org/abs/2004.06561"
    },
    "tasks": [
      {
        "name": "covid_dialogue",
        "config": {
          "name": "covid_dialogue",
          "prompt_function": "<function covid_dialogue_prompt at 0x7717f34b2c00>",
          "hf_repo": "lighteval/covid_dialogue",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b04170>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function system_message.<locals>.solve at 0x7717f34b2d40>, <function generate.<locals>.solve at 0x7717f34b2e80>]",
          "scorer": "<function _model_graded_qa_single.<locals>.score at 0x7717f34b2fc0>",
          "sample_fields": "<function record_to_sample at 0x7717f34b2ca0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "128",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.xstory_cloze",
    "docstring": {
      "name": "Xstory Cloze",
      "dataset": [
        "juletxara/xstory_cloze"
      ],
      "abstract": "XStoryCloze consists of the professionally translated version of the English\nStoryCloze dataset (Spring 2016 version) to 10 non-English languages. This\ndataset is released by Meta AI.",
      "languages": [
        "english",
        "russian",
        "chinese",
        "spanish",
        "arabic",
        "hindi",
        "indonesian",
        "telugu",
        "swahili",
        "basque",
        "burmese"
      ],
      "tags": [
        "multilingual",
        "narrative",
        "reasoning"
      ]
    },
    "tasks": [
      {
        "name": "xstory_cloze:en",
        "config": {
          "name": "xstory_cloze:en",
          "prompt_function": "<function storycloze_prompt at 0x7717f34b31a0>",
          "hf_repo": "juletxara/xstory_cloze",
          "hf_subset": "en",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b04710>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('training', 'eval')",
          "evaluation_splits": "('eval',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "xstory_cloze:ru",
        "config": {
          "name": "xstory_cloze:ru",
          "prompt_function": "<function storycloze_prompt at 0x7717f34b31a0>",
          "hf_repo": "juletxara/xstory_cloze",
          "hf_subset": "ru",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b04b00>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('training', 'eval')",
          "evaluation_splits": "('eval',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "xstory_cloze:zh",
        "config": {
          "name": "xstory_cloze:zh",
          "prompt_function": "<function storycloze_prompt at 0x7717f34b31a0>",
          "hf_repo": "juletxara/xstory_cloze",
          "hf_subset": "zh",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b04ef0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('training', 'eval')",
          "evaluation_splits": "('eval',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "xstory_cloze:es",
        "config": {
          "name": "xstory_cloze:es",
          "prompt_function": "<function storycloze_prompt at 0x7717f34b31a0>",
          "hf_repo": "juletxara/xstory_cloze",
          "hf_subset": "es",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b052e0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('training', 'eval')",
          "evaluation_splits": "('eval',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "xstory_cloze:ar",
        "config": {
          "name": "xstory_cloze:ar",
          "prompt_function": "<function storycloze_prompt at 0x7717f34b31a0>",
          "hf_repo": "juletxara/xstory_cloze",
          "hf_subset": "ar",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b056d0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('training', 'eval')",
          "evaluation_splits": "('eval',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "xstory_cloze:hi",
        "config": {
          "name": "xstory_cloze:hi",
          "prompt_function": "<function storycloze_prompt at 0x7717f34b31a0>",
          "hf_repo": "juletxara/xstory_cloze",
          "hf_subset": "hi",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b05ac0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('training', 'eval')",
          "evaluation_splits": "('eval',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "xstory_cloze:id",
        "config": {
          "name": "xstory_cloze:id",
          "prompt_function": "<function storycloze_prompt at 0x7717f34b31a0>",
          "hf_repo": "juletxara/xstory_cloze",
          "hf_subset": "id",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b05eb0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('training', 'eval')",
          "evaluation_splits": "('eval',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "xstory_cloze:te",
        "config": {
          "name": "xstory_cloze:te",
          "prompt_function": "<function storycloze_prompt at 0x7717f34b31a0>",
          "hf_repo": "juletxara/xstory_cloze",
          "hf_subset": "te",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b062a0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('training', 'eval')",
          "evaluation_splits": "('eval',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "xstory_cloze:sw",
        "config": {
          "name": "xstory_cloze:sw",
          "prompt_function": "<function storycloze_prompt at 0x7717f34b31a0>",
          "hf_repo": "juletxara/xstory_cloze",
          "hf_subset": "sw",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b06690>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('training', 'eval')",
          "evaluation_splits": "('eval',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "xstory_cloze:eu",
        "config": {
          "name": "xstory_cloze:eu",
          "prompt_function": "<function storycloze_prompt at 0x7717f34b31a0>",
          "hf_repo": "juletxara/xstory_cloze",
          "hf_subset": "eu",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b06a80>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('training', 'eval')",
          "evaluation_splits": "('eval',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "xstory_cloze:my",
        "config": {
          "name": "xstory_cloze:my",
          "prompt_function": "<function storycloze_prompt at 0x7717f34b31a0>",
          "hf_repo": "juletxara/xstory_cloze",
          "hf_subset": "my",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b06e70>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('training', 'eval')",
          "evaluation_splits": "('eval',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.boolq",
    "docstring": {
      "name": "Boolq",
      "dataset": [
        "lighteval/boolq_helm"
      ],
      "abstract": "The BoolQ benchmark for binary (yes/no) question answering.",
      "languages": [
        "english"
      ],
      "tags": [
        "qa"
      ],
      "paper": "https://arxiv.org/abs/1905.11946"
    },
    "tasks": [
      {
        "name": "boolq",
        "config": {
          "name": "boolq",
          "prompt_function": "<function boolq_prompt at 0x7717f34b3380>",
          "hf_repo": "lighteval/boolq_helm",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b073b0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f3411c60>]",
          "scorer": "<function choice.<locals>.score at 0x7717f34b3600>",
          "sample_fields": "<function record_to_sample at 0x7717f34b34c0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'validation')",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "boolq:contrastset",
        "config": {
          "name": "boolq:contrastset",
          "prompt_function": "<function boolq_contrastset_prompt at 0x7717f34b3420>",
          "hf_repo": "lighteval/boolq_helm",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0d36780>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f34b3740>]",
          "scorer": "<function choice.<locals>.score at 0x7717f34b37e0>",
          "sample_fields": "<function record_to_sample_contrastset at 0x7717f34b3560>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('validation',)",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.blimp",
    "docstring": {
      "name": "Blimp",
      "dataset": [
        "nyu-mll/blimp"
      ],
      "abstract": "BLiMP is a challenge set for evaluating what language models (LMs) know\nabout major grammatical phenomena in English. BLiMP consists of 67\nsub-datasets, each containing 1000 minimal pairs isolating specific\ncontrasts in syntax, morphology, or semantics. The data is automatically\ngenerated according to expert-crafted grammars.",
      "languages": [
        "english"
      ],
      "tags": [
        "language-modeling"
      ],
      "paper": "https://arxiv.org/abs/1912.00582"
    },
    "tasks": [
      {
        "name": "blimp:adjunct_island",
        "config": {
          "name": "blimp:adjunct_island",
          "prompt_function": "<function blimp_prompt at 0x7717f34b3920>",
          "hf_repo": "nyu-mll/blimp",
          "hf_subset": "adjunct_island",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b07b00>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "blimp:anaphor_gender_agreement",
        "config": {
          "name": "blimp:anaphor_gender_agreement",
          "prompt_function": "<function blimp_prompt at 0x7717f34b3920>",
          "hf_repo": "nyu-mll/blimp",
          "hf_subset": "anaphor_gender_agreement",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b07f20>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "blimp:anaphor_number_agreement",
        "config": {
          "name": "blimp:anaphor_number_agreement",
          "prompt_function": "<function blimp_prompt at 0x7717f34b3920>",
          "hf_repo": "nyu-mll/blimp",
          "hf_subset": "anaphor_number_agreement",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b10380>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "blimp:animate_subject_passive",
        "config": {
          "name": "blimp:animate_subject_passive",
          "prompt_function": "<function blimp_prompt at 0x7717f34b3920>",
          "hf_repo": "nyu-mll/blimp",
          "hf_subset": "animate_subject_passive",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b107a0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "blimp:animate_subject_trans",
        "config": {
          "name": "blimp:animate_subject_trans",
          "prompt_function": "<function blimp_prompt at 0x7717f34b3920>",
          "hf_repo": "nyu-mll/blimp",
          "hf_subset": "animate_subject_trans",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b10bc0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "blimp:causative",
        "config": {
          "name": "blimp:causative",
          "prompt_function": "<function blimp_prompt at 0x7717f34b3920>",
          "hf_repo": "nyu-mll/blimp",
          "hf_subset": "causative",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b10fe0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "blimp:complex_NP_island",
        "config": {
          "name": "blimp:complex_NP_island",
          "prompt_function": "<function blimp_prompt at 0x7717f34b3920>",
          "hf_repo": "nyu-mll/blimp",
          "hf_subset": "complex_NP_island",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b11400>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "blimp:drop_argument",
        "config": {
          "name": "blimp:drop_argument",
          "prompt_function": "<function blimp_prompt at 0x7717f34b3920>",
          "hf_repo": "nyu-mll/blimp",
          "hf_subset": "drop_argument",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b11820>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "blimp:ellipsis_n_bar_1",
        "config": {
          "name": "blimp:ellipsis_n_bar_1",
          "prompt_function": "<function blimp_prompt at 0x7717f34b3920>",
          "hf_repo": "nyu-mll/blimp",
          "hf_subset": "ellipsis_n_bar_1",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b11c40>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "blimp:ellipsis_n_bar_2",
        "config": {
          "name": "blimp:ellipsis_n_bar_2",
          "prompt_function": "<function blimp_prompt at 0x7717f34b3920>",
          "hf_repo": "nyu-mll/blimp",
          "hf_subset": "ellipsis_n_bar_2",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b12060>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "blimp:existential_there_object_raising",
        "config": {
          "name": "blimp:existential_there_object_raising",
          "prompt_function": "<function blimp_prompt at 0x7717f34b3920>",
          "hf_repo": "nyu-mll/blimp",
          "hf_subset": "existential_there_object_raising",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b12480>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "blimp:inchoative",
        "config": {
          "name": "blimp:inchoative",
          "prompt_function": "<function blimp_prompt at 0x7717f34b3920>",
          "hf_repo": "nyu-mll/blimp",
          "hf_subset": "inchoative",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b128a0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "blimp:intransitive",
        "config": {
          "name": "blimp:intransitive",
          "prompt_function": "<function blimp_prompt at 0x7717f34b3920>",
          "hf_repo": "nyu-mll/blimp",
          "hf_subset": "intransitive",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b12cc0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "blimp:irregular_past_participle_adjectives",
        "config": {
          "name": "blimp:irregular_past_participle_adjectives",
          "prompt_function": "<function blimp_prompt at 0x7717f34b3920>",
          "hf_repo": "nyu-mll/blimp",
          "hf_subset": "irregular_past_participle_adjectives",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b130e0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "blimp:irregular_past_participle_verbs",
        "config": {
          "name": "blimp:irregular_past_participle_verbs",
          "prompt_function": "<function blimp_prompt at 0x7717f34b3920>",
          "hf_repo": "nyu-mll/blimp",
          "hf_subset": "irregular_past_participle_verbs",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b13500>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "blimp:only_npi_scope",
        "config": {
          "name": "blimp:only_npi_scope",
          "prompt_function": "<function blimp_prompt at 0x7717f34b3920>",
          "hf_repo": "nyu-mll/blimp",
          "hf_subset": "only_npi_scope",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b13920>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "blimp:passive_1",
        "config": {
          "name": "blimp:passive_1",
          "prompt_function": "<function blimp_prompt at 0x7717f34b3920>",
          "hf_repo": "nyu-mll/blimp",
          "hf_subset": "passive_1",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b13d40>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "blimp:passive_2",
        "config": {
          "name": "blimp:passive_2",
          "prompt_function": "<function blimp_prompt at 0x7717f34b3920>",
          "hf_repo": "nyu-mll/blimp",
          "hf_subset": "passive_2",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b181a0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "blimp:principle_A_c_command",
        "config": {
          "name": "blimp:principle_A_c_command",
          "prompt_function": "<function blimp_prompt at 0x7717f34b3920>",
          "hf_repo": "nyu-mll/blimp",
          "hf_subset": "principle_A_c_command",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b185c0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "blimp:principle_A_reconstruction",
        "config": {
          "name": "blimp:principle_A_reconstruction",
          "prompt_function": "<function blimp_prompt at 0x7717f34b3920>",
          "hf_repo": "nyu-mll/blimp",
          "hf_subset": "principle_A_reconstruction",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b189e0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "blimp:regular_plural_subject_verb_agreement_1",
        "config": {
          "name": "blimp:regular_plural_subject_verb_agreement_1",
          "prompt_function": "<function blimp_prompt at 0x7717f34b3920>",
          "hf_repo": "nyu-mll/blimp",
          "hf_subset": "regular_plural_subject_verb_agreement_1",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b18e00>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "blimp:regular_plural_subject_verb_agreement_2",
        "config": {
          "name": "blimp:regular_plural_subject_verb_agreement_2",
          "prompt_function": "<function blimp_prompt at 0x7717f34b3920>",
          "hf_repo": "nyu-mll/blimp",
          "hf_subset": "regular_plural_subject_verb_agreement_2",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b19220>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "blimp:sentential_negation_npi_licensor_present",
        "config": {
          "name": "blimp:sentential_negation_npi_licensor_present",
          "prompt_function": "<function blimp_prompt at 0x7717f34b3920>",
          "hf_repo": "nyu-mll/blimp",
          "hf_subset": "sentential_negation_npi_licensor_present",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b19640>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "blimp:sentential_negation_npi_scope",
        "config": {
          "name": "blimp:sentential_negation_npi_scope",
          "prompt_function": "<function blimp_prompt at 0x7717f34b3920>",
          "hf_repo": "nyu-mll/blimp",
          "hf_subset": "sentential_negation_npi_scope",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b19a60>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "blimp:sentential_subject_island",
        "config": {
          "name": "blimp:sentential_subject_island",
          "prompt_function": "<function blimp_prompt at 0x7717f34b3920>",
          "hf_repo": "nyu-mll/blimp",
          "hf_subset": "sentential_subject_island",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b19e80>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "blimp:superlative_quantifiers_1",
        "config": {
          "name": "blimp:superlative_quantifiers_1",
          "prompt_function": "<function blimp_prompt at 0x7717f34b3920>",
          "hf_repo": "nyu-mll/blimp",
          "hf_subset": "superlative_quantifiers_1",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b1a2a0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "blimp:superlative_quantifiers_2",
        "config": {
          "name": "blimp:superlative_quantifiers_2",
          "prompt_function": "<function blimp_prompt at 0x7717f34b3920>",
          "hf_repo": "nyu-mll/blimp",
          "hf_subset": "superlative_quantifiers_2",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b1a6c0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "blimp:tough_vs_raising_1",
        "config": {
          "name": "blimp:tough_vs_raising_1",
          "prompt_function": "<function blimp_prompt at 0x7717f34b3920>",
          "hf_repo": "nyu-mll/blimp",
          "hf_subset": "tough_vs_raising_1",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b1aae0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "blimp:tough_vs_raising_2",
        "config": {
          "name": "blimp:tough_vs_raising_2",
          "prompt_function": "<function blimp_prompt at 0x7717f34b3920>",
          "hf_repo": "nyu-mll/blimp",
          "hf_subset": "tough_vs_raising_2",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b1af00>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "blimp:transitive",
        "config": {
          "name": "blimp:transitive",
          "prompt_function": "<function blimp_prompt at 0x7717f34b3920>",
          "hf_repo": "nyu-mll/blimp",
          "hf_subset": "transitive",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b1b320>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "blimp:wh_island",
        "config": {
          "name": "blimp:wh_island",
          "prompt_function": "<function blimp_prompt at 0x7717f34b3920>",
          "hf_repo": "nyu-mll/blimp",
          "hf_subset": "wh_island",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b1b740>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "blimp:wh_questions_object_gap",
        "config": {
          "name": "blimp:wh_questions_object_gap",
          "prompt_function": "<function blimp_prompt at 0x7717f34b3920>",
          "hf_repo": "nyu-mll/blimp",
          "hf_subset": "wh_questions_object_gap",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b1bb60>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "blimp:wh_questions_subject_gap",
        "config": {
          "name": "blimp:wh_questions_subject_gap",
          "prompt_function": "<function blimp_prompt at 0x7717f34b3920>",
          "hf_repo": "nyu-mll/blimp",
          "hf_subset": "wh_questions_subject_gap",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b1bf80>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "blimp:wh_questions_subject_gap_long_distance",
        "config": {
          "name": "blimp:wh_questions_subject_gap_long_distance",
          "prompt_function": "<function blimp_prompt at 0x7717f34b3920>",
          "hf_repo": "nyu-mll/blimp",
          "hf_subset": "wh_questions_subject_gap_long_distance",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b243e0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "blimp:wh_vs_that_no_gap",
        "config": {
          "name": "blimp:wh_vs_that_no_gap",
          "prompt_function": "<function blimp_prompt at 0x7717f34b3920>",
          "hf_repo": "nyu-mll/blimp",
          "hf_subset": "wh_vs_that_no_gap",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b24800>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "blimp:wh_vs_that_no_gap_long_distance",
        "config": {
          "name": "blimp:wh_vs_that_no_gap_long_distance",
          "prompt_function": "<function blimp_prompt at 0x7717f34b3920>",
          "hf_repo": "nyu-mll/blimp",
          "hf_subset": "wh_vs_that_no_gap_long_distance",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b24c20>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "blimp:wh_vs_that_with_gap",
        "config": {
          "name": "blimp:wh_vs_that_with_gap",
          "prompt_function": "<function blimp_prompt at 0x7717f34b3920>",
          "hf_repo": "nyu-mll/blimp",
          "hf_subset": "wh_vs_that_with_gap",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b25040>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "blimp:wh_vs_that_with_gap_long_distance",
        "config": {
          "name": "blimp:wh_vs_that_with_gap_long_distance",
          "prompt_function": "<function blimp_prompt at 0x7717f34b3920>",
          "hf_repo": "nyu-mll/blimp",
          "hf_subset": "wh_vs_that_with_gap_long_distance",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b25460>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.lsat_qa",
    "docstring": {
      "name": "Lsat Qa",
      "dataset": [
        "lighteval/lsat_qa"
      ],
      "abstract": "Questions from law school admission tests.",
      "languages": [
        "english"
      ],
      "tags": [
        "legal",
        "qa"
      ]
    },
    "tasks": [
      {
        "name": "lsat_qa",
        "config": {
          "name": "lsat_qa",
          "prompt_function": "<function lsat_qa_prompt at 0x7717f34b3880>",
          "hf_repo": "lighteval/lsat_qa",
          "hf_subset": "all",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b259d0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lsat_qa:assignment",
        "config": {
          "name": "lsat_qa:assignment",
          "prompt_function": "<function lsat_qa_prompt at 0x7717f34b3880>",
          "hf_repo": "lighteval/lsat_qa",
          "hf_subset": "assignment",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b25d90>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lsat_qa:grouping",
        "config": {
          "name": "lsat_qa:grouping",
          "prompt_function": "<function lsat_qa_prompt at 0x7717f34b3880>",
          "hf_repo": "lighteval/lsat_qa",
          "hf_subset": "grouping",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b26150>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lsat_qa:miscellaneous",
        "config": {
          "name": "lsat_qa:miscellaneous",
          "prompt_function": "<function lsat_qa_prompt at 0x7717f34b3880>",
          "hf_repo": "lighteval/lsat_qa",
          "hf_subset": "miscellaneous",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b26510>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lsat_qa:ordering",
        "config": {
          "name": "lsat_qa:ordering",
          "prompt_function": "<function lsat_qa_prompt at 0x7717f34b3880>",
          "hf_repo": "lighteval/lsat_qa",
          "hf_subset": "ordering",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b268d0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.olympiade_bench",
    "docstring": {
      "name": "Olympiade Bench",
      "dataset": [
        "Hothan/OlympiadBench"
      ],
      "abstract": "OlympiadBench is a benchmark for evaluating the performance of language models\non olympiad problems.",
      "languages": [
        "english",
        "chinese"
      ],
      "tags": [
        "math",
        "reasoning",
        "language"
      ],
      "paper": "https://arxiv.org/abs/2402.14008"
    },
    "tasks": [
      {
        "name": "olympiad_bench:OE_TO_maths_en_COMP",
        "config": {
          "name": "olympiad_bench:OE_TO_maths_en_COMP",
          "prompt_function": "<function olympiad_bench_prompt at 0x7717f34b3f60>",
          "hf_repo": "Hothan/OlympiadBench",
          "hf_subset": "OE_TO_maths_en_COMP",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b26de0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function generate.<locals>.solve at 0x7717f34f4180>]",
          "scorer": "<function olympiad_bench_scorer.<locals>.score at 0x7717f34f42c0>",
          "sample_fields": "<function create_record_to_sample.<locals>.record_to_sample at 0x7717f34f40e0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "train",
          "few_shots_select": "random_sampling",
          "generation_size": "2048",
          "generation_grammar": "None",
          "stop_sequence": "[]",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "1"
        }
      },
      {
        "name": "olympiad_bench:OE_TO_physics_zh_CEE",
        "config": {
          "name": "olympiad_bench:OE_TO_physics_zh_CEE",
          "prompt_function": "<function olympiad_bench_prompt at 0x7717f34b3f60>",
          "hf_repo": "Hothan/OlympiadBench",
          "hf_subset": "OE_TO_physics_zh_CEE",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b27110>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function generate.<locals>.solve at 0x7717f34f4360>]",
          "scorer": "<function olympiad_bench_scorer.<locals>.score at 0x7717f34f45e0>",
          "sample_fields": "<function create_record_to_sample.<locals>.record_to_sample at 0x7717f34f4400>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "train",
          "few_shots_select": "random_sampling",
          "generation_size": "2048",
          "generation_grammar": "None",
          "stop_sequence": "[]",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "1"
        }
      },
      {
        "name": "olympiad_bench:OE_TO_maths_zh_COMP",
        "config": {
          "name": "olympiad_bench:OE_TO_maths_zh_COMP",
          "prompt_function": "<function olympiad_bench_prompt at 0x7717f34b3f60>",
          "hf_repo": "Hothan/OlympiadBench",
          "hf_subset": "OE_TO_maths_zh_COMP",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b27440>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function generate.<locals>.solve at 0x7717f34f4540>]",
          "scorer": "<function olympiad_bench_scorer.<locals>.score at 0x7717f34f47c0>",
          "sample_fields": "<function create_record_to_sample.<locals>.record_to_sample at 0x7717f34f4680>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "train",
          "few_shots_select": "random_sampling",
          "generation_size": "2048",
          "generation_grammar": "None",
          "stop_sequence": "[]",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "1"
        }
      },
      {
        "name": "olympiad_bench:OE_TO_maths_zh_CEE",
        "config": {
          "name": "olympiad_bench:OE_TO_maths_zh_CEE",
          "prompt_function": "<function olympiad_bench_prompt at 0x7717f34b3f60>",
          "hf_repo": "Hothan/OlympiadBench",
          "hf_subset": "OE_TO_maths_zh_CEE",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b27770>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function generate.<locals>.solve at 0x7717f34f4860>]",
          "scorer": "<function olympiad_bench_scorer.<locals>.score at 0x7717f34f4a40>",
          "sample_fields": "<function create_record_to_sample.<locals>.record_to_sample at 0x7717f34f4900>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "train",
          "few_shots_select": "random_sampling",
          "generation_size": "2048",
          "generation_grammar": "None",
          "stop_sequence": "[]",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "1"
        }
      },
      {
        "name": "olympiad_bench:OE_TO_physics_en_COMP",
        "config": {
          "name": "olympiad_bench:OE_TO_physics_en_COMP",
          "prompt_function": "<function olympiad_bench_prompt at 0x7717f34b3f60>",
          "hf_repo": "Hothan/OlympiadBench",
          "hf_subset": "OE_TO_physics_en_COMP",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b27aa0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function generate.<locals>.solve at 0x7717f34f4ae0>]",
          "scorer": "<function olympiad_bench_scorer.<locals>.score at 0x7717f34f4cc0>",
          "sample_fields": "<function create_record_to_sample.<locals>.record_to_sample at 0x7717f34f4b80>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "train",
          "few_shots_select": "random_sampling",
          "generation_size": "2048",
          "generation_grammar": "None",
          "stop_sequence": "[]",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "1"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.civil_comments",
    "docstring": {
      "name": "Civil Comments",
      "dataset": [
        "lighteval/civil_comments_helm"
      ],
      "abstract": "The CivilComments benchmark for toxicity detection.",
      "languages": [
        "english"
      ],
      "tags": [
        "bias",
        "classification"
      ],
      "paper": "https://arxiv.org/abs/1903.04561"
    },
    "tasks": [
      {
        "name": "civil_comments:LGBTQ",
        "config": {
          "name": "civil_comments:LGBTQ",
          "prompt_function": "<function civil_comments_prompt at 0x7717f34f4d60>",
          "hf_repo": "lighteval/civil_comments_helm",
          "hf_subset": "LGBTQ",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b27ef0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f34f4f40>]",
          "scorer": "<function choice.<locals>.score at 0x7717f34f5080>",
          "sample_fields": "<function record_to_sample at 0x7717f34f4ea0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "100",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "civil_comments:black",
        "config": {
          "name": "civil_comments:black",
          "prompt_function": "<function civil_comments_prompt at 0x7717f34f4d60>",
          "hf_repo": "lighteval/civil_comments_helm",
          "hf_subset": "black",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b2c290>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f34f51c0>]",
          "scorer": "<function choice.<locals>.score at 0x7717f34f5260>",
          "sample_fields": "<function record_to_sample at 0x7717f34f4ea0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "100",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "civil_comments:christian",
        "config": {
          "name": "civil_comments:christian",
          "prompt_function": "<function civil_comments_prompt at 0x7717f34f4d60>",
          "hf_repo": "lighteval/civil_comments_helm",
          "hf_subset": "christian",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b2c5f0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f34f53a0>]",
          "scorer": "<function choice.<locals>.score at 0x7717f34f5440>",
          "sample_fields": "<function record_to_sample at 0x7717f34f4ea0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "100",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "civil_comments:female",
        "config": {
          "name": "civil_comments:female",
          "prompt_function": "<function civil_comments_prompt at 0x7717f34f4d60>",
          "hf_repo": "lighteval/civil_comments_helm",
          "hf_subset": "female",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b2c950>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f34f5580>]",
          "scorer": "<function choice.<locals>.score at 0x7717f34f5620>",
          "sample_fields": "<function record_to_sample at 0x7717f34f4ea0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "100",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "civil_comments:male",
        "config": {
          "name": "civil_comments:male",
          "prompt_function": "<function civil_comments_prompt at 0x7717f34f4d60>",
          "hf_repo": "lighteval/civil_comments_helm",
          "hf_subset": "male",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b2ccb0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f34f5760>]",
          "scorer": "<function choice.<locals>.score at 0x7717f34f5800>",
          "sample_fields": "<function record_to_sample at 0x7717f34f4ea0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "100",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "civil_comments:muslim",
        "config": {
          "name": "civil_comments:muslim",
          "prompt_function": "<function civil_comments_prompt at 0x7717f34f4d60>",
          "hf_repo": "lighteval/civil_comments_helm",
          "hf_subset": "muslim",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b2d010>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f34f5940>]",
          "scorer": "<function choice.<locals>.score at 0x7717f34f59e0>",
          "sample_fields": "<function record_to_sample at 0x7717f34f4ea0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "100",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "civil_comments:other_religions",
        "config": {
          "name": "civil_comments:other_religions",
          "prompt_function": "<function civil_comments_prompt at 0x7717f34f4d60>",
          "hf_repo": "lighteval/civil_comments_helm",
          "hf_subset": "other_religions",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b2d370>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f34f5b20>]",
          "scorer": "<function choice.<locals>.score at 0x7717f34f5bc0>",
          "sample_fields": "<function record_to_sample at 0x7717f34f4ea0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "100",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "civil_comments:white",
        "config": {
          "name": "civil_comments:white",
          "prompt_function": "<function civil_comments_prompt at 0x7717f34f4d60>",
          "hf_repo": "lighteval/civil_comments_helm",
          "hf_subset": "white",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b2d6d0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f34f5d00>]",
          "scorer": "<function choice.<locals>.score at 0x7717f34f5da0>",
          "sample_fields": "<function record_to_sample at 0x7717f34f4ea0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "100",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.mmlu_redux",
    "docstring": {
      "name": "Mmlu Redux",
      "dataset": [
        "edinburgh-dawg/mmlu-redux-2.0"
      ],
      "abstract": "MMLU-Redux is a subset of 5,700 manually re-annotated questions across 57 MMLU subjects.",
      "languages": [
        "english"
      ],
      "tags": [
        "general-knowledge",
        "knowledge",
        "multiple-choice"
      ],
      "paper": "https://arxiv.org/abs/2406.04127"
    },
    "tasks": [
      {
        "name": "mmlu_redux_2:abstract_algebra",
        "config": {
          "name": "mmlu_redux_2:abstract_algebra",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f6020>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "abstract_algebra",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b2db20>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b2db80>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:anatomy",
        "config": {
          "name": "mmlu_redux_2:anatomy",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f60c0>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "anatomy",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b2dfd0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b2e030>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:astronomy",
        "config": {
          "name": "mmlu_redux_2:astronomy",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f6160>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "astronomy",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b2e480>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b2e4e0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:business_ethics",
        "config": {
          "name": "mmlu_redux_2:business_ethics",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f62a0>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "business_ethics",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b2e930>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b2e990>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:clinical_knowledge",
        "config": {
          "name": "mmlu_redux_2:clinical_knowledge",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f6340>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "clinical_knowledge",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b2edb0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b2ee10>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:college_biology",
        "config": {
          "name": "mmlu_redux_2:college_biology",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f6200>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "college_biology",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b2f260>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b2f2c0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:college_chemistry",
        "config": {
          "name": "mmlu_redux_2:college_chemistry",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f63e0>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "college_chemistry",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b2f710>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b2f770>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:college_computer_science",
        "config": {
          "name": "mmlu_redux_2:college_computer_science",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f6520>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "college_computer_science",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b2fbc0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b2fc20>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:college_mathematics",
        "config": {
          "name": "mmlu_redux_2:college_mathematics",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f65c0>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "college_mathematics",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b300b0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b30110>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:college_medicine",
        "config": {
          "name": "mmlu_redux_2:college_medicine",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f6480>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "college_medicine",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b30560>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b305c0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:college_physics",
        "config": {
          "name": "mmlu_redux_2:college_physics",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f6660>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "college_physics",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b308c0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b308f0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:computer_security",
        "config": {
          "name": "mmlu_redux_2:computer_security",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f6700>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "computer_security",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b30bf0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b30c50>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:conceptual_physics",
        "config": {
          "name": "mmlu_redux_2:conceptual_physics",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f67a0>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "conceptual_physics",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b30f50>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b30fb0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:econometrics",
        "config": {
          "name": "mmlu_redux_2:econometrics",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f6840>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "econometrics",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b312b0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b312e0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:electrical_engineering",
        "config": {
          "name": "mmlu_redux_2:electrical_engineering",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f68e0>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "electrical_engineering",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b315e0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b31640>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:elementary_mathematics",
        "config": {
          "name": "mmlu_redux_2:elementary_mathematics",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f6980>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "elementary_mathematics",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b31940>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b319a0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:formal_logic",
        "config": {
          "name": "mmlu_redux_2:formal_logic",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f6a20>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "formal_logic",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b31ca0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b31d00>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:global_facts",
        "config": {
          "name": "mmlu_redux_2:global_facts",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f6ac0>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "global_facts",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b32000>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b32060>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:high_school_biology",
        "config": {
          "name": "mmlu_redux_2:high_school_biology",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f6b60>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "high_school_biology",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b32360>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b323c0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:high_school_chemistry",
        "config": {
          "name": "mmlu_redux_2:high_school_chemistry",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f6c00>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "high_school_chemistry",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b326c0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b32720>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:high_school_computer_science",
        "config": {
          "name": "mmlu_redux_2:high_school_computer_science",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f6ca0>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "high_school_computer_science",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b32a20>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b32a80>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:high_school_european_history",
        "config": {
          "name": "mmlu_redux_2:high_school_european_history",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f6d40>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "high_school_european_history",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b32d80>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b32de0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:high_school_geography",
        "config": {
          "name": "mmlu_redux_2:high_school_geography",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f6de0>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "high_school_geography",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b330b0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b33110>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:high_school_government_and_politics",
        "config": {
          "name": "mmlu_redux_2:high_school_government_and_politics",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f6e80>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "high_school_government_and_politics",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b33410>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b33470>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:high_school_macroeconomics",
        "config": {
          "name": "mmlu_redux_2:high_school_macroeconomics",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f6f20>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "high_school_macroeconomics",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b33770>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b337d0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:high_school_mathematics",
        "config": {
          "name": "mmlu_redux_2:high_school_mathematics",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f6fc0>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "high_school_mathematics",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b33aa0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b33b00>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:high_school_microeconomics",
        "config": {
          "name": "mmlu_redux_2:high_school_microeconomics",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f7060>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "high_school_microeconomics",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b33e00>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b33e60>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:high_school_physics",
        "config": {
          "name": "mmlu_redux_2:high_school_physics",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f7100>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "high_school_physics",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b381a0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b38200>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:high_school_psychology",
        "config": {
          "name": "mmlu_redux_2:high_school_psychology",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f71a0>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "high_school_psychology",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b384d0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b38530>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:high_school_statistics",
        "config": {
          "name": "mmlu_redux_2:high_school_statistics",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f7240>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "high_school_statistics",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b38830>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b38890>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:high_school_us_history",
        "config": {
          "name": "mmlu_redux_2:high_school_us_history",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f72e0>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "high_school_us_history",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b38b90>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b38bf0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:high_school_world_history",
        "config": {
          "name": "mmlu_redux_2:high_school_world_history",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f7380>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "high_school_world_history",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b38ef0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b38f50>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:human_aging",
        "config": {
          "name": "mmlu_redux_2:human_aging",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f7420>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "human_aging",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b39250>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b392b0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:human_sexuality",
        "config": {
          "name": "mmlu_redux_2:human_sexuality",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f74c0>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "human_sexuality",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b395b0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b39610>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:international_law",
        "config": {
          "name": "mmlu_redux_2:international_law",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f7560>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "international_law",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b39910>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b39970>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:jurisprudence",
        "config": {
          "name": "mmlu_redux_2:jurisprudence",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f7600>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "jurisprudence",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b39c70>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b39cd0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:logical_fallacies",
        "config": {
          "name": "mmlu_redux_2:logical_fallacies",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f76a0>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "logical_fallacies",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b39fd0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b3a030>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:machine_learning",
        "config": {
          "name": "mmlu_redux_2:machine_learning",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f7740>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "machine_learning",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b3a330>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b3a390>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:management",
        "config": {
          "name": "mmlu_redux_2:management",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f77e0>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "management",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b3a690>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b3a6f0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:marketing",
        "config": {
          "name": "mmlu_redux_2:marketing",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f7880>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "marketing",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b3a9f0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b3aa50>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:medical_genetics",
        "config": {
          "name": "mmlu_redux_2:medical_genetics",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f7920>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "medical_genetics",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b3ad50>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b3adb0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:miscellaneous",
        "config": {
          "name": "mmlu_redux_2:miscellaneous",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f79c0>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "miscellaneous",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b3b0b0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b3b110>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:moral_disputes",
        "config": {
          "name": "mmlu_redux_2:moral_disputes",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f7a60>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "moral_disputes",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b3b410>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b3b470>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:moral_scenarios",
        "config": {
          "name": "mmlu_redux_2:moral_scenarios",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f7b00>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "moral_scenarios",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b3b770>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b3b7d0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:nutrition",
        "config": {
          "name": "mmlu_redux_2:nutrition",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f7ba0>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "nutrition",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b3bad0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b3bb30>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:philosophy",
        "config": {
          "name": "mmlu_redux_2:philosophy",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f7c40>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "philosophy",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b3be30>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b3be90>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:prehistory",
        "config": {
          "name": "mmlu_redux_2:prehistory",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f7ce0>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "prehistory",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b401d0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b40230>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:professional_accounting",
        "config": {
          "name": "mmlu_redux_2:professional_accounting",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f7d80>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "professional_accounting",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b40530>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b40590>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:professional_law",
        "config": {
          "name": "mmlu_redux_2:professional_law",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f7e20>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "professional_law",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b40890>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b408f0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:professional_medicine",
        "config": {
          "name": "mmlu_redux_2:professional_medicine",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f7ec0>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "professional_medicine",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b40bf0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b40c50>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:professional_psychology",
        "config": {
          "name": "mmlu_redux_2:professional_psychology",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f34f7f60>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "professional_psychology",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b40f20>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b40f80>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:public_relations",
        "config": {
          "name": "mmlu_redux_2:public_relations",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f3338040>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "public_relations",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b41280>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b412e0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:security_studies",
        "config": {
          "name": "mmlu_redux_2:security_studies",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f33380e0>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "security_studies",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b415e0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b41640>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:sociology",
        "config": {
          "name": "mmlu_redux_2:sociology",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f3338180>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "sociology",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b41940>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b419a0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:us_foreign_policy",
        "config": {
          "name": "mmlu_redux_2:us_foreign_policy",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f3338220>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "us_foreign_policy",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b41ca0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b41d00>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:virology",
        "config": {
          "name": "mmlu_redux_2:virology",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f33382c0>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "virology",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b42000>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b42060>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mmlu_redux_2:world_religions",
        "config": {
          "name": "mmlu_redux_2:world_religions",
          "prompt_function": "<function mmlu_redux_2_prompt.<locals>._fn at 0x7717f3338360>",
          "hf_repo": "edinburgh-dawg/mmlu-redux-2.0",
          "hf_subset": "world_religions",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b42360>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b423c0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.multi_challenge",
    "docstring": {
      "name": "MultiChallenge",
      "dataset": [
        "nmayorga7/multichallenge"
      ],
      "abstract": "MultiChallenge evaluates large language models (LLMs) on their ability to\nconduct multi-turn conversations with human users.\nThe model is given a target question belonging to one or\nmore axes (categories) and must provide a free-form answer.\nThe evaluation uses a secondary judge model to determine if the\nanswer satisfies the pass criteria for that question.",
      "languages": [
        "english"
      ],
      "tags": [
        "conversational",
        "generation",
        "instruction-following"
      ],
      "paper": "https://arxiv.org/abs/2501.17399",
      "starred": "true"
    },
    "tasks": [
      {
        "name": "multi_challenge",
        "config": {
          "name": "multi_challenge",
          "prompt_function": "<function <lambda> at 0x7717f33387c0>",
          "hf_repo": "nmayorga7/multichallenge",
          "hf_subset": "default",
          "metrics": "()",
          "hf_data_files": "None",
          "solver": "[<function generate.<locals>.solve at 0x7717f3338860>]",
          "scorer": "<function multi_challenge_scorer.<locals>.score at 0x7717f3338ae0>",
          "sample_fields": "<function record_to_sample at 0x7717f3338680>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "2048",
          "generation_grammar": "None",
          "stop_sequence": "[]",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.imdb",
    "docstring": {
      "name": "Imdb",
      "dataset": [
        "lighteval/IMDB_helm"
      ],
      "The IMDB benchmark for sentiment analysis in movie review, from": "Learning Word Vectors for Sentiment Analysis",
      "languages": [
        "english"
      ],
      "tags": [
        "classification"
      ],
      "paper": "https://aclanthology.org/P11-1015/"
    },
    "tasks": [
      {
        "name": "imdb",
        "config": {
          "name": "imdb",
          "prompt_function": "<function imdb_prompt at 0x7717f3338b80>",
          "hf_repo": "lighteval/IMDB_helm",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b42c30>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "imdb:contrastset",
        "config": {
          "name": "imdb:contrastset",
          "prompt_function": "<function imdb_contrastset_prompt at 0x7717f3338a40>",
          "hf_repo": "lighteval/IMDB_helm",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b42f90>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "5",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.mgsm",
    "docstring": {
      "name": "Mgsm",
      "dataset": [
        "juletxara/mgsm"
      ],
      "abstract": "Multilingual Grade School Math Benchmark (MGSM) is a benchmark of grade-school\nmath problems.\nThe same 250 problems from GSM8K are each translated via human annotators in 10\nlanguages.",
      "languages": [
        "english",
        "spanish",
        "french",
        "german",
        "russian",
        "chinese",
        "japanese",
        "thai",
        "swahili",
        "bengali",
        "telugu"
      ],
      "tags": [
        "math",
        "multilingual",
        "reasoning"
      ],
      "paper": "https://arxiv.org/abs/2210.03057"
    },
    "tasks": [
      {
        "name": "mgsm:en",
        "config": {
          "name": "mgsm:en",
          "prompt_function": "<function mgsm_en_prompt at 0x7717f3338cc0>",
          "hf_repo": "juletxara/mgsm",
          "hf_subset": "en",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b435f0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mgsm:es",
        "config": {
          "name": "mgsm:es",
          "prompt_function": "<function mgsm_es_prompt at 0x7717f3338d60>",
          "hf_repo": "juletxara/mgsm",
          "hf_subset": "es",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b43950>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mgsm:fr",
        "config": {
          "name": "mgsm:fr",
          "prompt_function": "<function mgsm_fr_prompt at 0x7717f3338e00>",
          "hf_repo": "juletxara/mgsm",
          "hf_subset": "fr",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b43cb0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mgsm:de",
        "config": {
          "name": "mgsm:de",
          "prompt_function": "<function mgsm_de_prompt at 0x7717f3338ea0>",
          "hf_repo": "juletxara/mgsm",
          "hf_subset": "de",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b48050>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mgsm:ru",
        "config": {
          "name": "mgsm:ru",
          "prompt_function": "<function mgsm_ru_prompt at 0x7717f3338f40>",
          "hf_repo": "juletxara/mgsm",
          "hf_subset": "ru",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b483b0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mgsm:zh",
        "config": {
          "name": "mgsm:zh",
          "prompt_function": "<function mgsm_zh_prompt at 0x7717f3338fe0>",
          "hf_repo": "juletxara/mgsm",
          "hf_subset": "zh",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b48710>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mgsm:ja",
        "config": {
          "name": "mgsm:ja",
          "prompt_function": "<function mgsm_ja_prompt at 0x7717f3339080>",
          "hf_repo": "juletxara/mgsm",
          "hf_subset": "ja",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b48a70>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mgsm:th",
        "config": {
          "name": "mgsm:th",
          "prompt_function": "<function mgsm_th_prompt at 0x7717f3339120>",
          "hf_repo": "juletxara/mgsm",
          "hf_subset": "th",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b48dd0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mgsm:sw",
        "config": {
          "name": "mgsm:sw",
          "prompt_function": "<function mgsm_sw_prompt at 0x7717f33391c0>",
          "hf_repo": "juletxara/mgsm",
          "hf_subset": "sw",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b49130>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mgsm:bn",
        "config": {
          "name": "mgsm:bn",
          "prompt_function": "<function mgsm_bn_prompt at 0x7717f3339260>",
          "hf_repo": "juletxara/mgsm",
          "hf_subset": "bn",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b49490>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "mgsm:te",
        "config": {
          "name": "mgsm:te",
          "prompt_function": "<function mgsm_te_prompt at 0x7717f3339300>",
          "hf_repo": "juletxara/mgsm",
          "hf_subset": "te",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b497f0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.aime",
    "docstring": {
      "name": "Aime",
      "dataset": [
        "HuggingFaceH4/aime_2024",
        "yentinglin/aime_2025"
      ],
      "abstract": "The American Invitational Mathematics Examination (AIME) is a prestigious,\ninvite-only mathematics competition for high-school students who perform in the\ntop 5% of the AMC 12 mathematics exam. It involves 15 questions of increasing\ndifficulty, with the answer to every question being a single integer from 0 to\n999. The median score is historically between 4 and 6 questions correct (out of\nthe 15 possible). Two versions of the test are given every year (thirty\nquestions total).",
      "languages": [
        "english"
      ],
      "tags": [
        "math",
        "reasoning"
      ],
      "paper": "https://maa.org/aime-thresholds-are-available/",
      "starred": "true"
    },
    "tasks": [
      {
        "name": "aime24",
        "config": {
          "name": "aime24",
          "prompt_function": "<function aime_prompt at 0x7717f3339440>",
          "hf_repo": "HuggingFaceH4/aime_2024",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'pass@k:k=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b49d00>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'avg@n:n=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.AvgAtN object at 0x7717f0b49cd0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "[<function prompt_template.<locals>.solve at 0x7717f33394e0>, <function generate.<locals>.solve at 0x7717f3339620>]",
          "scorer": "<function math_scorer.<locals>.score at 0x7717f3339760>",
          "sample_fields": "<function record_to_sample at 0x7717f33393a0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "2"
        }
      },
      {
        "name": "aime24_avg",
        "config": {
          "name": "aime24_avg",
          "prompt_function": "<function aime_prompt at 0x7717f3339440>",
          "hf_repo": "HuggingFaceH4/aime_2024",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'avg@n:n=64', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.AvgAtN object at 0x7717f0b49cd0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "<function record_to_sample at 0x7717f33393a0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "2"
        }
      },
      {
        "name": "aime24_gpassk",
        "config": {
          "name": "aime24_gpassk",
          "prompt_function": "<function aime_prompt at 0x7717f3339440>",
          "hf_repo": "HuggingFaceH4/aime_2024",
          "hf_subset": "default",
          "metrics": "({'metric_name': ['math_g-pass@16_0.0', 'math_g-pass@16_0.25', 'math_g-pass@16_0.5', 'math_g-pass@16_0.75', 'math_g-pass@16_1.0', 'mmath_g-pass@16'], 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.GPassAtK object at 0x7717f0b4a1b0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "<function record_to_sample at 0x7717f33393a0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "1"
        }
      },
      {
        "name": "aime25",
        "config": {
          "name": "aime25",
          "prompt_function": "<function aime_prompt at 0x7717f3339440>",
          "hf_repo": "yentinglin/aime_2025",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'pass@k:k=1&n=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.PassAtK object at 0x7717f0b4a510>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False}, {'metric_name': 'avg@n:n=1', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.AvgAtN object at 0x7717f0b4a480>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False})",
          "hf_data_files": "None",
          "solver": "[<function prompt_template.<locals>.solve at 0x7717f33398a0>, <function generate.<locals>.solve at 0x7717f3339940>]",
          "scorer": "<function math_scorer.<locals>.score at 0x7717f3339a80>",
          "sample_fields": "<function record_to_sample at 0x7717f33393a0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "2"
        }
      },
      {
        "name": "aime25_avg",
        "config": {
          "name": "aime25_avg",
          "prompt_function": "<function aime_prompt at 0x7717f3339440>",
          "hf_repo": "yentinglin/aime_2025",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'avg@n:n=64', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.AvgAtN object at 0x7717f0b4a480>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "<function record_to_sample at 0x7717f33393a0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "2"
        }
      },
      {
        "name": "aime25_gpassk",
        "config": {
          "name": "aime25_gpassk",
          "prompt_function": "<function aime_prompt at 0x7717f3339440>",
          "hf_repo": "yentinglin/aime_2025",
          "hf_subset": "default",
          "metrics": "({'metric_name': ['math_g-pass@16_0.0', 'math_g-pass@16_0.25', 'math_g-pass@16_0.5', 'math_g-pass@16_0.75', 'math_g-pass@16_1.0', 'mmath_g-pass@16'], 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.GPassAtK object at 0x7717f0b4aba0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "<function record_to_sample at 0x7717f33393a0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "1"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.narrativeqa",
    "docstring": {
      "name": "Narrativeqa",
      "dataset": [
        "lighteval/narrative_qa_helm"
      ],
      "abstract": "NarrativeQA is a reading comprehension benchmark that tests deep understanding\nof full narratives\u2014books and movie scripts\u2014rather than shallow text matching. To\nanswer its questions, models must integrate information across entire stories.",
      "languages": [
        "english"
      ],
      "tags": [
        "qa",
        "reading-comprehension"
      ],
      "paper": "https://aclanthology.org/Q18-1023/"
    },
    "tasks": [
      {
        "name": "narrativeqa",
        "config": {
          "name": "narrativeqa",
          "prompt_function": "<function narrativeqa_prompt at 0x7717f3339c60>",
          "hf_repo": "lighteval/narrative_qa_helm",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b4b020>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "100",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.legalsupport",
    "docstring": {
      "name": "Legalsupport",
      "dataset": [
        "lighteval/LegalSupport"
      ],
      "abstract": "Measures fine-grained legal reasoning through reverse entailment.",
      "languages": [
        "english"
      ],
      "tags": [
        "legal"
      ]
    },
    "tasks": [
      {
        "name": "legalsupport",
        "config": {
          "name": "legalsupport",
          "prompt_function": "<function legalsupport_prompt at 0x7717f3339d00>",
          "hf_repo": "lighteval/LegalSupport",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b4b470>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation', 'test')",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.headqa",
    "docstring": {
      "name": "Headqa",
      "dataset": [
        "lighteval/headqa_harness"
      ],
      "abstract": "HEAD-QA is a multi-choice HEAlthcare Dataset. The questions come from exams to\naccess a specialized position in the Spanish healthcare system, and are\nchallenging even for highly specialized humans. They are designed by the\nMinisterio de Sanidad, Consumo y Bienestar Social, who also provides direct\naccess to the exams of the last 5 years.",
      "languages": [
        "english",
        "spanish"
      ],
      "tags": [
        "health",
        "medical",
        "multiple-choice",
        "qa"
      ],
      "paper": "https://arxiv.org/abs/1906.04701"
    },
    "tasks": [
      {
        "name": "headqa:en",
        "config": {
          "name": "headqa:en",
          "prompt_function": "<function headqa_prompt at 0x7717f3339da0>",
          "hf_repo": "lighteval/headqa_harness",
          "hf_subset": "en",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b4b9b0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "headqa:es",
        "config": {
          "name": "headqa:es",
          "prompt_function": "<function headqa_prompt at 0x7717f3339da0>",
          "hf_repo": "lighteval/headqa_harness",
          "hf_subset": "es",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b4bd10>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.drop_qa",
    "docstring": {
      "name": "Drop Qa",
      "dataset": [
        "lighteval/drop_harness"
      ],
      "abstract": "The DROP dataset is a new question-answering dataset designed to evaluate the\nability of language models to answer complex questions that require reasoning\nover multiple sentences.",
      "languages": [
        "english"
      ],
      "tags": [
        "math",
        "qa",
        "reasoning"
      ],
      "paper": "https://arxiv.org/abs/1810.00505"
    },
    "tasks": [
      {
        "name": "drop",
        "config": {
          "name": "drop",
          "prompt_function": "<function drop_prompt at 0x7717f3339ee0>",
          "hf_repo": "lighteval/drop_harness",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b58230>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'validation', 'test')",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "train",
          "few_shots_select": "None",
          "generation_size": "250",
          "generation_grammar": "None",
          "stop_sequence": "['Question:', 'question:', '\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "1"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.winogrande",
    "docstring": {
      "name": "Winogrande",
      "dataset": [
        "allenai/winogrande"
      ],
      "abstract": "WinoGrande is a new collection of 44k problems, inspired by Winograd Schema\nChallenge (Levesque, Davis, and Morgenstern 2011), but adjusted to improve the\nscale and robustness against the dataset-specific bias. Formulated as a\nfill-in-a-blank task with binary options, the goal is to choose the right option\nfor a given sentence which requires commonsense reasoning.",
      "languages": [
        "english"
      ],
      "tags": [
        "commonsense",
        "multiple-choice"
      ],
      "paper": "https://arxiv.org/abs/1907.10641"
    },
    "tasks": [
      {
        "name": "winogrande",
        "config": {
          "name": "winogrande",
          "prompt_function": "<function winogrande_prompt at 0x7717f3455e40>",
          "hf_repo": "allenai/winogrande",
          "hf_subset": "winogrande_xl",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b585f0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "random_sampling",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.commonsenseqa",
    "docstring": {
      "name": "Commonsenseqa",
      "dataset": [
        "tau/commonsense_qa"
      ],
      "abstract": "CommonsenseQA is a new multiple-choice question answering dataset that requires\ndifferent types of commonsense knowledge to predict the correct answers . It\ncontains 12,102 questions with one correct answer and four distractor answers.",
      "The dataset is provided in two major training/validation/testing set splits": "\"Random split\" which is the main evaluation split, and \"Question token split\",\nsee paper for details.",
      "languages": [
        "english"
      ],
      "tags": [
        "commonsense",
        "multiple-choice",
        "qa"
      ],
      "paper": "https://arxiv.org/abs/1811.00937"
    },
    "tasks": [
      {
        "name": "commonsenseqa",
        "config": {
          "name": "commonsenseqa",
          "prompt_function": "<function commonsenseqa_prompt at 0x7717f333a020>",
          "hf_repo": "tau/commonsense_qa",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b58a40>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f333a160>]",
          "scorer": "<function choice.<locals>.score at 0x7717f333a2a0>",
          "sample_fields": "<function record_to_sample at 0x7717f333a0c0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test', 'validation')",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.mathqa",
    "docstring": {
      "name": "Mathqa",
      "dataset": [
        "allenai/math_qa"
      ],
      "abstract": "large-scale dataset of math word problems.  Our dataset is gathered by using a\nnew representation language to annotate over the AQuA-RAT dataset with\nfully-specified operational programs.  AQuA-RAT has provided the questions,\noptions, rationale, and the correct options.",
      "languages": [
        "english"
      ],
      "tags": [
        "math",
        "qa",
        "reasoning"
      ],
      "paper": "https://arxiv.org/abs/1905.13319"
    },
    "tasks": [
      {
        "name": "mathqa",
        "config": {
          "name": "mathqa",
          "prompt_function": "<function mathqa_prompt at 0x7717f333a3e0>",
          "hf_repo": "allenai/math_qa",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b58e90>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'validation', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.xwinograd",
    "docstring": {
      "name": "Xwinograd",
      "dataset": [
        "Muennighoff/xwinograd"
      ],
      "abstract": "Multilingual winograd schema challenge as used in Crosslingual Generalization through Multitask Finetuning.",
      "languages": [
        "english",
        "french",
        "japanese",
        "portuguese",
        "russian",
        "chinese"
      ],
      "tags": [
        "commonsense",
        "multilingual",
        "reasoning"
      ],
      "paper": "https://arxiv.org/abs/2211.01786"
    },
    "tasks": [
      {
        "name": "xwinograd:en",
        "config": {
          "name": "xwinograd:en",
          "prompt_function": "<function xwinograd_prompt at 0x7717f333a480>",
          "hf_repo": "Muennighoff/xwinograd",
          "hf_subset": "en",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b59370>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "xwinograd:fr",
        "config": {
          "name": "xwinograd:fr",
          "prompt_function": "<function xwinograd_prompt at 0x7717f333a480>",
          "hf_repo": "Muennighoff/xwinograd",
          "hf_subset": "fr",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b596d0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "xwinograd:jp",
        "config": {
          "name": "xwinograd:jp",
          "prompt_function": "<function xwinograd_prompt at 0x7717f333a480>",
          "hf_repo": "Muennighoff/xwinograd",
          "hf_subset": "jp",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b59a30>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "xwinograd:pt",
        "config": {
          "name": "xwinograd:pt",
          "prompt_function": "<function xwinograd_prompt at 0x7717f333a480>",
          "hf_repo": "Muennighoff/xwinograd",
          "hf_subset": "pt",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b59d90>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "xwinograd:ru",
        "config": {
          "name": "xwinograd:ru",
          "prompt_function": "<function xwinograd_prompt at 0x7717f333a480>",
          "hf_repo": "Muennighoff/xwinograd",
          "hf_subset": "ru",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b5a0f0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "xwinograd:zh",
        "config": {
          "name": "xwinograd:zh",
          "prompt_function": "<function xwinograd_prompt at 0x7717f333a480>",
          "hf_repo": "Muennighoff/xwinograd",
          "hf_subset": "zh",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b5a450>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.webqs",
    "docstring": {
      "name": "Webqs",
      "dataset": [
        "stanfordnlp/web_questions"
      ],
      "abstract": "This dataset consists of 6,642 question/answer pairs. The questions are supposed\nto be answerable by Freebase, a large knowledge graph. The questions are mostly\ncentered around a single named entity. The questions are popular ones asked on\nthe web.",
      "languages": [
        "english"
      ],
      "tags": [
        "qa"
      ],
      "paper": "https://aclanthology.org/D13-1160.pdf"
    },
    "tasks": [
      {
        "name": "webqs",
        "config": {
          "name": "webqs",
          "prompt_function": "<function webqs_prompt at 0x7717f333a520>",
          "hf_repo": "stanfordnlp/web_questions",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b5a900>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.agieval",
    "docstring": {
      "name": "Agieval",
      "dataset": [
        "dmayhem93/agieval-aqua-rat",
        "dmayhem93/agieval-gaokao-biology",
        "dmayhem93/agieval-gaokao-chemistry",
        "dmayhem93/agieval-gaokao-chinese",
        "dmayhem93/agieval-gaokao-english",
        "dmayhem93/agieval-gaokao-geography",
        "dmayhem93/agieval-gaokao-history",
        "dmayhem93/agieval-gaokao-mathqa",
        "dmayhem93/agieval-gaokao-physics",
        "dmayhem93/agieval-logiqa-en",
        "dmayhem93/agieval-logiqa-zh",
        "dmayhem93/agieval-lsat-ar",
        "dmayhem93/agieval-lsat-lr",
        "dmayhem93/agieval-lsat-rc",
        "dmayhem93/agieval-sat-en",
        "dmayhem93/agieval-sat-en-without-passage",
        "dmayhem93/agieval-sat-math"
      ],
      "abstract": "AGIEval is a human-centric benchmark specifically designed to evaluate the\ngeneral abilities of foundation models in tasks pertinent to human cognition and\nproblem-solving. This benchmark is derived from 20 official, public, and\nhigh-standard admission and qualification exams intended for general human\ntest-takers, such as general college admission tests (e.g., Chinese College\nEntrance Exam (Gaokao) and American SAT), law school admission tests, math\ncompetitions, lawyer qualification tests, and national civil service exams.",
      "languages": [
        "english",
        "chinese"
      ],
      "tags": [
        "biology",
        "chemistry",
        "geography",
        "history",
        "knowledge",
        "language",
        "multiple-choice",
        "physics",
        "reasoning"
      ],
      "paper": "https://arxiv.org/abs/2304.06364"
    },
    "tasks": [
      {
        "name": "agieval:aqua-rat",
        "config": {
          "name": "agieval:aqua-rat",
          "prompt_function": "<function agieval_prompt at 0x7717f333a660>",
          "hf_repo": "dmayhem93/agieval-aqua-rat",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b5ae40>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f333a700>]",
          "scorer": "<function choice.<locals>.score at 0x7717f333a840>",
          "sample_fields": "<function record_to_sample at 0x7717f333a5c0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "random_sampling",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "agieval:gaokao-biology",
        "config": {
          "name": "agieval:gaokao-biology",
          "prompt_function": "<function agieval_prompt at 0x7717f333a660>",
          "hf_repo": "dmayhem93/agieval-gaokao-biology",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b5b0e0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f333a980>]",
          "scorer": "<function choice.<locals>.score at 0x7717f333aa20>",
          "sample_fields": "<function record_to_sample at 0x7717f333a5c0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "random_sampling",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "agieval:gaokao-chemistry",
        "config": {
          "name": "agieval:gaokao-chemistry",
          "prompt_function": "<function agieval_prompt at 0x7717f333a660>",
          "hf_repo": "dmayhem93/agieval-gaokao-chemistry",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b5b380>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f333ab60>]",
          "scorer": "<function choice.<locals>.score at 0x7717f333ac00>",
          "sample_fields": "<function record_to_sample at 0x7717f333a5c0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "random_sampling",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "agieval:gaokao-chinese",
        "config": {
          "name": "agieval:gaokao-chinese",
          "prompt_function": "<function agieval_prompt at 0x7717f333a660>",
          "hf_repo": "dmayhem93/agieval-gaokao-chinese",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b5b620>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f333ad40>]",
          "scorer": "<function choice.<locals>.score at 0x7717f333ade0>",
          "sample_fields": "<function record_to_sample at 0x7717f333a5c0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "random_sampling",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "agieval:gaokao-english",
        "config": {
          "name": "agieval:gaokao-english",
          "prompt_function": "<function agieval_prompt at 0x7717f333a660>",
          "hf_repo": "dmayhem93/agieval-gaokao-english",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b5b8c0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f333af20>]",
          "scorer": "<function choice.<locals>.score at 0x7717f333afc0>",
          "sample_fields": "<function record_to_sample at 0x7717f333a5c0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "random_sampling",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "agieval:gaokao-geography",
        "config": {
          "name": "agieval:gaokao-geography",
          "prompt_function": "<function agieval_prompt at 0x7717f333a660>",
          "hf_repo": "dmayhem93/agieval-gaokao-geography",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b5bb60>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f333b100>]",
          "scorer": "<function choice.<locals>.score at 0x7717f333b1a0>",
          "sample_fields": "<function record_to_sample at 0x7717f333a5c0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "random_sampling",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "agieval:gaokao-history",
        "config": {
          "name": "agieval:gaokao-history",
          "prompt_function": "<function agieval_prompt at 0x7717f333a660>",
          "hf_repo": "dmayhem93/agieval-gaokao-history",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b5be00>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f333b2e0>]",
          "scorer": "<function choice.<locals>.score at 0x7717f333b380>",
          "sample_fields": "<function record_to_sample at 0x7717f333a5c0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "random_sampling",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "agieval:gaokao-mathqa",
        "config": {
          "name": "agieval:gaokao-mathqa",
          "prompt_function": "<function agieval_prompt at 0x7717f333a660>",
          "hf_repo": "dmayhem93/agieval-gaokao-mathqa",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b640b0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f333b4c0>]",
          "scorer": "<function choice.<locals>.score at 0x7717f333b560>",
          "sample_fields": "<function record_to_sample at 0x7717f333a5c0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "random_sampling",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "agieval:gaokao-physics",
        "config": {
          "name": "agieval:gaokao-physics",
          "prompt_function": "<function agieval_prompt at 0x7717f333a660>",
          "hf_repo": "dmayhem93/agieval-gaokao-physics",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b64350>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f333b6a0>]",
          "scorer": "<function choice.<locals>.score at 0x7717f333b740>",
          "sample_fields": "<function record_to_sample at 0x7717f333a5c0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "random_sampling",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "agieval:logiqa-en",
        "config": {
          "name": "agieval:logiqa-en",
          "prompt_function": "<function agieval_prompt at 0x7717f333a660>",
          "hf_repo": "dmayhem93/agieval-logiqa-en",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b645f0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f333b880>]",
          "scorer": "<function choice.<locals>.score at 0x7717f333b920>",
          "sample_fields": "<function record_to_sample at 0x7717f333a5c0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "random_sampling",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "agieval:logiqa-zh",
        "config": {
          "name": "agieval:logiqa-zh",
          "prompt_function": "<function agieval_prompt at 0x7717f333a660>",
          "hf_repo": "dmayhem93/agieval-logiqa-zh",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b64890>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f333ba60>]",
          "scorer": "<function choice.<locals>.score at 0x7717f333bb00>",
          "sample_fields": "<function record_to_sample at 0x7717f333a5c0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "random_sampling",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "agieval:lsat-ar",
        "config": {
          "name": "agieval:lsat-ar",
          "prompt_function": "<function agieval_prompt at 0x7717f333a660>",
          "hf_repo": "dmayhem93/agieval-lsat-ar",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b64b30>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f333bc40>]",
          "scorer": "<function choice.<locals>.score at 0x7717f333bce0>",
          "sample_fields": "<function record_to_sample at 0x7717f333a5c0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "random_sampling",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "agieval:lsat-lr",
        "config": {
          "name": "agieval:lsat-lr",
          "prompt_function": "<function agieval_prompt at 0x7717f333a660>",
          "hf_repo": "dmayhem93/agieval-lsat-lr",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b64dd0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f333be20>]",
          "scorer": "<function choice.<locals>.score at 0x7717f333bec0>",
          "sample_fields": "<function record_to_sample at 0x7717f333a5c0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "random_sampling",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "agieval:lsat-rc",
        "config": {
          "name": "agieval:lsat-rc",
          "prompt_function": "<function agieval_prompt at 0x7717f333a660>",
          "hf_repo": "dmayhem93/agieval-lsat-rc",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b65070>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f333bf60>]",
          "scorer": "<function choice.<locals>.score at 0x7717f33640e0>",
          "sample_fields": "<function record_to_sample at 0x7717f333a5c0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "random_sampling",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "agieval:sat-en",
        "config": {
          "name": "agieval:sat-en",
          "prompt_function": "<function agieval_prompt at 0x7717f333a660>",
          "hf_repo": "dmayhem93/agieval-sat-en",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b65310>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f3364220>]",
          "scorer": "<function choice.<locals>.score at 0x7717f33642c0>",
          "sample_fields": "<function record_to_sample at 0x7717f333a5c0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "random_sampling",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "agieval:sat-en-without-passage",
        "config": {
          "name": "agieval:sat-en-without-passage",
          "prompt_function": "<function agieval_prompt at 0x7717f333a660>",
          "hf_repo": "dmayhem93/agieval-sat-en-without-passage",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b655b0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f3364400>]",
          "scorer": "<function choice.<locals>.score at 0x7717f33644a0>",
          "sample_fields": "<function record_to_sample at 0x7717f333a5c0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "random_sampling",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "agieval:sat-math",
        "config": {
          "name": "agieval:sat-math",
          "prompt_function": "<function agieval_prompt at 0x7717f333a660>",
          "hf_repo": "dmayhem93/agieval-sat-math",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b65850>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f33645e0>]",
          "scorer": "<function choice.<locals>.score at 0x7717f3364680>",
          "sample_fields": "<function record_to_sample at 0x7717f333a5c0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "random_sampling",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.siqa",
    "docstring": {
      "name": "Siqa",
      "dataset": [
        "allenai/social_i_qa"
      ],
      "abstract": "We introduce Social IQa: Social Interaction QA, a new question-answering\nbenchmark for testing social commonsense intelligence. Contrary to many prior\nbenchmarks that focus on physical or taxonomic knowledge, Social IQa focuses on\nreasoning about people's actions and their social implications. For example,\ngiven an action like \"Jesse saw a concert\" and a question like \"Why did Jesse do\nthis?\", humans can easily infer that Jesse wanted \"to see their favorite\nperformer\" or \"to enjoy the music\", and not \"to see what's happening inside\" or\n\"to see if it works\". The actions in Social IQa span a wide variety of social\nsituations, and answer candidates contain both human-curated answers and\nadversarially-filtered machine-generated candidates. Social IQa contains over\n37,000 QA pairs for evaluating models' abilities to reason about the social\nimplications of everyday events and situations.",
      "languages": [
        "english"
      ],
      "tags": [
        "commonsense",
        "multiple-choice",
        "qa"
      ]
    },
    "tasks": [
      {
        "name": "siqa",
        "config": {
          "name": "siqa",
          "prompt_function": "<function siqa_prompt at 0x7717f3364720>",
          "hf_repo": "allenai/social_i_qa",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b65c10>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'validation')",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.long_horizon_execution",
    "docstring": {
      "name": "Long Horizon Execution",
      "dataset": [
        "arvindh75/Long-Horizon-Execution"
      ],
      "abstract": "Evaluation benchmark for long-context execution capabilities of language models.\nTests a model's ability to maintain state and perform cumulative operations over\nlong sequences of inputs. Supports both single-turn (all inputs at once) and\nmulti-turn (inputs provided incrementally) evaluation modes.",
      "The task requires models to": "Multi-turn evaluation: Model processes keys in batches of K per turn, maintaining\nconversation history and outputting cumulative sums incrementally. Evaluates\nfractional accuracy (correct turns / total turns).",
      "languages": [
        "english"
      ],
      "tags": [
        "long-context",
        "state-tracking",
        "arithmetic",
        "execution"
      ],
      "paper": "https://arxiv.org/abs/2509.09677",
      "starred": "true"
    },
    "tasks": [
      {
        "name": "long_horizon_execution",
        "config": {
          "name": "long_horizon_execution",
          "prompt_function": "<function <lambda> at 0x7717f3364cc0>",
          "hf_repo": "arvindh75/Long-Horizon-Execution",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b660c0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function solver.<locals>.solve at 0x7717f3364e00>]",
          "scorer": "[<function scorer.<locals>.score at 0x7717f3364f40>]",
          "sample_fields": "functools.partial(<function record_to_sample at 0x7717f3364860>, k=10, max_turns=30)",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'validation', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "None",
          "generation_grammar": "None",
          "stop_sequence": "()",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.swag",
    "docstring": {
      "name": "Swag",
      "dataset": [
        "allenai/swag"
      ],
      "abstract": "The dataset consists of 113k multiple choice questions about grounded situations\n(73k training, 20k validation, 20k test). Each question is a video caption from\nLSMDC or ActivityNet Captions, with four answer choices about what might happen\nnext in the scene. The correct answer is the (real) video caption for the next\nevent in the video; the three incorrect answers are adversarially generated and\nhuman verified, so as to fool machines but not humans. SWAG aims to be a\nbenchmark for evaluating grounded commonsense NLI and for learning\nrepresentations.",
      "languages": [
        "english"
      ],
      "tags": [
        "narrative",
        "reasoning"
      ],
      "paper": "https://arxiv.org/abs/1808.05326"
    },
    "tasks": [
      {
        "name": "swag",
        "config": {
          "name": "swag",
          "prompt_function": "<function swag_prompt at 0x7717f3365080>",
          "hf_repo": "allenai/swag",
          "hf_subset": "regular",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b664b0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'validation')",
          "evaluation_splits": "('validation',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.jeopardy",
    "docstring": {
      "name": "Jeopardy",
      "dataset": [
        "openaccess-ai-collective/jeopardy"
      ],
      "abstract": "Jeopardy is a dataset of questions and answers from the Jeopardy game show.",
      "languages": [
        "english"
      ],
      "tags": [
        "knowledge",
        "qa"
      ]
    },
    "tasks": [
      {
        "name": "jeopardy",
        "config": {
          "name": "jeopardy",
          "prompt_function": "<function get_mcq_prompt_function.<locals>.prompt_fn at 0x7717f33651c0>",
          "hf_repo": "openaccess-ai-collective/jeopardy",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b66900>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'validation', 'test')",
          "evaluation_splits": "('train',)",
          "few_shots_split": "train",
          "few_shots_select": "None",
          "generation_size": "250",
          "generation_grammar": "None",
          "stop_sequence": "['\\n', 'Question:', 'question:']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "1"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.bigbench_extra_hard",
    "docstring": {
      "name": "BIG-Bench Extra Hard",
      "dataset": [
        "jgyasu/bbeh"
      ],
      "abstract": "BIG-Bench Extra Hard (BBEH) is a successor to BIG-Bench Hard (BBH), created to evaluate large\nlanguage models on substantially more difficult general-reasoning tasks. Each BBH task is replaced\nwith a new task targeting the same underlying reasoning skill but at a significantly higher difficulty.",
      "languages": [
        "english"
      ],
      "tags": [
        "reasoning"
      ],
      "paper": "https://arxiv.org/abs/2502.19187",
      "starred": "true"
    },
    "tasks": [
      {
        "name": "bigbench_extra_hard:boardgame_qa",
        "config": {
          "name": "bigbench_extra_hard:boardgame_qa",
          "prompt_function": "<function bbeh_prompt at 0x7717f3365300>",
          "hf_repo": "jgyasu/bbeh",
          "hf_subset": "boardgame_qa",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b66d50>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function system_message.<locals>.solve at 0x7717f3365440>, <function generate.<locals>.solve at 0x7717f3365580>]",
          "scorer": "<function pattern.<locals>.score at 0x7717f33656c0>",
          "sample_fields": "<function record_to_sample at 0x7717f33653a0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['</s>', 'Q=', '\\n\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench_extra_hard:boolean_expressions",
        "config": {
          "name": "bigbench_extra_hard:boolean_expressions",
          "prompt_function": "<function bbeh_prompt at 0x7717f3365300>",
          "hf_repo": "jgyasu/bbeh",
          "hf_subset": "boolean_expressions",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b66ff0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function system_message.<locals>.solve at 0x7717f3365440>, <function generate.<locals>.solve at 0x7717f3365580>]",
          "scorer": "<function pattern.<locals>.score at 0x7717f33656c0>",
          "sample_fields": "<function record_to_sample at 0x7717f33653a0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['</s>', 'Q=', '\\n\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench_extra_hard:buggy_tables",
        "config": {
          "name": "bigbench_extra_hard:buggy_tables",
          "prompt_function": "<function bbeh_prompt at 0x7717f3365300>",
          "hf_repo": "jgyasu/bbeh",
          "hf_subset": "buggy_tables",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b67290>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function system_message.<locals>.solve at 0x7717f3365440>, <function generate.<locals>.solve at 0x7717f3365580>]",
          "scorer": "<function pattern.<locals>.score at 0x7717f33656c0>",
          "sample_fields": "<function record_to_sample at 0x7717f33653a0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['</s>', 'Q=', '\\n\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench_extra_hard:causal_understanding",
        "config": {
          "name": "bigbench_extra_hard:causal_understanding",
          "prompt_function": "<function bbeh_prompt at 0x7717f3365300>",
          "hf_repo": "jgyasu/bbeh",
          "hf_subset": "causal_understanding",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b67530>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function system_message.<locals>.solve at 0x7717f3365440>, <function generate.<locals>.solve at 0x7717f3365580>]",
          "scorer": "<function pattern.<locals>.score at 0x7717f33656c0>",
          "sample_fields": "<function record_to_sample at 0x7717f33653a0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['</s>', 'Q=', '\\n\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench_extra_hard:disambiguation_qa",
        "config": {
          "name": "bigbench_extra_hard:disambiguation_qa",
          "prompt_function": "<function bbeh_prompt at 0x7717f3365300>",
          "hf_repo": "jgyasu/bbeh",
          "hf_subset": "disambiguation_qa",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b677d0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function system_message.<locals>.solve at 0x7717f3365440>, <function generate.<locals>.solve at 0x7717f3365580>]",
          "scorer": "<function pattern.<locals>.score at 0x7717f33656c0>",
          "sample_fields": "<function record_to_sample at 0x7717f33653a0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['</s>', 'Q=', '\\n\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench_extra_hard:dyck_languages",
        "config": {
          "name": "bigbench_extra_hard:dyck_languages",
          "prompt_function": "<function bbeh_prompt at 0x7717f3365300>",
          "hf_repo": "jgyasu/bbeh",
          "hf_subset": "dyck_languages",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b67a70>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function system_message.<locals>.solve at 0x7717f3365440>, <function generate.<locals>.solve at 0x7717f3365580>]",
          "scorer": "<function pattern.<locals>.score at 0x7717f33656c0>",
          "sample_fields": "<function record_to_sample at 0x7717f33653a0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['</s>', 'Q=', '\\n\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench_extra_hard:geometric_shapes",
        "config": {
          "name": "bigbench_extra_hard:geometric_shapes",
          "prompt_function": "<function bbeh_prompt at 0x7717f3365300>",
          "hf_repo": "jgyasu/bbeh",
          "hf_subset": "geometric_shapes",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b67d10>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function system_message.<locals>.solve at 0x7717f3365440>, <function generate.<locals>.solve at 0x7717f3365580>]",
          "scorer": "<function pattern.<locals>.score at 0x7717f33656c0>",
          "sample_fields": "<function record_to_sample at 0x7717f33653a0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['</s>', 'Q=', '\\n\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench_extra_hard:hyperbaton",
        "config": {
          "name": "bigbench_extra_hard:hyperbaton",
          "prompt_function": "<function bbeh_prompt at 0x7717f3365300>",
          "hf_repo": "jgyasu/bbeh",
          "hf_subset": "hyperbaton",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b67fb0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function system_message.<locals>.solve at 0x7717f3365440>, <function generate.<locals>.solve at 0x7717f3365580>]",
          "scorer": "<function pattern.<locals>.score at 0x7717f33656c0>",
          "sample_fields": "<function record_to_sample at 0x7717f33653a0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['</s>', 'Q=', '\\n\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench_extra_hard:linguini",
        "config": {
          "name": "bigbench_extra_hard:linguini",
          "prompt_function": "<function bbeh_prompt at 0x7717f3365300>",
          "hf_repo": "jgyasu/bbeh",
          "hf_subset": "linguini",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b70290>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function system_message.<locals>.solve at 0x7717f3365440>, <function generate.<locals>.solve at 0x7717f3365580>]",
          "scorer": "<function pattern.<locals>.score at 0x7717f33656c0>",
          "sample_fields": "<function record_to_sample at 0x7717f33653a0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['</s>', 'Q=', '\\n\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench_extra_hard:movie_recommendation",
        "config": {
          "name": "bigbench_extra_hard:movie_recommendation",
          "prompt_function": "<function bbeh_prompt at 0x7717f3365300>",
          "hf_repo": "jgyasu/bbeh",
          "hf_subset": "movie_recommendation",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b70530>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function system_message.<locals>.solve at 0x7717f3365440>, <function generate.<locals>.solve at 0x7717f3365580>]",
          "scorer": "<function pattern.<locals>.score at 0x7717f33656c0>",
          "sample_fields": "<function record_to_sample at 0x7717f33653a0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['</s>', 'Q=', '\\n\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench_extra_hard:multistep_arithmetic",
        "config": {
          "name": "bigbench_extra_hard:multistep_arithmetic",
          "prompt_function": "<function bbeh_prompt at 0x7717f3365300>",
          "hf_repo": "jgyasu/bbeh",
          "hf_subset": "multistep_arithmetic",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b707d0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function system_message.<locals>.solve at 0x7717f3365440>, <function generate.<locals>.solve at 0x7717f3365580>]",
          "scorer": "<function pattern.<locals>.score at 0x7717f33656c0>",
          "sample_fields": "<function record_to_sample at 0x7717f33653a0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['</s>', 'Q=', '\\n\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench_extra_hard:nycc",
        "config": {
          "name": "bigbench_extra_hard:nycc",
          "prompt_function": "<function bbeh_prompt at 0x7717f3365300>",
          "hf_repo": "jgyasu/bbeh",
          "hf_subset": "nycc",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b70a70>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function system_message.<locals>.solve at 0x7717f3365440>, <function generate.<locals>.solve at 0x7717f3365580>]",
          "scorer": "<function pattern.<locals>.score at 0x7717f33656c0>",
          "sample_fields": "<function record_to_sample at 0x7717f33653a0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['</s>', 'Q=', '\\n\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench_extra_hard:object_counting",
        "config": {
          "name": "bigbench_extra_hard:object_counting",
          "prompt_function": "<function bbeh_prompt at 0x7717f3365300>",
          "hf_repo": "jgyasu/bbeh",
          "hf_subset": "object_counting",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b70d10>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function system_message.<locals>.solve at 0x7717f3365440>, <function generate.<locals>.solve at 0x7717f3365580>]",
          "scorer": "<function pattern.<locals>.score at 0x7717f33656c0>",
          "sample_fields": "<function record_to_sample at 0x7717f33653a0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['</s>', 'Q=', '\\n\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench_extra_hard:object_properties",
        "config": {
          "name": "bigbench_extra_hard:object_properties",
          "prompt_function": "<function bbeh_prompt at 0x7717f3365300>",
          "hf_repo": "jgyasu/bbeh",
          "hf_subset": "object_properties",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b70fb0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function system_message.<locals>.solve at 0x7717f3365440>, <function generate.<locals>.solve at 0x7717f3365580>]",
          "scorer": "<function pattern.<locals>.score at 0x7717f33656c0>",
          "sample_fields": "<function record_to_sample at 0x7717f33653a0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['</s>', 'Q=', '\\n\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench_extra_hard:sarc_triples",
        "config": {
          "name": "bigbench_extra_hard:sarc_triples",
          "prompt_function": "<function bbeh_prompt at 0x7717f3365300>",
          "hf_repo": "jgyasu/bbeh",
          "hf_subset": "sarc_triples",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b71250>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function system_message.<locals>.solve at 0x7717f3365440>, <function generate.<locals>.solve at 0x7717f3365580>]",
          "scorer": "<function pattern.<locals>.score at 0x7717f33656c0>",
          "sample_fields": "<function record_to_sample at 0x7717f33653a0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['</s>', 'Q=', '\\n\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench_extra_hard:shuffled_objects",
        "config": {
          "name": "bigbench_extra_hard:shuffled_objects",
          "prompt_function": "<function bbeh_prompt at 0x7717f3365300>",
          "hf_repo": "jgyasu/bbeh",
          "hf_subset": "shuffled_objects",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b714f0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function system_message.<locals>.solve at 0x7717f3365440>, <function generate.<locals>.solve at 0x7717f3365580>]",
          "scorer": "<function pattern.<locals>.score at 0x7717f33656c0>",
          "sample_fields": "<function record_to_sample at 0x7717f33653a0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['</s>', 'Q=', '\\n\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench_extra_hard:spatial_reasoning",
        "config": {
          "name": "bigbench_extra_hard:spatial_reasoning",
          "prompt_function": "<function bbeh_prompt at 0x7717f3365300>",
          "hf_repo": "jgyasu/bbeh",
          "hf_subset": "spatial_reasoning",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b71790>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function system_message.<locals>.solve at 0x7717f3365440>, <function generate.<locals>.solve at 0x7717f3365580>]",
          "scorer": "<function pattern.<locals>.score at 0x7717f33656c0>",
          "sample_fields": "<function record_to_sample at 0x7717f33653a0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['</s>', 'Q=', '\\n\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench_extra_hard:sportqa",
        "config": {
          "name": "bigbench_extra_hard:sportqa",
          "prompt_function": "<function bbeh_prompt at 0x7717f3365300>",
          "hf_repo": "jgyasu/bbeh",
          "hf_subset": "sportqa",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b71a30>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function system_message.<locals>.solve at 0x7717f3365440>, <function generate.<locals>.solve at 0x7717f3365580>]",
          "scorer": "<function pattern.<locals>.score at 0x7717f33656c0>",
          "sample_fields": "<function record_to_sample at 0x7717f33653a0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['</s>', 'Q=', '\\n\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench_extra_hard:temporal_sequence",
        "config": {
          "name": "bigbench_extra_hard:temporal_sequence",
          "prompt_function": "<function bbeh_prompt at 0x7717f3365300>",
          "hf_repo": "jgyasu/bbeh",
          "hf_subset": "temporal_sequence",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b71cd0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function system_message.<locals>.solve at 0x7717f3365440>, <function generate.<locals>.solve at 0x7717f3365580>]",
          "scorer": "<function pattern.<locals>.score at 0x7717f33656c0>",
          "sample_fields": "<function record_to_sample at 0x7717f33653a0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['</s>', 'Q=', '\\n\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench_extra_hard:time_arithmetic",
        "config": {
          "name": "bigbench_extra_hard:time_arithmetic",
          "prompt_function": "<function bbeh_prompt at 0x7717f3365300>",
          "hf_repo": "jgyasu/bbeh",
          "hf_subset": "time_arithmetic",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b71f70>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function system_message.<locals>.solve at 0x7717f3365440>, <function generate.<locals>.solve at 0x7717f3365580>]",
          "scorer": "<function pattern.<locals>.score at 0x7717f33656c0>",
          "sample_fields": "<function record_to_sample at 0x7717f33653a0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['</s>', 'Q=', '\\n\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench_extra_hard:web_of_lies",
        "config": {
          "name": "bigbench_extra_hard:web_of_lies",
          "prompt_function": "<function bbeh_prompt at 0x7717f3365300>",
          "hf_repo": "jgyasu/bbeh",
          "hf_subset": "web_of_lies",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b72210>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function system_message.<locals>.solve at 0x7717f3365440>, <function generate.<locals>.solve at 0x7717f3365580>]",
          "scorer": "<function pattern.<locals>.score at 0x7717f33656c0>",
          "sample_fields": "<function record_to_sample at 0x7717f33653a0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['</s>', 'Q=', '\\n\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench_extra_hard:word_sorting",
        "config": {
          "name": "bigbench_extra_hard:word_sorting",
          "prompt_function": "<function bbeh_prompt at 0x7717f3365300>",
          "hf_repo": "jgyasu/bbeh",
          "hf_subset": "word_sorting",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b724b0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function system_message.<locals>.solve at 0x7717f3365440>, <function generate.<locals>.solve at 0x7717f3365580>]",
          "scorer": "<function pattern.<locals>.score at 0x7717f33656c0>",
          "sample_fields": "<function record_to_sample at 0x7717f33653a0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['</s>', 'Q=', '\\n\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "bigbench_extra_hard:zebra_puzzles",
        "config": {
          "name": "bigbench_extra_hard:zebra_puzzles",
          "prompt_function": "<function bbeh_prompt at 0x7717f3365300>",
          "hf_repo": "jgyasu/bbeh",
          "hf_subset": "zebra_puzzles",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b72750>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function system_message.<locals>.solve at 0x7717f3365440>, <function generate.<locals>.solve at 0x7717f3365580>]",
          "scorer": "<function pattern.<locals>.score at 0x7717f33656c0>",
          "sample_fields": "<function record_to_sample at 0x7717f33653a0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['</s>', 'Q=', '\\n\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.musr",
    "docstring": {
      "name": "Musr",
      "dataset": [
        "TAUR-Lab/MuSR"
      ],
      "abstract": "MuSR is a benchmark for evaluating multistep reasoning in natural language\nnarratives. Built using a neurosymbolic synthetic-to-natural generation process,\nit features complex, realistic tasks\u2014such as long-form murder mysteries.",
      "languages": [
        "english"
      ],
      "tags": [
        "long-context",
        "multiple-choice",
        "reasoning"
      ],
      "paper": "https://arxiv.org/abs/2310.16049",
      "starred": "true"
    },
    "tasks": [
      {
        "name": "musr:murder_mysteries",
        "config": {
          "name": "musr:murder_mysteries",
          "prompt_function": "<function musr_prompt at 0x7717f3365760>",
          "hf_repo": "TAUR-Lab/MuSR",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b72b70>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f33658a0>]",
          "scorer": "<function choice.<locals>.score at 0x7717f33659e0>",
          "sample_fields": "<function record_to_sample at 0x7717f3365800>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('murder_mysteries',)",
          "evaluation_splits": "('murder_mysteries',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "musr:object_placements",
        "config": {
          "name": "musr:object_placements",
          "prompt_function": "<function musr_prompt at 0x7717f3365760>",
          "hf_repo": "TAUR-Lab/MuSR",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b72e40>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f3365b20>]",
          "scorer": "<function choice.<locals>.score at 0x7717f3365bc0>",
          "sample_fields": "<function record_to_sample at 0x7717f3365800>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('object_placements',)",
          "evaluation_splits": "('object_placements',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "musr:team_allocation",
        "config": {
          "name": "musr:team_allocation",
          "prompt_function": "<function musr_prompt at 0x7717f3365760>",
          "hf_repo": "TAUR-Lab/MuSR",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b73110>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f3365d00>]",
          "scorer": "<function choice.<locals>.score at 0x7717f3365da0>",
          "sample_fields": "<function record_to_sample at 0x7717f3365800>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('team_allocation',)",
          "evaluation_splits": "('team_allocation',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.sciq",
    "docstring": {
      "name": "Sciq",
      "dataset": [
        "allenai/sciq"
      ],
      "abstract": "The SciQ dataset contains 13,679 crowdsourced science exam questions about\nPhysics, Chemistry and Biology, among others. The questions are in\nmultiple-choice format with 4 answer options each. For the majority of the\nquestions, an additional paragraph with supporting evidence for the correct\nanswer is provided.",
      "languages": [
        "english"
      ],
      "tags": [
        "physics",
        "chemistry",
        "biology",
        "reasoning",
        "multiple-choice",
        "qa"
      ],
      "paper": "https://arxiv.org/abs/1707.06209"
    },
    "tasks": [
      {
        "name": "sciq",
        "config": {
          "name": "sciq",
          "prompt_function": "<function sciq_prompt at 0x7717f3365e40>",
          "hf_repo": "allenai/sciq",
          "hf_subset": "default",
          "metrics": "({'metric_name': 'acc', 'higher_is_better': True, 'category': <SamplingMethod.LOGPROBS: 'LOGPROBS'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7717f0b73560>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train', 'validation', 'test')",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "-1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.pubmedqa",
    "docstring": {
      "name": "Pubmedqa",
      "dataset": [
        "pubmed_qa"
      ],
      "abstract": "PubMedQA is a dataset for biomedical research question answering.",
      "languages": [
        "english"
      ],
      "tags": [
        "biomedical",
        "health",
        "medical",
        "qa"
      ],
      "paper": "https://pubmedqa.github.io/"
    },
    "tasks": [
      {
        "name": "pubmedqa",
        "config": {
          "name": "pubmedqa",
          "prompt_function": "<function pubmed_qa_prompt at 0x7717f3366020>",
          "hf_repo": "pubmed_qa",
          "hf_subset": "pqa_labeled",
          "metrics": "({'metric_name': 'em', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.ExactMatches object at 0x7717f0b73a40>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "1",
          "generation_grammar": "None",
          "stop_sequence": "['\\n']",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.ifbench.main",
    "docstring": {
      "name": "IFBench",
      "dataset": [
        "allenai/IFBench_test",
        "allenai/IFBench_multi-turn"
      ],
      "abstract": "Challenging benchmark for precise instruction following.",
      "languages": [
        "english"
      ],
      "tags": [
        "instruction-following"
      ],
      "paper": "https://arxiv.org/abs/2507.02833",
      "starred": "true"
    },
    "tasks": [
      {
        "name": "ifbench_test",
        "config": {
          "name": "ifbench_test",
          "prompt_function": "<function ifbench_prompt at 0x7717f33660c0>",
          "hf_repo": "allenai/IFBench_test",
          "hf_subset": "default",
          "metrics": "({'metric_name': ['prompt_level_strict_acc', 'inst_level_strict_acc', 'prompt_level_loose_acc', 'inst_level_loose_acc'], 'higher_is_better': {'prompt_level_strict_acc': True, 'inst_level_strict_acc': True, 'prompt_level_loose_acc': True, 'inst_level_loose_acc': True}, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.tasks.tasks.ifbench.main.IFBench object at 0x7717f0b73ef0>, 'corpus_level_fn': {'prompt_level_strict_acc': <function mean at 0x771aa6d5b3b0>, 'inst_level_strict_acc': <function agg_inst_level_acc at 0x7717f3366340>, 'prompt_level_loose_acc': <function mean at 0x771aa6d5b3b0>, 'inst_level_loose_acc': <function agg_inst_level_acc at 0x7717f3366340>}, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function generate.<locals>.solve at 0x7717f0d50860>]",
          "scorer": "<function ifbench_scorer.<locals>.score at 0x7717f0d50a40>",
          "sample_fields": "<function record_to_sample at 0x7717f0d50540>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "train",
          "few_shots_select": "random_sampling",
          "generation_size": "1280",
          "generation_grammar": "None",
          "stop_sequence": "[]",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0.1"
        }
      },
      {
        "name": "ifbench_multiturn",
        "config": {
          "name": "ifbench_multiturn",
          "prompt_function": "<function ifbench_prompt at 0x7717f33660c0>",
          "hf_repo": "allenai/IFBench_multi-turn",
          "hf_subset": "ifbench_constraints",
          "metrics": "({'metric_name': ['prompt_level_strict_acc', 'inst_level_strict_acc', 'prompt_level_loose_acc', 'inst_level_loose_acc'], 'higher_is_better': {'prompt_level_strict_acc': True, 'inst_level_strict_acc': True, 'prompt_level_loose_acc': True, 'inst_level_loose_acc': True}, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.tasks.tasks.ifbench.main.IFBench object at 0x7717f0b78170>, 'corpus_level_fn': {'prompt_level_strict_acc': <function mean at 0x771aa6d5b3b0>, 'inst_level_strict_acc': <function agg_inst_level_acc at 0x7717f3366340>, 'prompt_level_loose_acc': <function mean at 0x771aa6d5b3b0>, 'inst_level_loose_acc': <function agg_inst_level_acc at 0x7717f3366340>}, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function generate.<locals>.solve at 0x7717f0d50b80>]",
          "scorer": "<function ifbench_scorer.<locals>.score at 0x7717f0d50c20>",
          "sample_fields": "<function record_to_sample at 0x7717f0d50540>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "test",
          "few_shots_select": "random_sampling",
          "generation_size": "1280",
          "generation_grammar": "None",
          "stop_sequence": "[]",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0.1"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.ifeval.main",
    "docstring": {
      "name": "IFEval",
      "dataset": [
        "google/IFEval"
      ],
      "abstract": "Very specific task where there are no precise outputs but instead we test if the\nformat obeys rules.",
      "languages": [
        "english"
      ],
      "tags": [
        "instruction-following"
      ],
      "paper": "https://arxiv.org/abs/2311.07911",
      "starred": "true"
    },
    "tasks": [
      {
        "name": "ifeval",
        "config": {
          "name": "ifeval",
          "prompt_function": "<function ifeval_prompt at 0x7717f0d50fe0>",
          "hf_repo": "google/IFEval",
          "hf_subset": "default",
          "metrics": "({'metric_name': ['prompt_level_strict_acc', 'inst_level_strict_acc', 'prompt_level_loose_acc', 'inst_level_loose_acc'], 'higher_is_better': {'prompt_level_strict_acc': True, 'inst_level_strict_acc': True, 'prompt_level_loose_acc': True, 'inst_level_loose_acc': True}, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.tasks.tasks.ifeval.main.IFEvalMetrics object at 0x7717f0b78530>, 'corpus_level_fn': {'prompt_level_strict_acc': <function mean at 0x771aa6d5b3b0>, 'inst_level_strict_acc': <function agg_inst_level_acc at 0x7717f0dd8e00>, 'prompt_level_loose_acc': <function mean at 0x771aa6d5b3b0>, 'inst_level_loose_acc': <function agg_inst_level_acc at 0x7717f0dd8e00>}, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "[<function generate.<locals>.solve at 0x7717f0dd9260>]",
          "scorer": "<function ifeval_scorer.<locals>.score at 0x7717f0dd9440>",
          "sample_fields": "<function record_to_sample at 0x7717f0dd8f40>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "train",
          "few_shots_select": "random_sampling",
          "generation_size": "1280",
          "generation_grammar": "None",
          "stop_sequence": "[]",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0.1"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.mt_bench.main",
    "docstring": {
      "name": "Mt Bench",
      "dataset": [
        "lighteval/mt-bench"
      ],
      "abstract": "MT-Bench is a multi-turn conversational benchmark for evaluating language\nmodels. It consists of 80 high-quality multi-turn questions across 8 common\ncategories (writing, roleplay, reasoning, math, coding, extraction, STEM,\nhumanities). Model responses are evaluated by a judge LLM.",
      "languages": [
        "english"
      ],
      "tags": [
        "conversational",
        "generation",
        "multi-turn"
      ],
      "paper": "https://arxiv.org/abs/2402.14762"
    },
    "tasks": [
      {
        "name": "mt_bench",
        "config": {
          "name": "mt_bench",
          "prompt_function": "<function mt_bench_prompt at 0x7717f0dd94e0>",
          "hf_repo": "lighteval/mt-bench",
          "hf_subset": "default",
          "metrics": "({'metric_name': ['judge_score_turn_1', 'judge_score_turn_2'], 'higher_is_better': {'judge_score_turn_1': True, 'judge_score_turn_2': True}, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.JudgeLLMMTBench object at 0x7717f0b78860>, 'corpus_level_fn': {'judge_score_turn_1': <function mean at 0x771aa6d5b3b0>, 'judge_score_turn_2': <function mean at 0x771aa6d5b3b0>}, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('train',)",
          "evaluation_splits": "('train',)",
          "few_shots_split": "",
          "few_shots_select": "random",
          "generation_size": "1024",
          "generation_grammar": "None",
          "stop_sequence": "[]",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.lcb.main",
    "docstring": {
      "name": "Live Code Bench",
      "dataset": [
        "lighteval/code_generation_lite"
      ],
      "abstract": "LiveCodeBench collects problems from periodic contests on LeetCode, AtCoder, and\nCodeforces platforms and uses them for constructing a holistic benchmark for\nevaluating Code LLMs across variety of code-related scenarios continuously over\ntime.",
      "languages": [
        "english"
      ],
      "tags": [
        "code-generation"
      ],
      "paper": "https://livecodebench.github.io/",
      "starred": "true"
    },
    "tasks": [
      {
        "name": "lcb:codegeneration_release_v1",
        "config": {
          "name": "lcb:codegeneration_release_v1",
          "prompt_function": "<function lcb_codegeneration_prompt_fn at 0x7717f0dd9a80>",
          "hf_repo": "lighteval/code_generation_lite",
          "hf_subset": "release_v1",
          "metrics": "({'metric_name': 'codegen_pass@1:16', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.tasks.tasks.lcb.main.CodegenMetric object at 0x7717f0b78c80>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "32768",
          "generation_grammar": "None",
          "stop_sequence": "[]",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lcb:codegeneration_release_v2",
        "config": {
          "name": "lcb:codegeneration_release_v2",
          "prompt_function": "<function lcb_codegeneration_prompt_fn at 0x7717f0dd9a80>",
          "hf_repo": "lighteval/code_generation_lite",
          "hf_subset": "release_v2",
          "metrics": "({'metric_name': 'codegen_pass@1:16', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.tasks.tasks.lcb.main.CodegenMetric object at 0x7717f0b78fb0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "32768",
          "generation_grammar": "None",
          "stop_sequence": "[]",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lcb:codegeneration_release_v3",
        "config": {
          "name": "lcb:codegeneration_release_v3",
          "prompt_function": "<function lcb_codegeneration_prompt_fn at 0x7717f0dd9a80>",
          "hf_repo": "lighteval/code_generation_lite",
          "hf_subset": "release_v3",
          "metrics": "({'metric_name': 'codegen_pass@1:16', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.tasks.tasks.lcb.main.CodegenMetric object at 0x7717f0b79310>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "32768",
          "generation_grammar": "None",
          "stop_sequence": "[]",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lcb:codegeneration_release_v4",
        "config": {
          "name": "lcb:codegeneration_release_v4",
          "prompt_function": "<function lcb_codegeneration_prompt_fn at 0x7717f0dd9a80>",
          "hf_repo": "lighteval/code_generation_lite",
          "hf_subset": "release_v4",
          "metrics": "({'metric_name': 'codegen_pass@1:16', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.tasks.tasks.lcb.main.CodegenMetric object at 0x7717f0b79670>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "32768",
          "generation_grammar": "None",
          "stop_sequence": "[]",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lcb:codegeneration_release_v5",
        "config": {
          "name": "lcb:codegeneration_release_v5",
          "prompt_function": "<function lcb_codegeneration_prompt_fn at 0x7717f0dd9a80>",
          "hf_repo": "lighteval/code_generation_lite",
          "hf_subset": "release_v5",
          "metrics": "({'metric_name': 'codegen_pass@1:16', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.tasks.tasks.lcb.main.CodegenMetric object at 0x7717f0b799d0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "32768",
          "generation_grammar": "None",
          "stop_sequence": "[]",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lcb:codegeneration_release_v6",
        "config": {
          "name": "lcb:codegeneration_release_v6",
          "prompt_function": "<function lcb_codegeneration_prompt_fn at 0x7717f0dd9a80>",
          "hf_repo": "lighteval/code_generation_lite",
          "hf_subset": "release_v6",
          "metrics": "({'metric_name': 'codegen_pass@1:16', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.tasks.tasks.lcb.main.CodegenMetric object at 0x7717f0b79d30>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "32768",
          "generation_grammar": "None",
          "stop_sequence": "[]",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lcb:codegeneration_release_latest",
        "config": {
          "name": "lcb:codegeneration_release_latest",
          "prompt_function": "<function lcb_codegeneration_prompt_fn at 0x7717f0dd9a80>",
          "hf_repo": "lighteval/code_generation_lite",
          "hf_subset": "release_latest",
          "metrics": "({'metric_name': 'codegen_pass@1:16', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.tasks.tasks.lcb.main.CodegenMetric object at 0x7717f0b7a090>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "32768",
          "generation_grammar": "None",
          "stop_sequence": "[]",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lcb:codegeneration_v1",
        "config": {
          "name": "lcb:codegeneration_v1",
          "prompt_function": "<function lcb_codegeneration_prompt_fn at 0x7717f0dd9a80>",
          "hf_repo": "lighteval/code_generation_lite",
          "hf_subset": "v1",
          "metrics": "({'metric_name': 'codegen_pass@1:16', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.tasks.tasks.lcb.main.CodegenMetric object at 0x7717f0b7a3f0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "32768",
          "generation_grammar": "None",
          "stop_sequence": "[]",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lcb:codegeneration_v2",
        "config": {
          "name": "lcb:codegeneration_v2",
          "prompt_function": "<function lcb_codegeneration_prompt_fn at 0x7717f0dd9a80>",
          "hf_repo": "lighteval/code_generation_lite",
          "hf_subset": "v2",
          "metrics": "({'metric_name': 'codegen_pass@1:16', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.tasks.tasks.lcb.main.CodegenMetric object at 0x7717f0b7a750>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "32768",
          "generation_grammar": "None",
          "stop_sequence": "[]",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lcb:codegeneration_v3",
        "config": {
          "name": "lcb:codegeneration_v3",
          "prompt_function": "<function lcb_codegeneration_prompt_fn at 0x7717f0dd9a80>",
          "hf_repo": "lighteval/code_generation_lite",
          "hf_subset": "v3",
          "metrics": "({'metric_name': 'codegen_pass@1:16', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.tasks.tasks.lcb.main.CodegenMetric object at 0x7717f0b7aab0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "32768",
          "generation_grammar": "None",
          "stop_sequence": "[]",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lcb:codegeneration_v4",
        "config": {
          "name": "lcb:codegeneration_v4",
          "prompt_function": "<function lcb_codegeneration_prompt_fn at 0x7717f0dd9a80>",
          "hf_repo": "lighteval/code_generation_lite",
          "hf_subset": "v4",
          "metrics": "({'metric_name': 'codegen_pass@1:16', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.tasks.tasks.lcb.main.CodegenMetric object at 0x7717f0b7ae10>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "32768",
          "generation_grammar": "None",
          "stop_sequence": "[]",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lcb:codegeneration_v5",
        "config": {
          "name": "lcb:codegeneration_v5",
          "prompt_function": "<function lcb_codegeneration_prompt_fn at 0x7717f0dd9a80>",
          "hf_repo": "lighteval/code_generation_lite",
          "hf_subset": "v5",
          "metrics": "({'metric_name': 'codegen_pass@1:16', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.tasks.tasks.lcb.main.CodegenMetric object at 0x7717f0b7b170>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "32768",
          "generation_grammar": "None",
          "stop_sequence": "[]",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lcb:codegeneration_v6",
        "config": {
          "name": "lcb:codegeneration_v6",
          "prompt_function": "<function lcb_codegeneration_prompt_fn at 0x7717f0dd9a80>",
          "hf_repo": "lighteval/code_generation_lite",
          "hf_subset": "v6",
          "metrics": "({'metric_name': 'codegen_pass@1:16', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.tasks.tasks.lcb.main.CodegenMetric object at 0x7717f0b7b4d0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "32768",
          "generation_grammar": "None",
          "stop_sequence": "[]",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lcb:codegeneration_v1_v2",
        "config": {
          "name": "lcb:codegeneration_v1_v2",
          "prompt_function": "<function lcb_codegeneration_prompt_fn at 0x7717f0dd9a80>",
          "hf_repo": "lighteval/code_generation_lite",
          "hf_subset": "v1_v2",
          "metrics": "({'metric_name': 'codegen_pass@1:16', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.tasks.tasks.lcb.main.CodegenMetric object at 0x7717f0b7b830>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "32768",
          "generation_grammar": "None",
          "stop_sequence": "[]",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lcb:codegeneration_v1_v3",
        "config": {
          "name": "lcb:codegeneration_v1_v3",
          "prompt_function": "<function lcb_codegeneration_prompt_fn at 0x7717f0dd9a80>",
          "hf_repo": "lighteval/code_generation_lite",
          "hf_subset": "v1_v3",
          "metrics": "({'metric_name': 'codegen_pass@1:16', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.tasks.tasks.lcb.main.CodegenMetric object at 0x7717f0b7bb90>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "32768",
          "generation_grammar": "None",
          "stop_sequence": "[]",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lcb:codegeneration_v1_v4",
        "config": {
          "name": "lcb:codegeneration_v1_v4",
          "prompt_function": "<function lcb_codegeneration_prompt_fn at 0x7717f0dd9a80>",
          "hf_repo": "lighteval/code_generation_lite",
          "hf_subset": "v1_v4",
          "metrics": "({'metric_name': 'codegen_pass@1:16', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.tasks.tasks.lcb.main.CodegenMetric object at 0x7717f0b7bef0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "32768",
          "generation_grammar": "None",
          "stop_sequence": "[]",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lcb:codegeneration_v1_v5",
        "config": {
          "name": "lcb:codegeneration_v1_v5",
          "prompt_function": "<function lcb_codegeneration_prompt_fn at 0x7717f0dd9a80>",
          "hf_repo": "lighteval/code_generation_lite",
          "hf_subset": "v1_v5",
          "metrics": "({'metric_name': 'codegen_pass@1:16', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.tasks.tasks.lcb.main.CodegenMetric object at 0x7717f0b84290>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "32768",
          "generation_grammar": "None",
          "stop_sequence": "[]",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lcb:codegeneration_v2_v3",
        "config": {
          "name": "lcb:codegeneration_v2_v3",
          "prompt_function": "<function lcb_codegeneration_prompt_fn at 0x7717f0dd9a80>",
          "hf_repo": "lighteval/code_generation_lite",
          "hf_subset": "v2_v3",
          "metrics": "({'metric_name': 'codegen_pass@1:16', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.tasks.tasks.lcb.main.CodegenMetric object at 0x7717f0b845f0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "32768",
          "generation_grammar": "None",
          "stop_sequence": "[]",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lcb:codegeneration_v2_v4",
        "config": {
          "name": "lcb:codegeneration_v2_v4",
          "prompt_function": "<function lcb_codegeneration_prompt_fn at 0x7717f0dd9a80>",
          "hf_repo": "lighteval/code_generation_lite",
          "hf_subset": "v2_v4",
          "metrics": "({'metric_name': 'codegen_pass@1:16', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.tasks.tasks.lcb.main.CodegenMetric object at 0x7717f0b84950>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "32768",
          "generation_grammar": "None",
          "stop_sequence": "[]",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lcb:codegeneration_v2_v5",
        "config": {
          "name": "lcb:codegeneration_v2_v5",
          "prompt_function": "<function lcb_codegeneration_prompt_fn at 0x7717f0dd9a80>",
          "hf_repo": "lighteval/code_generation_lite",
          "hf_subset": "v2_v5",
          "metrics": "({'metric_name': 'codegen_pass@1:16', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.tasks.tasks.lcb.main.CodegenMetric object at 0x7717f0b84cb0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "32768",
          "generation_grammar": "None",
          "stop_sequence": "[]",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lcb:codegeneration_v3_v4",
        "config": {
          "name": "lcb:codegeneration_v3_v4",
          "prompt_function": "<function lcb_codegeneration_prompt_fn at 0x7717f0dd9a80>",
          "hf_repo": "lighteval/code_generation_lite",
          "hf_subset": "v3_v4",
          "metrics": "({'metric_name': 'codegen_pass@1:16', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.tasks.tasks.lcb.main.CodegenMetric object at 0x7717f0b85010>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "32768",
          "generation_grammar": "None",
          "stop_sequence": "[]",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lcb:codegeneration_v3_v5",
        "config": {
          "name": "lcb:codegeneration_v3_v5",
          "prompt_function": "<function lcb_codegeneration_prompt_fn at 0x7717f0dd9a80>",
          "hf_repo": "lighteval/code_generation_lite",
          "hf_subset": "v3_v5",
          "metrics": "({'metric_name': 'codegen_pass@1:16', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.tasks.tasks.lcb.main.CodegenMetric object at 0x7717f0b85370>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "32768",
          "generation_grammar": "None",
          "stop_sequence": "[]",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      },
      {
        "name": "lcb:codegeneration",
        "config": {
          "name": "lcb:codegeneration",
          "prompt_function": "<function lcb_codegeneration_prompt_fn at 0x7717f0dd9a80>",
          "hf_repo": "lighteval/code_generation_lite",
          "hf_subset": "v4_v5",
          "metrics": "({'metric_name': 'codegen_pass@1:16', 'higher_is_better': True, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.tasks.tasks.lcb.main.CodegenMetric object at 0x7717f0b856d0>, 'corpus_level_fn': <function mean at 0x771aa6d5b3b0>, 'batched_compute': False},)",
          "hf_data_files": "None",
          "solver": "None",
          "scorer": "None",
          "sample_fields": "None",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('test',)",
          "evaluation_splits": "('test',)",
          "few_shots_split": "None",
          "few_shots_select": "None",
          "generation_size": "32768",
          "generation_grammar": "None",
          "stop_sequence": "[]",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0"
        }
      }
    ]
  },
  {
    "module": "lighteval.tasks.tasks.mix_eval.main",
    "docstring": {
      "name": "Mix Eval",
      "dataset": [
        "MixEval/MixEval"
      ],
      "abstract": "Ground-truth-based dynamic benchmark derived from off-the-shelf benchmark\nmixtures, which evaluates LLMs with a highly capable model ranking (i.e., 0.96\ncorrelation with Chatbot Arena) while running locally and quickly (6% the time\nand cost of running MMLU), with its queries being stably and effortlessly\nupdated every month to avoid contamination.",
      "languages": [
        "english"
      ],
      "tags": [
        "general-knowledge",
        "reasoning",
        "qa"
      ],
      "paper": "https://mixeval.github.io/",
      "starred": "true"
    },
    "tasks": [
      {
        "name": "mixeval_easy:multichoice",
        "config": {
          "name": "mixeval_easy:multichoice",
          "prompt_function": "<function mixeval_multichoice_prompt at 0x7717f0ddb420>",
          "hf_repo": "MixEval/MixEval",
          "hf_subset": "MixEval",
          "metrics": "({'metric_name': ['llm_judge_mixeval_flow'], 'higher_is_better': {'judge_score_flow': True}, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.JudgeLLMMixEval object at 0x7717f0b85bb0>, 'corpus_level_fn': {'judge_score_flow': <function mean at 0x771aa6d5b3b0>}, 'batched_compute': True}, {'metric_name': ['llm_judge_mixeval_gpt3'], 'higher_is_better': {'judge_score_gpt-3.5': True}, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.JudgeLLMMixEval object at 0x7717f0b85b20>, 'corpus_level_fn': {'judge_score_gpt-3.5': <function mean at 0x771aa6d5b3b0>}, 'batched_compute': True})",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f0ddba60>]",
          "scorer": "<function choice.<locals>.score at 0x7717f0ddbc40>",
          "sample_fields": "<function record_to_sample_multichoice at 0x7717f0ddb560>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('multiple_choice',)",
          "evaluation_splits": "('multiple_choice',)",
          "few_shots_split": "None",
          "few_shots_select": "random_sampling",
          "generation_size": "100",
          "generation_grammar": "None",
          "stop_sequence": "[]",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0.1"
        }
      },
      {
        "name": "mixeval_easy:freeform",
        "config": {
          "name": "mixeval_easy:freeform",
          "prompt_function": "<function mixeval_freeform_prompt at 0x7717f0ddad40>",
          "hf_repo": "MixEval/MixEval",
          "hf_subset": "MixEval",
          "metrics": "({'metric_name': ['llm_judge_mixeval_flow'], 'higher_is_better': {'judge_score': True}, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.JudgeLLMMixEval object at 0x7717f0b85df0>, 'corpus_level_fn': {'judge_score_flow': <function mean_dv_5 at 0x7717f0ddb7e0>}, 'batched_compute': True}, {'metric_name': ['llm_judge_mixeval_gpt3'], 'higher_is_better': {'judge_score_gpt-3.5': True}, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.JudgeLLMMixEval object at 0x7717f0b85e50>, 'corpus_level_fn': {'judge_score_gpt-3.5': <function mean at 0x771aa6d5b3b0>}, 'batched_compute': True})",
          "hf_data_files": "None",
          "solver": "[<function generate.<locals>.solve at 0x7717f0ddb880>]",
          "scorer": "<function _model_graded_qa_single.<locals>.score at 0x7717f0ddb9c0>",
          "sample_fields": "<function record_to_sample_freeform at 0x7717f0ddb4c0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('free_form',)",
          "evaluation_splits": "('free_form',)",
          "few_shots_split": "None",
          "few_shots_select": "random_sampling",
          "generation_size": "100",
          "generation_grammar": "None",
          "stop_sequence": "[]",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0.1"
        }
      },
      {
        "name": "mixeval_hard:multichoice",
        "config": {
          "name": "mixeval_hard:multichoice",
          "prompt_function": "<function mixeval_multichoice_prompt at 0x7717f0ddb420>",
          "hf_repo": "MixEval/MixEval",
          "hf_subset": "MixEval_Hard",
          "metrics": "({'metric_name': ['llm_judge_mixeval_flow'], 'higher_is_better': {'judge_score_flow': True}, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.JudgeLLMMixEval object at 0x7717f0b86060>, 'corpus_level_fn': {'judge_score_flow': <function mean at 0x771aa6d5b3b0>}, 'batched_compute': True}, {'metric_name': ['llm_judge_mixeval_gpt3'], 'higher_is_better': {'judge_score_gpt-3.5': True}, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.JudgeLLMMixEval object at 0x7717f0b860c0>, 'corpus_level_fn': {'judge_score_gpt-3.5': <function mean at 0x771aa6d5b3b0>}, 'batched_compute': True})",
          "hf_data_files": "None",
          "solver": "[<function multiple_choice.<locals>.solve at 0x7717f0ddbf60>]",
          "scorer": "<function choice.<locals>.score at 0x7717f0dfc040>",
          "sample_fields": "<function record_to_sample_multichoice at 0x7717f0ddb560>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('multiple_choice',)",
          "evaluation_splits": "('multiple_choice',)",
          "few_shots_split": "None",
          "few_shots_select": "random_sampling",
          "generation_size": "100",
          "generation_grammar": "None",
          "stop_sequence": "[]",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0.1"
        }
      },
      {
        "name": "mixeval_hard:freeform",
        "config": {
          "name": "mixeval_hard:freeform",
          "prompt_function": "<function mixeval_freeform_prompt at 0x7717f0ddad40>",
          "hf_repo": "MixEval/MixEval",
          "hf_subset": "MixEval_Hard",
          "metrics": "({'metric_name': ['llm_judge_mixeval_flow'], 'higher_is_better': {'judge_score': True}, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.JudgeLLMMixEval object at 0x7717f0b862d0>, 'corpus_level_fn': {'judge_score_flow': <function mean_dv_5 at 0x7717f0ddb7e0>}, 'batched_compute': True}, {'metric_name': ['llm_judge_mixeval_gpt3'], 'higher_is_better': {'judge_score_gpt-3.5': True}, 'category': <SamplingMethod.GENERATIVE: 'GENERATIVE'>, 'sample_level_fn': <lighteval.metrics.metrics_sample.JudgeLLMMixEval object at 0x7717f0b86330>, 'corpus_level_fn': {'judge_score_gpt-3.5': <function mean at 0x771aa6d5b3b0>}, 'batched_compute': True})",
          "hf_data_files": "None",
          "solver": "[<function generate.<locals>.solve at 0x7717f0ddbd80>]",
          "scorer": "<function _model_graded_qa_single.<locals>.score at 0x7717f0ddbe20>",
          "sample_fields": "<function record_to_sample_freeform at 0x7717f0ddb4c0>",
          "sample_to_fewshot": "None",
          "filter": "None",
          "hf_revision": "None",
          "hf_filter": "None",
          "hf_avail_splits": "('free_form',)",
          "evaluation_splits": "('free_form',)",
          "few_shots_split": "None",
          "few_shots_select": "random_sampling",
          "generation_size": "100",
          "generation_grammar": "None",
          "stop_sequence": "[]",
          "num_samples": "None",
          "original_num_docs": "-1",
          "effective_num_docs": "-1",
          "must_remove_duplicate_docs": "False",
          "num_fewshots": "0",
          "version": "0.1"
        }
      }
    ]
  }
]
