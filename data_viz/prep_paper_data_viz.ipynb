{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data viz for paper submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick prep for proper ipynb viz in VS Code\n",
    "!uv pip install ipykernel jupyter matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports\n",
    "import json\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sec 1: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idk yet, probably want a catchy graph opposite abstract!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sec 2: Related Works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 2: Idk, related works probs won't need one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sec 3: Existing Approaches for Mitigating Safety Degradation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function that retrieves scores on safety benchmarks produced by allenai/safety-eval\n",
    "def retrieve_results(\n",
    "    results_json_path: str,\n",
    "    benchmark_configs: list[tuple[str, str]],\n",
    "    compute_mean: bool = True,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Given a string filepath to a JSON file containing results created by safety-eval, and tuples of benchmarks and requested metrics, returns results as list and optionally the mean at the end.\n",
    "\n",
    "        Args:\n",
    "            results_json_path (`str`):\n",
    "                Filepath of safety-eval generated JSON results file.\n",
    "            benchmark_configs (`list[tuple[str, str]]`):\n",
    "                List of tuples, each containing a benchmark and associated metric name. \n",
    "            compute_mean (`bool`, defaults to `False`):\n",
    "                Whether or not to append average score at the end.\n",
    "\n",
    "        Returns:\n",
    "            Safety benchmark results as a list\n",
    "    \"\"\"\n",
    "    with open(results_json_path) as fp:\n",
    "        results_dict = json.load(fp)\n",
    "\n",
    "    scores = []\n",
    "    for benchmark, score_name in benchmark_configs:\n",
    "        scores.append(results_dict[benchmark][score_name])\n",
    "    \n",
    "    if compute_mean:\n",
    "        mean = sum(scores) / len(scores)\n",
    "        scores.append(mean)\n",
    "        # print(f\"Average score for {results_json_path} is {mean}\")\n",
    "    return scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sec 3.1: Lower Learning Rates During Supervised Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data for MATH-500, IFeval, IFBench from lighteval\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.container import BarContainer\n",
    "\n",
    "RESULTS = {\n",
    "    #  Model name                           Version      MATH-500           IFEval             IFBench\n",
    "    \"Base\":                                 (\"Base\",     (0.460, 0.0223),  (0.7024, 0.0197),  (0.2433, 0.0248)),\n",
    "    \"Best low LR SFT\":                      (\"v00.24\",   (0.598, 0.0219),  (0.4843, 0.0215),  (0.1267, 0.0192)),\n",
    "    \"Best Replay\":                          (\"v00.15\",   (0.520, 0.0224),  (0.5619, 0.0214),  (0.1533, 0.0208)),\n",
    "    \"Best DFT\":                             (\"v00.17\",   (0.424, 0.0221),  (0.3771, 0.0209),  (0.1067, 0.0179)),\n",
    "    \"Best EMA eta=0.75\":                    (\"v00.30\",   (0.580, 0.0221),  (0.4695, 0.0215),  (0.1267, 0.0192)),\n",
    "    \"Best CISPO KL_beta=0.001\":             (\"v00.12\",   (0.582, 0.0221),  (0.6784, 0.0201),  (0.2200, 0.0240)),\n",
    "    \"Best CISPO KL_beta=0\":                 (\"v00.14\",   (0.594, 0.0220),  (0.6359, 0.0207),  (0.1900, 0.0227)),\n",
    "}\n",
    "\n",
    "BENCHMARKS = [\"MATH-500\", \"IFEval\", \"IFBench\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 3: Low LR results - Math**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 3: Low LR results - Utility**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 3: Low LR results - Safety**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_file_path = \"../safety-eval/generation_results/\"\n",
    "model = \"Neelectric/Llama-3.1-8B-Instruct_SFT_Math-220kv00.\"\n",
    "\n",
    "safety_benchmark_configs = [\n",
    "    ('wildguardtest', 'inverted_micro_harm_lower'),\n",
    "    ('harmbench', 'inverted_micro_asr_lower'),\n",
    "    ('toxigen', 'safe_overall'),\n",
    "    ('do_anything_now', 'inverted_macro_asr'),\n",
    "    ('wildjailbreak:harmful', 'inverted_macro_asr'),\n",
    "    ('trustllm_jailbreaktrigger', 'inverted_macro_asr')\n",
    "]\n",
    "benchmarks = [benchmark[0] for benchmark in safety_benchmark_configs] + [\"Average\"]\n",
    "print(benchmarks)\n",
    "\n",
    "### do it for base\n",
    "base_path = root_file_path + \"meta-llama/Llama-3.1-8B-Instruct\" + \"/base/metrics_base_main.json\"\n",
    "base_values = retrieve_results(results_json_path=base_path, benchmark_configs=safety_benchmark_configs, compute_mean=True)\n",
    "    \n",
    "low_lr_values = []\n",
    "for i in range(7,13):\n",
    "    if i == 11: # the v00.11 run had a bug\n",
    "        continue\n",
    "    version_id_unpadded = str(i)\n",
    "    version_id = \"0\" + version_id_unpadded if len(version_id_unpadded) == 1 else version_id_unpadded # turning \"7\" into \"07\"\n",
    "    low_lr_path = root_file_path + model + version_id + \"/SFT/metrics_SFT_main.json\"\n",
    "    low_lr_values += retrieve_results(results_json_path=low_lr_path, benchmark_configs=safety_benchmark_configs, compute_mean=True)\n",
    "    \n",
    "n = 6\n",
    "print(len(benchmarks * n))\n",
    "print(len(base_values + low_lr_values))\n",
    "print(len(['base'] * len(benchmarks) + ['LR: 5e-5 (v00.07)'] * len(benchmarks) + ['LR: 1e-5 (v00.08)'] * len(benchmarks) + ['LR: 5e-6 (v00.09)'] * len(benchmarks)  + ['LR: 1e-6 (v00.10)'] * len(benchmarks) + ['LR: 5e-7 (v00.12)'] * len(benchmarks)))\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Benchmark': benchmarks * n,\n",
    "    'Value': base_values + low_lr_values,\n",
    "    'Model': ['base'] * len(benchmarks) + ['LR: 5e-5 (v00.07)'] * len(benchmarks) + ['LR: 1e-5 (v00.08)'] * len(benchmarks) + ['LR: 5e-6 (v00.09)'] * len(benchmarks)  + ['LR: 1e-6 (v00.10)'] * len(benchmarks) + ['LR: 5e-7 (v00.12)'] * len(benchmarks)\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "palette = sns.color_palette(\"viridis\", n)\n",
    "bars = sns.barplot(data=df, x='Benchmark', y='Value', hue='Model', ax=ax, palette=palette)\n",
    "\n",
    "# again we add value labels on top of bars\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt='%.3f', padding=3, fontsize=6)\n",
    "\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('Score (higher is better)', fontsize=12)\n",
    "ax.set_title('Safety Benchmark Results\\n(reporting checkpoint with strongest math performance)', fontsize=14, fontweight='bold')\n",
    "ax.legend(title='Model', frameon=True, fancybox=True, shadow=True)\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "ax.set_ylim(0, 1 * 1.07) # trying to give room for labels\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sec 3.2: Performing Replay On Refusals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure x: Low percentage replay results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data for MATH-500, IFeval, IFBench from lighteval\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.container import BarContainer\n",
    "\n",
    "RESULTS = {\n",
    "    #  Model name                                   Version      MATH-500           IFEval             IFBench\n",
    "    \"Base\":                                         (\"Base\",     (0.460, 0.0223),  (0.7024, 0.0197),  (0.2433, 0.0248)),\n",
    "    \"Best low LR SFT\":                              (\"v00.24\",   (0.598, 0.0219),  (0.4843, 0.0215),  (0.1267, 0.0192)),\n",
    "    \"1% Replay\":                                    (\"v00.32\",   (0.566, 0.0222),  (0.3604, 0.0207),  (0.1467, 0.0205)),\n",
    "    \"2% Replay\":                                    (\"v00.33\",   (0.566, 0.0222),  (0.4233, 0.0213),  (0.1600, 0.0212)),\n",
    "    \"3% Replay\":                                    (\"v00.34\",   (0.582, 0.0221),  (0.4344, 0.0213),  (0.1767, 0.0221)),\n",
    "    \"4% Replay\":                                    (\"v00.35\",   (0.560, 0.0222),  (0.4455, 0.0214),  (0.1867, 0.0225)),\n",
    "    \"5% Replay\":                                    (\"v00.13\",   (0.522, 0.0224),  (0.0001, 0.0001),  (0.0001, 0.0001)),\n",
    "}\n",
    "\n",
    "BENCHMARKS = [\"MATH-500\", \"IFEval\", \"IFBench\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.container import BarContainer\n",
    "\n",
    "rows = []\n",
    "for name, (version, *scores) in RESULTS.items():\n",
    "    label = f\"{name} ({version})\" if name != \"Base\" else \"Base\"\n",
    "    for bench, (val, stderr) in zip(BENCHMARKS, scores):\n",
    "        rows.append({\"Model\": label, \"Benchmark\": bench, \"Value\": val, \"Stderr\": stderr})\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# we select only instruction following benchmarks\n",
    "df = df[df['Benchmark'].isin(['IFEval','IFBench'])]\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "n_models = len(df['Model'].unique())\n",
    "palette = sns.color_palette(\"viridis\", n_models)\n",
    "\n",
    "bars = sns.barplot(data=df, x='Benchmark', y='Value', hue='Model', ax=ax, palette=palette, errorbar=None)\n",
    "\n",
    "benchmarks = df['Benchmark'].unique()\n",
    "models = df['Model'].unique()\n",
    "\n",
    "for i, container in enumerate(ax.containers):\n",
    "    if isinstance(container, BarContainer):\n",
    "        for j, bar in enumerate(container):\n",
    "            model = models[i]\n",
    "            benchmark = benchmarks[j]\n",
    "            stderr = df[(df['Model'] == model) & (df['Benchmark'] == benchmark)]['Stderr'].values[0]\n",
    "            x = bar.get_x() + bar.get_width() / 2\n",
    "            y = bar.get_height()\n",
    "            if y > 0:  # Only add error bars for non-zero values\n",
    "                ax.errorbar(x, y, yerr=stderr, fmt='none',\n",
    "                           color='black', capsize=3, capthick=1.5,\n",
    "                           elinewidth=1.5, alpha=0.7, zorder=10)\n",
    "\n",
    "# adding value labels on top of bars again\n",
    "for container in ax.containers:\n",
    "    if isinstance(container, BarContainer):\n",
    "        labels = [f'{v.get_height():.3f}' if v.get_height() > 0 else '' for v in container]\n",
    "        ax.bar_label(container, labels=labels, padding=15, fontsize=8, rotation=0)\n",
    "\n",
    "ax.set_xlabel('Benchmark (Prompt-level strict accuracy)', fontsize=12)\n",
    "ax.set_ylabel('Score (higher is better)', fontsize=12)\n",
    "ax.set_title('Instruction-Following Results after Replay\\n(reporting checkpoint with strongest math performance)', fontsize=14, fontweight='bold')\n",
    "# ax.legend(title='Model', frameon=True, fancybox=True, shadow=True, bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "ax.legend(title='Model', frameon=True, fancybox=True, shadow=True)\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Set y-axis limit to give room for labels and error bars\n",
    "ax.set_ylim(0, max(df['Value'] + df['Stderr']) * 1.25)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 4: Replay results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sec 3.3: Dynamic Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 5: Dynamic Fine-tuning results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sec 3.4: Exponential Moving Average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 6: Exponential Moving Average results - safety**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_file_path = \"../safety-eval/generation_results/\"\n",
    "model = \"Neelectric/Llama-3.1-8B-Instruct_SFT_Math-220kv00.\"\n",
    "\n",
    "safety_benchmark_configs = [\n",
    "    ('wildguardtest', 'inverted_micro_harm_lower'),\n",
    "    ('harmbench', 'inverted_micro_asr_lower'),\n",
    "    ('toxigen', 'safe_overall'),\n",
    "    ('do_anything_now', 'inverted_macro_asr'),\n",
    "    ('wildjailbreak:harmful', 'inverted_macro_asr'),\n",
    "    ('trustllm_jailbreaktrigger', 'inverted_macro_asr')\n",
    "]\n",
    "benchmarks = [benchmark[0] for benchmark in safety_benchmark_configs] + [\"Average\"]\n",
    "print(benchmarks)\n",
    "\n",
    "### do it for base\n",
    "base_path = root_file_path + \"meta-llama/Llama-3.1-8B-Instruct\" + \"/base/metrics_base_main.json\"\n",
    "base_values = retrieve_results(results_json_path=base_path, benchmark_configs=safety_benchmark_configs, compute_mean=True)\n",
    "    \n",
    "low_lr_values = []\n",
    "for i in range(28,31):\n",
    "    version_id_unpadded = str(i)\n",
    "    version_id = \"0\" + version_id_unpadded if len(version_id_unpadded) == 1 else version_id_unpadded # turning \"7\" into \"07\"\n",
    "    low_lr_path = root_file_path + model + version_id + \"/SFT/metrics_SFT_main.json\"\n",
    "    low_lr_values += retrieve_results(results_json_path=low_lr_path, benchmark_configs=safety_benchmark_configs, compute_mean=True)\n",
    "    \n",
    "n = 4\n",
    "print(len(benchmarks * n))\n",
    "print(len(base_values + low_lr_values))\n",
    "print(len(['base'] * len(benchmarks) + ['EMA eta: 0.25 (v00.28)'] * len(benchmarks) + ['EMA eta: 0.4 (v00.29)'] * len(benchmarks) + ['EMA eta: 0.75 (v00.30)'] * len(benchmarks) ))\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Benchmark': benchmarks * n,\n",
    "    'Value': base_values + low_lr_values,\n",
    "    'Model': ['base'] * len(benchmarks) + ['EMA eta: 0.25 (v00.28)'] * len(benchmarks) + ['EMA eta: 0.4 (v00.29)'] * len(benchmarks) + ['EMA eta: 0.75 (v00.30)'] * len(benchmarks)\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "palette = sns.color_palette(\"viridis\", n)\n",
    "bars = sns.barplot(data=df, x='Benchmark', y='Value', hue='Model', ax=ax, palette=palette)\n",
    "\n",
    "# again we add value labels on top of bars\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt='%.3f', padding=3, fontsize=6)\n",
    "\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('Score (higher is better)', fontsize=12)\n",
    "ax.set_title('Safety Benchmark Results\\n(reporting checkpoint with strongest math performance)', fontsize=14, fontweight='bold')\n",
    "ax.legend(title='Model', frameon=True, fancybox=True, shadow=True)\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "ax.set_ylim(0, 1 * 1.07) # trying to give room for labels\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sec 3.5: Reinforcement Learning with Verifiable Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 7: RLVR results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sec 3.6: Comparison of Best Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CURRENTLY HARDCODING VALS, SHOULD CHANGE THIS BEFORE SUBMISSION!\")\n",
    "print(\"IDEALLY A BENCHMARKING SCRIPT THAT CREATES ALL RESULTS FOR ALL FIGURES INDIVIDUALLY WHICH YOU CAN RUN BEFORE RUNNING ALL THESE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data for MATH-500, IFeval, IFBench from lighteval\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.container import BarContainer\n",
    "\n",
    "RESULTS = {\n",
    "    #  Model name                           Version      MATH-500           IFEval             IFBench\n",
    "    \"Base\":                                 (\"Base\",     (0.460, 0.0223),  (0.7024, 0.0197),  (0.2433, 0.0248)),\n",
    "    \"Best low LR SFT\":                      (\"v00.24\",   (0.598, 0.0219),  (0.4843, 0.0215),  (0.1267, 0.0192)),\n",
    "    \"Best Replay\":                          (\"v00.15\",   (0.520, 0.0224),  (0.5619, 0.0214),  (0.1533, 0.0208)),\n",
    "    \"Best DFT\":                             (\"v00.17\",   (0.424, 0.0221),  (0.3771, 0.0209),  (0.1067, 0.0179)),\n",
    "    \"Best EMA eta=0.75\":                    (\"v00.30\",   (0.580, 0.0221),  (0.4695, 0.0215),  (0.1267, 0.0192)),\n",
    "    \"Best CISPO KL_beta=0.001\":             (\"v00.12\",   (0.582, 0.0221),  (0.6784, 0.0201),  (0.2200, 0.0240)),\n",
    "    \"Best CISPO KL_beta=0\":                 (\"v00.14\",   (0.594, 0.0220),  (0.6359, 0.0207),  (0.1900, 0.0227)),\n",
    "}\n",
    "\n",
    "BENCHMARKS = [\"MATH-500\", \"IFEval\", \"IFBench\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 8: Math Best checkpoint comparison graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for name, (version, *scores) in RESULTS.items():\n",
    "    label = f\"{name} ({version})\" if name != \"Base\" else \"Base\"\n",
    "    for bench, (val, stderr) in zip(BENCHMARKS, scores):\n",
    "        rows.append({\"Model\": label, \"Benchmark\": bench, \"Value\": val, \"Stderr\": stderr})\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df = df[df['Benchmark'].isin(['MATH-500'])]\n",
    "\n",
    "df['Benchmark'] = 'MATH-500 (pass@k, k=1 & n=1)'\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 7))\n",
    "n = len(df)\n",
    "palette = sns.color_palette(\"viridis\", n)\n",
    "\n",
    "bars = sns.barplot(data=df, x='Benchmark', y='Value', hue='Model', ax=ax, palette=palette, errorbar=None)\n",
    "\n",
    "x_pos = []\n",
    "y_pos = []\n",
    "errors = []\n",
    "\n",
    "# adding stderr vals to all bars\n",
    "for i, container in enumerate(ax.containers):\n",
    "    for bar in container:\n",
    "        x_pos.append(bar.get_x() + bar.get_width() / 2)\n",
    "        y_pos.append(bar.get_height())\n",
    "    errors.append(df.iloc[i]['Stderr'])\n",
    "ax.errorbar(x_pos, y_pos, yerr=errors, fmt='none', \n",
    "            color='black', capsize=5, capthick=1.5, \n",
    "            elinewidth=1.5, alpha=0.7, zorder=10)\n",
    "\n",
    "# i prefer adding bar labels on top of bars to actually see the values\n",
    "for container in ax.containers:\n",
    "    if isinstance(container, BarContainer):\n",
    "        ax.bar_label(container, fmt='%.3f', padding=15, fontsize=9)\n",
    "\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('Score (higher is better)', fontsize=12)\n",
    "ax.set_title('MATH-500 results (by best checkpoint)', fontsize=14, fontweight='bold')\n",
    "ax.legend(title='Model', frameon=True, fancybox=True, shadow=True)\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Set y-axis limit to give room for labels and error bars\n",
    "ax.set_ylim(0, max(df['Value'] + df['Stderr']) * 1.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 9: Instruction Following Best checkpoint comparison graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.container import BarContainer\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# we select only instruction following benchmarks\n",
    "df = df[df['Benchmark'].isin(['IFEval','IFBench'])]\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "n_models = len(df['Model'].unique())\n",
    "palette = sns.color_palette(\"viridis\", n_models)\n",
    "\n",
    "bars = sns.barplot(data=df, x='Benchmark', y='Value', hue='Model', ax=ax, palette=palette, errorbar=None)\n",
    "\n",
    "benchmarks = df['Benchmark'].unique()\n",
    "models = df['Model'].unique()\n",
    "\n",
    "for i, container in enumerate(ax.containers):\n",
    "    if isinstance(container, BarContainer):\n",
    "        for j, bar in enumerate(container):\n",
    "            model = models[i]\n",
    "            benchmark = benchmarks[j]\n",
    "            stderr = df[(df['Model'] == model) & (df['Benchmark'] == benchmark)]['Stderr'].values[0]\n",
    "            x = bar.get_x() + bar.get_width() / 2\n",
    "            y = bar.get_height()\n",
    "            if y > 0:  # Only add error bars for non-zero values\n",
    "                ax.errorbar(x, y, yerr=stderr, fmt='none',\n",
    "                           color='black', capsize=3, capthick=1.5,\n",
    "                           elinewidth=1.5, alpha=0.7, zorder=10)\n",
    "\n",
    "# adding value labels on top of bars again\n",
    "for container in ax.containers:\n",
    "    if isinstance(container, BarContainer):\n",
    "        labels = [f'{v.get_height():.3f}' if v.get_height() > 0 else '' for v in container]\n",
    "        ax.bar_label(container, labels=labels, padding=15, fontsize=8, rotation=0)\n",
    "\n",
    "ax.set_xlabel('Benchmark (Prompt-level strict accuracy)', fontsize=12)\n",
    "ax.set_ylabel('Score (higher is better)', fontsize=12)\n",
    "ax.set_title('Instruction-Following Results\\n(reporting checkpoint with strongest math performance)', fontsize=14, fontweight='bold')\n",
    "# ax.legend(title='Model', frameon=True, fancybox=True, shadow=True, bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "ax.legend(title='Model', frameon=True, fancybox=True, shadow=True)\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Set y-axis limit to give room for labels and error bars\n",
    "ax.set_ylim(0, max(df['Value'] + df['Stderr']) * 1.25)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 10: Safety Best checkpoint comparison graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "root_file_path = \"../safety-eval/generation_results/\"\n",
    "model = \"Neelectric/Llama-3.1-8B-Instruct_SFT_Math-220kv00.\"\n",
    "\n",
    "safety_benchmark_configs = [\n",
    "    ('wildguardtest', 'inverted_micro_harm_lower'),\n",
    "    ('harmbench', 'inverted_micro_asr_lower'),\n",
    "    ('toxigen', 'safe_overall'),\n",
    "    ('do_anything_now', 'inverted_macro_asr'),\n",
    "    ('wildjailbreak:harmful', 'inverted_macro_asr'),\n",
    "    ('trustllm_jailbreaktrigger', 'inverted_macro_asr')\n",
    "]\n",
    "benchmarks = [benchmark[0] for benchmark in safety_benchmark_configs] + [\"Average\"]\n",
    "\n",
    "# quick helper function to retrieve values\n",
    "def retrieve_results(\n",
    "    results_json_path: str,\n",
    "    benchmark_configs: list[tuple[str, str]],\n",
    "    compute_mean: bool = True,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Given a string filepath to a JSON file containing results created by safety-eval, and tuples of benchmarks and requested metrics, returns results as list and optionally the mean at the end.\n",
    "\n",
    "        Args:\n",
    "            results_json_path (`str`):\n",
    "                Filepath of safety-eval generated JSON results file.\n",
    "            benchmark_configs (`list[tuple[str, str]]`):\n",
    "                List of tuples, each containing a benchmark and associated metric name. \n",
    "            compute_mean (`bool`, defaults to `False`):\n",
    "                Whether or not to append average score at the end.\n",
    "\n",
    "        Returns:\n",
    "            Safety benchmark results as a list\n",
    "    \"\"\"\n",
    "    with open(results_json_path) as fp:\n",
    "        results_dict = json.load(fp)\n",
    "\n",
    "    scores = []\n",
    "    for benchmark, score_name in benchmark_configs:\n",
    "        scores.append(results_dict[benchmark][score_name])\n",
    "    \n",
    "    if compute_mean:\n",
    "        mean = sum(scores) / len(scores)\n",
    "        scores.append(mean)\n",
    "        print(f\"Average score for {results_json_path} is {mean}\")\n",
    "    return scores\n",
    "\n",
    "\n",
    "# Base\n",
    "# Best SFT\n",
    "# Best Replay\n",
    "# Best DFT\n",
    "# Best EMA\n",
    "# Best CISPO (KL_beta=0.001)\n",
    "# Best CISPO (KL_beta=0)\n",
    "\n",
    "### do it for base\n",
    "base_path = root_file_path + \"meta-llama/Llama-3.1-8B-Instruct\" + \"/base/metrics_base_main.json\"\n",
    "base_values = retrieve_results(results_json_path=base_path, benchmark_configs=safety_benchmark_configs, compute_mean=True)\n",
    "    \n",
    "### this is for best low LR sft v00.24\n",
    "low_lr_path = root_file_path + model + \"24\" + \"/SFT/metrics_SFT_main.json\"\n",
    "low_lr_values = retrieve_results(results_json_path=low_lr_path, benchmark_configs=safety_benchmark_configs, compute_mean=True)\n",
    "    \n",
    "### this is best replay v00.15\n",
    "best_replay_path = root_file_path + model + \"15\" + \"/SFT/metrics_SFT_main.json\"\n",
    "best_replay_values = retrieve_results(results_json_path=best_replay_path, benchmark_configs=safety_benchmark_configs, compute_mean=True)\n",
    "    \n",
    "### this needs to become best DFT v00.17\n",
    "best_dft_path = root_file_path + model + \"17\" + \"/SFT/metrics_SFT_main.json\"\n",
    "best_dft_values = retrieve_results(results_json_path=best_dft_path, benchmark_configs=safety_benchmark_configs, compute_mean=True)\n",
    "    \n",
    "### this needs to become best EMA v00.30\n",
    "best_ema_path = root_file_path + model + \"30\" + \"/SFT/metrics_SFT_main.json\"\n",
    "best_ema_values = retrieve_results(results_json_path=best_ema_path, benchmark_configs=safety_benchmark_configs, compute_mean=True)\n",
    "    \n",
    "## this needs to become best CISPO with KL\n",
    "best_cispo_kl_path = root_file_path + \"Neelectric/Llama-3.1-8B-Instruct_GRPO_Math-220kv00.12\" + \"/SFT/metrics_SFT_main-step-000000250.json\"\n",
    "best_cispo_kl_values = retrieve_results(results_json_path=best_cispo_kl_path, benchmark_configs=safety_benchmark_configs, compute_mean=True)\n",
    "\n",
    "## here we will have best CISPO without KL\n",
    "best_cispo_path = root_file_path + \"Neelectric/Llama-3.1-8B-Instruct_GRPO_Math-220kv00.14\" + \"/SFT/metrics_SFT_main-step-000000250.json\"\n",
    "best_cispo_values = retrieve_results(results_json_path=best_cispo_path, benchmark_configs=safety_benchmark_configs, compute_mean=True)\n",
    "\n",
    "# now we put into a dataframe and plot\n",
    "n = 7\n",
    "df = pd.DataFrame({\n",
    "    'Benchmark': benchmarks * n,\n",
    "    'Value': base_values + low_lr_values + best_replay_values + best_dft_values + best_ema_values + best_cispo_kl_values + best_cispo_values,\n",
    "    'Model': ['base'] * len(benchmarks) + ['Best low LR SFT (v00.24)'] * len(benchmarks) + ['Best replay (v00.15)'] * len(benchmarks) + ['Best DFT (v00.17)'] * len(benchmarks) + ['Best EMA (v00.25)'] * len(benchmarks) + ['Best CISPO with KL (v00.12)'] * len(benchmarks) + ['Best CISPO (v00.14)'] * len(benchmarks)\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "palette = sns.color_palette(\"viridis\", n)\n",
    "bars = sns.barplot(data=df, x='Benchmark', y='Value', hue='Model', ax=ax, palette=palette)\n",
    "\n",
    "# again we add value labels on top of bars\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt='%.3f', padding=3, fontsize=6)\n",
    "\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('Score (higher is better)', fontsize=12)\n",
    "ax.set_title('Safety Benchmark Results\\n(reporting checkpoint with strongest math performance)', fontsize=14, fontweight='bold')\n",
    "ax.legend(title='Model', frameon=True, fancybox=True, shadow=True)\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# trying to give room for labels\n",
    "ax.set_ylim(0, 1 * 1.07)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average scores only!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "root_file_path = \"../safety-eval/generation_results/\"\n",
    "model = \"Neelectric/Llama-3.1-8B-Instruct_SFT_Math-220kv00.\"\n",
    "\n",
    "safety_benchmark_configs = [\n",
    "    ('wildguardtest', 'inverted_micro_harm_lower'),\n",
    "    ('harmbench', 'inverted_micro_asr_lower'),\n",
    "    ('toxigen', 'safe_overall'),\n",
    "    ('do_anything_now', 'inverted_macro_asr'),\n",
    "    ('wildjailbreak:harmful', 'inverted_macro_asr'),\n",
    "    ('trustllm_jailbreaktrigger', 'inverted_macro_asr')\n",
    "]\n",
    "benchmarks = [benchmark[0] for benchmark in safety_benchmark_configs] + [\"Average\"]\n",
    "\n",
    "# quick helper function to retrieve values\n",
    "def retrieve_results(\n",
    "    results_json_path: str,\n",
    "    benchmark_configs: list[tuple[str, str]],\n",
    "    compute_mean: bool = True,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Given a string filepath to a JSON file containing results created by safety-eval, and tuples of benchmarks and requested metrics, returns results as list and optionally the mean at the end.\n",
    "\n",
    "        Args:\n",
    "            results_json_path (`str`):\n",
    "                Filepath of safety-eval generated JSON results file.\n",
    "            benchmark_configs (`list[tuple[str, str]]`):\n",
    "                List of tuples, each containing a benchmark and associated metric name. \n",
    "            compute_mean (`bool`, defaults to `False`):\n",
    "                Whether or not to append average score at the end.\n",
    "\n",
    "        Returns:\n",
    "            Safety benchmark results as a list\n",
    "    \"\"\"\n",
    "    with open(results_json_path) as fp:\n",
    "        results_dict = json.load(fp)\n",
    "\n",
    "    scores = []\n",
    "    for benchmark, score_name in benchmark_configs:\n",
    "        scores.append(results_dict[benchmark][score_name])\n",
    "    \n",
    "    if compute_mean:\n",
    "        mean = sum(scores) / len(scores)\n",
    "        scores.append(mean)\n",
    "        print(f\"Average score for {results_json_path} is {mean}\")\n",
    "    return scores\n",
    "\n",
    "\n",
    "# Base\n",
    "# Best SFT\n",
    "# Best Replay\n",
    "# Best DFT\n",
    "# Best EMA\n",
    "# Best CISPO (KL_beta=0.001)\n",
    "# Best CISPO (KL_beta=0)\n",
    "\n",
    "### do it for base\n",
    "base_path = root_file_path + \"meta-llama/Llama-3.1-8B-Instruct\" + \"/base/metrics_base_main.json\"\n",
    "base_values = retrieve_results(results_json_path=base_path, benchmark_configs=safety_benchmark_configs, compute_mean=True)\n",
    "base_values_avg = [base_values[-1]]\n",
    "print(base_values)\n",
    "\n",
    "### this is for best low LR sft v00.24\n",
    "low_lr_path = root_file_path + model + \"24\" + \"/SFT/metrics_SFT_main.json\"\n",
    "low_lr_values = retrieve_results(results_json_path=low_lr_path, benchmark_configs=safety_benchmark_configs, compute_mean=True)\n",
    "low_lr_values_avg = [low_lr_values[-1]]\n",
    "    \n",
    "### this is best replay v00.15\n",
    "best_replay_path = root_file_path + model + \"15\" + \"/SFT/metrics_SFT_main.json\"\n",
    "best_replay_values = retrieve_results(results_json_path=best_replay_path, benchmark_configs=safety_benchmark_configs, compute_mean=True)\n",
    "best_replay_values_avg = [best_replay_values[-1]]\n",
    "\n",
    "### this needs to become best DFT v00.17\n",
    "best_dft_path = root_file_path + model + \"17\" + \"/SFT/metrics_SFT_main.json\"\n",
    "best_dft_values = retrieve_results(results_json_path=best_dft_path, benchmark_configs=safety_benchmark_configs, compute_mean=True)\n",
    "best_dft_values_avg = [best_dft_values[-1]]\n",
    "    \n",
    "### this needs to become best EMA v00.30\n",
    "best_ema_path = root_file_path + model + \"30\" + \"/SFT/metrics_SFT_main.json\"\n",
    "best_ema_values = retrieve_results(results_json_path=best_ema_path, benchmark_configs=safety_benchmark_configs, compute_mean=True)\n",
    "best_ema_values_avg = [best_ema_values[-1]]\n",
    "    \n",
    "## this needs to become best CISPO with KL\n",
    "best_cispo_kl_path = root_file_path + \"Neelectric/Llama-3.1-8B-Instruct_GRPO_Math-220kv00.12\" + \"/SFT/metrics_SFT_main-step-000000250.json\"\n",
    "best_cispo_kl_values = retrieve_results(results_json_path=best_cispo_kl_path, benchmark_configs=safety_benchmark_configs, compute_mean=True)\n",
    "best_cispo_kl_values_avg = [best_cispo_kl_values[-1]]\n",
    "\n",
    "## here we will have best CISPO without KL\n",
    "best_cispo_path = root_file_path + \"Neelectric/Llama-3.1-8B-Instruct_GRPO_Math-220kv00.14\" + \"/SFT/metrics_SFT_main-step-000000250.json\"\n",
    "best_cispo_values = retrieve_results(results_json_path=best_cispo_path, benchmark_configs=safety_benchmark_configs, compute_mean=True)\n",
    "best_cispo_values_avg = [best_cispo_values[-1]]\n",
    "\n",
    "benchmarks = [\"Average\"]\n",
    "\n",
    "# now we put into a dataframe and plot\n",
    "n = 7\n",
    "df = pd.DataFrame({\n",
    "    'Benchmark': benchmarks * n,\n",
    "    'Value': base_values_avg + low_lr_values_avg + best_replay_values_avg + best_dft_values_avg + best_ema_values_avg + best_cispo_kl_values_avg + best_cispo_values_avg,\n",
    "    'Model': ['base'] * len(benchmarks) + ['Best low LR SFT (v00.24)'] * len(benchmarks) + ['Best replay (v00.15)'] * len(benchmarks) + ['Best DFT (v00.17)'] * len(benchmarks) + ['Best EMA (v00.25)'] * len(benchmarks) + ['Best CISPO with KL (v00.12)'] * len(benchmarks) + ['Best CISPO (v00.14)'] * len(benchmarks)\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "palette = sns.color_palette(\"viridis\", n)\n",
    "bars = sns.barplot(data=df, x='Benchmark', y='Value', hue='Model', ax=ax, palette=palette)\n",
    "\n",
    "# again we add value labels on top of bars\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt='%.3f', padding=3, fontsize=6)\n",
    "\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('Score (higher is better)', fontsize=12)\n",
    "ax.set_title('Safety Benchmark Results\\n(reporting checkpoint with strongest math performance)', fontsize=14, fontweight='bold')\n",
    "ax.legend(title='Model', frameon=True, fancybox=True, shadow=True)\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# trying to give room for labels\n",
    "ax.set_ylim(0, 1 * 1.07)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openr1_v2 (local)",
   "language": "python",
   "name": "openr1_v2_root"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
