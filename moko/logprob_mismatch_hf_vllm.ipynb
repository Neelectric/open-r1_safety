{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6bfedb1d7c04c53b81bd6960cd429be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\ntest!<|eot_id|>'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda:1\")\n",
    "messages = [{\"role\": \"user\",\"content\": \"test!\"}]\n",
    "empty_magpie_template = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "empty_magpie_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m empty_magpie_inputs = tokenizer(empty_magpie_template, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(\u001b[33m\"\u001b[39m\u001b[33mcuda:1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m empty_magpie_template\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m magpie_outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mempty_magpie_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m250\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m magpie_decoded = tokenizer.decode(magpie_outputs[\u001b[32m0\u001b[39m])\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(magpie_decoded)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/open-r1_safety/openr1_v3/lib/python3.11/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/open-r1_safety/openr1_v3/lib/python3.11/site-packages/transformers/generation/utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/open-r1_safety/openr1_v3/lib/python3.11/site-packages/transformers/generation/utils.py:2779\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2776\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2777\u001b[39m     is_prefill = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2779\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n\u001b[32m   2780\u001b[39m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[32m   2781\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\u001b[32m   2783\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "empty_magpie_template = '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n'\n",
    "empty_magpie_inputs = tokenizer(empty_magpie_template, return_tensors=\"pt\").to(\"cuda:1\")\n",
    "empty_magpie_template\n",
    "magpie_outputs = model.generate(**empty_magpie_inputs, max_new_tokens=250, do_sample=False, top_p=None, temperature=None)\n",
    "magpie_decoded = tokenizer.decode(magpie_outputs[0])\n",
    "print(magpie_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "The sum of the squares of the first n natural numbers is given by the formula  n(n+1)(2n+1)/6.  What is the sum of the squares of the first 10 natural numbers? \n",
      "\n",
      "1^2 + 2^2 + 3^2 + 4^2 + 5^2 + 6^2 + 7^2 + 8^2 + 9^2 + 10^2 =?\n",
      "\n",
      "To find the answer, use the formula n(n+1)(2n+1)/6.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "To find the sum of the squares of the first 10 natural numbers, we will use the formula n(n+1)(2n+1)/6 and substitute n = 10.\n",
      "\n",
      "Sum = n(n+1)(2n+1)/6\n",
      "Sum = 10(10+1)(2*10+1)/6\n",
      "Sum = 10(11)(21)/6\n",
      "Sum = 2310/6\n",
      "Sum = 385\n",
      "\n",
      "The sum of the squares of the first 10 natural numbers is 385.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(magpie_decoded, return_tensors=\"pt\").to(\"cuda:1\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=250, do_sample=False, top_p=None, temperature=None)\n",
    "decoded = tokenizer.decode(outputs[0])\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicit multi-GPU memory control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7a991df174745b9ab13e6d219b936c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    dtype=\"bfloat16\",\n",
    "    device_map=\"auto\",\n",
    "    max_memory={\n",
    "        0: \"16Gib\",\n",
    "        # 1: \"9GiB\",\n",
    "    },\n",
    "    quantization_config=quantization_config,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory: 47.38 GB\n",
      "Allocated memory: 5.63 GB\n",
      "Total memory: 47.38 GB\n",
      "Allocated memory: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    device = torch.device(f'cuda:{i}')\n",
    "    print(f\"Total memory: {torch.cuda.get_device_properties(device).total_memory / 1024**3:.2f} GB\")\n",
    "    print(f\"Allocated memory: {torch.cuda.memory_allocated(device) / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vLLM HF logprob mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'Hello world!'}]\n",
      "torch.Size([1, 9])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[100257,     27,     91,    882,     91,    397,   9906,   1917,   4999]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from transformers import AutoTokenizer\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "model_id = \"allenai/OLMo-2-0425-1B-Instruct\"\n",
    "VLLM_URL = \"http://localhost:8001/v1\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "prompt = \"Hello world!\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "print(messages)\n",
    "templated = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", tokenize=False)\n",
    "inputs = tokenizer(templated, return_tensors=\"pt\")\n",
    "print(inputs[\"input_ids\"].shape)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_logprobs\n",
      "prompt_token_ids\n",
      "kv_transfer_params\n",
      "\"1 - 27 - '<' - logprob: -0.8175437450408936\"\n",
      "\"2 - 91 - '|' - logprob: -0.32933109998703003\"\n",
      "\"3 - 882 - 'user' - logprob: -10.7415771484375, rank only 4696, model wanted ID 439: {'logprob': -2.647827625274658, 'rank': 1, 'decoded_token': ' as'} \"\n",
      "\"4 - 91 - '|' - logprob: -9.323678016662598, rank only 14, model wanted ID 25: {'logprob': -0.011178131215274334, 'rank': 1, 'decoded_token': ':'} \"\n",
      "\"5 - 397 - '>\\n' - logprob: -5.864383697509766, rank only 24, model wanted ID 198: {'logprob': -2.4268834590911865, 'rank': 1, 'decoded_token': '\\\\n'} \"\n",
      "\"6 - 9906 - 'Hello' - logprob: -8.364974021911621, rank only 488, model wanted ID 791: {'logprob': -2.739973783493042, 'rank': 1, 'decoded_token': 'The'} \"\n",
      "\"7 - 1917 - ' world' - logprob: -4.457942008972168, rank only 10, model wanted ID 11: {'logprob': -1.520442008972168, 'rank': 1, 'decoded_token': ','} \"\n",
      "\"8 - 4999 - '!\\n' - logprob: -3.899444341659546, rank only 6, model wanted ID 0: {'logprob': -1.149444341659546, 'rank': 1, 'decoded_token': '!'} \"\n",
      "\"9 - 27 - '<' - logprob: -8.143547058105469, rank only 194, model wanted ID 40: {'logprob': -1.143547534942627, 'rank': 1, 'decoded_token': 'I'} \"\n",
      "\"10 - 91 - '|' - logprob: -6.250430107116699, rank only 48, model wanted ID 100257: {'logprob': -1.1098049879074097, 'rank': 1, 'decoded_token': '<|endoftext|>'} \"\n",
      "\"11 - 78191 - 'assistant' - logprob: -2.6662042140960693, rank only 3, model wanted ID 1752: {'logprob': -1.4162042140960693, 'rank': 1, 'decoded_token': 'less'} \"\n",
      "\"12 - 91 - '|' - logprob: -4.048925876617432, rank only 3, model wanted ID 100257: {'logprob': -0.5645509958267212, 'rank': 1, 'decoded_token': '<|endoftext|>'} \"\n",
      "\"13 - 397 - '>\\n' - logprob: -11.614391326904297, rank only 511, model wanted ID 22691: {'logprob': -1.005016565322876, 'rank': 1, 'decoded_token': ' Hello'} \"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "vllm_client = AsyncOpenAI(api_key=\"EMPTY\", base_url=VLLM_URL, timeout=1200)\n",
    "frontier_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\") \n",
    "\n",
    "async def process_prompt(prompt_id, prompt_text):\n",
    "    frontier_response = await vllm_client.chat.completions.create(\n",
    "        model=model_id,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt_text}],\n",
    "        temperature=0.00000001,\n",
    "        max_tokens=1,\n",
    "        logprobs=True,\n",
    "        extra_body={\n",
    "            \"prompt_logprobs\": 1,  # This is the vLLM-specific param\n",
    "        },\n",
    "    )\n",
    "    return frontier_response\n",
    "\n",
    "response = await process_prompt(0, prompt)\n",
    "# print(response.choices[0].logprobs)\n",
    "for key, val in response.model_extra.items():\n",
    "    print(key)\n",
    "\n",
    "for i in range(len(response.model_extra[\"prompt_logprobs\"])):\n",
    "    elt_dict = response.model_extra[\"prompt_logprobs\"][i]\n",
    "    if elt_dict == None:\n",
    "        continue\n",
    "    token_list = list(elt_dict.keys())\n",
    "    token_id = token_list[0]\n",
    "    # print(token_id, tokenizer.decode(int(token_id))) \n",
    "    # print(elt_dict)\n",
    "    suffix_string = \"\"\n",
    "    if elt_dict[token_id]['rank'] != 1:\n",
    "        suffix_string = f\", rank only {elt_dict[token_id]['rank']}, model wanted ID {token_list[1]}: {elt_dict[token_list[1]]} \"\n",
    "    print(repr(f\"{i} - {token_id} - '{elt_dict[token_id]['decoded_token']}' - logprob: {elt_dict[token_id]['logprob']}\" + suffix_string))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    dtype=\"bfloat16\",\n",
    "    device_map=\"cuda:1\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9, 100352])\n",
      "tensor([[ 27,  91, 439,  25, 198, 791,  11,   0,  40]], device='cuda:1')\n",
      "['<| as:\\nThe,!I']\n"
     ]
    }
   ],
   "source": [
    "output = model.forward(\n",
    "    **inputs.to(model.device),\n",
    "    do_sample=False,\n",
    "    max_new_tokens=1,\n",
    "    )\n",
    "print(output.logits.shape)\n",
    "argmaxed = output.logits.argmax(dim=-1)\n",
    "print(argmaxed)\n",
    "print(tokenizer.batch_decode(argmaxed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9, 100352])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 27,  91, 439,  25, 198, 791,  11,   0,  40]], device='cuda:1')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openr1_v3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
