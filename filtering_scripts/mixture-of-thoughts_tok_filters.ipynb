{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5caf1ab9",
   "metadata": {},
   "source": [
    "# Let's filter Mixture-of-Thoughts 'default' split down to 4096 toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e7950c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e74aa3cc2bdb4d58ad8af3ea77913924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad28017c5c8f4b27b93e503b7643830a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "all/train-00000-of-00015.parquet:   0%|          | 0.00/205M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ded51c126bd4a85a8af74f70f377b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "all/train-00001-of-00015.parquet:   0%|          | 0.00/205M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "043cc8b4999543e293bb180960583dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "all/train-00002-of-00015.parquet:   0%|          | 0.00/203M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff728d20fbd24015b43cd39e277d8961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "all/train-00003-of-00015.parquet:   0%|          | 0.00/205M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2499e0aaca4f4c21961d589cd5506b93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "all/train-00004-of-00015.parquet:   0%|          | 0.00/204M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71d3ac3af5b64487a32675782632a49d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "all/train-00005-of-00015.parquet:   0%|          | 0.00/205M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "mot_science = load_dataset(\"open-r1/Mixture-of-Thoughts\", \"all\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f784a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f98062",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.chat_template = \"{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- set date_string = \\\"26 Jul 2024\\\" %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \\\"You are a helpful AI Assistant that provides well-reasoned and detailed responses. You first think about the reasoning process as an internal monologue and then provide the user with the answer. Respond in the following format: <think>\\n...\\n</think>\\n<answer>\\n...\\n</answer>\\\" %}\\n{%- endif %}\\n\\n{#- System message + builtin tools #}\\n{{- \\\"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\\\" }}\\n{%- if builtin_tools is defined or tools is not none %}\\n    {{- \\\"Environment: ipython\\\\n\\\" }}\\n{%- endif %}\\n{%- if builtin_tools is defined %}\\n    {{- \\\"Tools: \\\" + builtin_tools | reject('equalto', 'code_interpreter') | join(\\\", \\\") + \\\"\\\\n\\\\n\\\"}}\\n{%- endif %}\\n{{- \\\"Cutting Knowledge Date: December 2023\\\\n\\\" }}\\n{{- \\\"Today Date: \\\" + date_string + \\\"\\\\n\\\\n\\\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \\\"You have access to the following functions. To call a function, please respond with JSON for a function call.\\\" }}\\n    {{- 'Respond in the format {\\\"name\\\": function name, \\\"parameters\\\": dictionary of argument name and its value}.' }}\\n    {{- \\\"Do not use variables.\\\\n\\\\n\\\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \\\"\\\\n\\\\n\\\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \\\"<|eot_id|>\\\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0]['content']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\\\"Cannot put tools in the first user message when there's no first user message!\\\") }}\\n    {%- endif %}\\n    {{- '<|start_header_id|>user<|end_header_id|>\\\\n\\\\n' -}}\\n    {{- \\\"Given the following functions, please respond with a JSON for a function call \\\" }}\\n    {{- \\\"with its proper arguments that best answers the given prompt.\\\\n\\\\n\\\" }}\\n    {{- 'Respond in the format {\\\"name\\\": function name, \\\"parameters\\\": dictionary of argument name and its value}.' }}\\n    {{- \\\"Do not use variables.\\\\n\\\\n\\\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \\\"\\\\n\\\\n\\\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \\\"<|eot_id|>\\\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\\n        {%- if message['role'] == 'assistant' %}\\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' }}\\n            {% generation %}\\n            {{- message['content'] | trim + '<|eot_id|>' }}\\n            {% endgeneration %}\\n        {%- else %}\\n            {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\\\n\\\\n'+ message['content'] | trim + '<|eot_id|>' }}\\n        {%- endif %}\\n    {%- elif 'tool_calls' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\\\"This model only supports single tool-calls at once!\\\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {{- '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' -}}\\n        {% generation %}\\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\\n            {{- \\\"<|python_tag|>\\\" + tool_call.name + \\\".call(\\\" }}\\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\\n                {{- arg_name + '=\\\"' + arg_val + '\\\"' }}\\n                {%- if not loop.last %}\\n                    {{- \\\", \\\" }}\\n                {%- endif %}\\n            {%- endfor %}\\n            {{- \\\")\\\" }}\\n        {%- else %}\\n            {{- '{\\\"name\\\": \\\"' + tool_call.name + '\\\", ' }}\\n            {{- '\\\"parameters\\\": ' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\\"}\\\" }}\\n        {%- endif %}\\n        {%- if builtin_tools is defined %}\\n            {{- \\\"<|eom_id|>\\\" }}\\n        {%- else %}\\n            {{- \\\"<|eot_id|>\\\" }}\\n        {%- endif %}\\n        {% endgeneration %}\\n    {%- elif message.role == \\\"tool\\\" or message.role == \\\"ipython\\\" %}\\n        {{- \\\"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\\\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \\\"<|eot_id|>\\\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' }}\\n{%- endif %}\" #adapted for Llama with reasoning chat template, and generation tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8a70a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mot_first = mot_science[0]\n",
    "return_dict = tokenizer.apply_chat_template(\n",
    "    mot_first['messages'],\n",
    "    return_dict=True,\n",
    "    return_assistant_tokens_mask=True,\n",
    ")\n",
    "length = len(return_dict['input_ids'])\n",
    "\n",
    "for i in range(length):\n",
    "    input_id = return_dict['input_ids'][i]\n",
    "    decoded = tokenizer.decode([input_id])\n",
    "    attention_mask = return_dict['attention_mask'][i]\n",
    "    assistant_masks = return_dict['assistant_masks'][i]\n",
    "    print(i, attention_mask, assistant_masks, input_id, repr(decoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dd78ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_length_mot = 172514\n",
    "lengths = []\n",
    "from tqdm.notebook import tqdm\n",
    "for i in tqdm(range(1000)):\n",
    "    item = mot_science[i]\n",
    "    return_dict = tokenizer.apply_chat_template(\n",
    "        item['messages'],\n",
    "        return_dict=True,\n",
    "        return_assistant_tokens_mask=True,\n",
    "    )\n",
    "    length = len(return_dict['input_ids'])\n",
    "    lengths.append(length)\n",
    "\n",
    "mean = sum(lengths) / len(lengths)\n",
    "maximum = max(lengths)\n",
    "print(mean)\n",
    "print(maximum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b0adaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check tokenized length\n",
    "max_length = 4096\n",
    "def filter_long_rows(example):\n",
    "    messages = example['messages']\n",
    "    tokenized = tokenizer.apply_chat_template(messages, tokenize=True)\n",
    "    length = len(tokenized)\n",
    "    return length <= max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ba8399",
   "metadata": {},
   "outputs": [],
   "source": [
    "science_filtered = mot_science.filter(filter_long_rows, num_proc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22459d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "science_filtered.push_to_hub(\"Neelectric/MoT_science_Llama3_4096toks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c18ab43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check tokenized length\n",
    "max_length = 8192\n",
    "def filter_long_rows(example):\n",
    "    messages = example['messages']\n",
    "    tokenized = tokenizer.apply_chat_template(messages, tokenize=True)\n",
    "    length = len(tokenized)\n",
    "    return length <= max_length\n",
    "science_filtered_longer = mot_science.filter(filter_long_rows, num_proc=16)\n",
    "science_filtered_longer.push_to_hub(\"Neelectric/MoT_science_Llama3_8192toks\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openr1_v2 (local)",
   "language": "python",
   "name": "openr1_v2_root"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
